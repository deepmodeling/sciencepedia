## 引言
当严重错误发生时，我们的本能反应通常是问：“谁该负责？”这种以人为中心的观点，侧重于个人指責和惩罚，虽然看似直观，但对于创建更安全的环境却从根本上适得其反。它将错误掩盖起来，使组织对其面临的最大风险视而不见。然而，存在一种更强大、更有效的理念：系统性安全方法，它不将人为失误视为失败的原因，而是视为系统内部深层问题的症状。

本文探讨了这种理解和管理风险的变革性方法。它旨在填补在指责个人与设计韧性系统之间的关键知识鸿沟。您将学习重塑了高风险行业现代安全科学与实践的核心原则。第一部分“原则与机制”将解析瑞士奶酪模型、公正文化以及从Safety-I到Safety-II的演变等基本概念。随后，“应用与跨学科联系”部分将展示这些理论如何付诸实践，揭示它们在重塑医疗保健、影响法律思想以及改善复杂技术环境安全性方面的深远影响。

## 原则与机制

试想一下，两家不同的医院面临完全相同的问题：一位技术娴熟、尽职尽责的护士犯了用药剂量错误。幸运的是，患者没有受到伤害，但事件本身是严重的。每家医院的应对方式揭示了思维上的深刻差异，这是一个关乎两种理念的故事，也是现代安全科学的核心。

### 两种方法的故事：个人 vs. 系统

第一家医院，我们称之为P医院，采取了看似合乎常理的方法。其领导者将焦点放在护士身上。他们问：“她为什么不多加小心？”他们得出结论，原因是注意力不集中或知识欠缺。解决方案很简单：将错误记录在护士的绩效档案中，派她参加补救培训，并分发一份备忘录，提醒所有员工要更加警惕。为了鼓励更好的绩效，领导层甚至将科室奖金与报告的错误数量保持在低水平挂钩。这就是**个人方法**：它将错误视为个人的失败，某种程度上的道德缺陷，并试图通过告诫、再培训和惩罚来纠正它们[@problem_id:4381495]。

第二家医院，S医院，则采取了截然不同的观点。当错误发生时，他们的第一个问题不是“谁干的？”，而是“为什么会发生？”。他们不把护士看作是失败的原因，而是系统缺陷的承受者。他们调查了背景情况。这位护士是否护理了太多病人？不同剂量的药瓶是否外观相似、容易混淆？在配药的关键时刻，她是否被反复打断？这就是**系统性方法**。它明白人类，无论多么熟练或谨慎，都是会犯错的。因此，它将错误视为系统内部更深层次问题的症状——即由人员、任务、工具和环境构成的“社会技术系统”的问题。其目标不是指责个人，而是重新设计系统，使错误不易发生且后果不那么严重[@problem_id:4391541]。

乍一看，个人方法感觉很直观，甚至可能很公正。然而，对于创建安全环境而言，它是一个灾难性的失败。在一个惩罚性的环境中，你会报告自己犯的错误吗？或者你及时发现的“有惊无险”的事件？当然不会。你会把它藏起来。六个月后，P医院的官方错误报告数量非常低——也许只有20份——并且“有惊无險”的事件报告为零。它在纸面上看起来很安全，但实际上却是在盲目飞行，完全没有意识到潜伏在表面之下的风险。S医院凭借其非惩罚性、以学习为中心的文化，在同一时期内有180份报告，其中130份是“有惊无险”事件——即在造成伤害前就被发现的错误。哪家医院实际上更安全？是那家有勇气诚实看待自己的医院[@problem_id:4381495]。系统性方法并非对错误“手软”；而是要足够明智，从而从中学习。

### 事故剖析：瑞士奶酪与潜在危险

为了理解系统如何失效，伟大的安全科学家James Reason给我们提供了一个绝妙、简单而有力的类比：**瑞士奶酪模型**[@problem_id:4391541]。想象一个组织抵御失败的防御体系就像一叠瑞士奶酪片。每一片都是一层保护：一项政策、一项像条形码扫描仪这样的技术、一位训练有素的专业人士、一次独立的双重核查。

在完美的世界里，这些奶酪片将是坚固的屏障。但在我们的世界里，每道防御都有弱点——即奶酪上的“孔洞”。政策可能写得不好。技术可能失灵或在现实环境中无法使用。人可能感到疲劳或分心。大多数时候，这些孔洞是无害的。一层防御的单个失效会被下一层坚固的防御所阻挡。灾难性事件，即不良事件，只在极少数情况下发生，即当*所有*奶酪片上的孔洞瞬间对齐，使得某个危险直接穿透所有防御层时。

这个模型揭示了两种类型失效之间的关键区别：

-   **主动失误**是由处于系统“尖端”的人员——如飞行员、外科医生或护士——所犯下的不安全行为。它们是事故最终的、可见的部分，就像护士给予了错误的剂量。在以人为中心的观点中，这就是故事的全部。但在系统观点中，这仅仅是结局。

-   **潜在条件**是隐藏的、系统层面的弱点，它们制造了奶酪上的孔洞。它们是设计者、管理者和决策者——即系统“钝端”——所做战略决策的必然结果。这些条件可能潜伏多年，如同等待发生的事故[@problem_id:4395185]。

考虑一个繁忙的儿科急诊部，一名儿童被给予了十倍于常规剂量的关键药物[@problem_id:5198081]。主动失误是护士输入了以磅为单位的儿童体重，而计算机系统期望的是千克。一个可怕的错误。但深入探究，潜在条件才是真正的罪魁禍首。物理体重秤默认单位是磅，而软件假设是千克——这是一个典型的设计缺陷，为用户设下了陷阱。分诊区拥挤嘈杂，平均每小时有12次打扰，超出了护士有限的注意力容量。医院的安全策略依赖于第二名护士进行双重核查计算，但那第二名护士被 sürekli响起的几十个非可操作性警报之一分了心——这是“警报疲劳”的受害者，是另一个潜在条件。最终的错误不是某一个人的警惕性失误；而是系统中奶酪孔洞完全可预见的、悲剧性的对齐。

### 为人工程：设计更安全的系统

如果问题在于系统的设计，那么解决方案必须是重新设计系统。这就是**人因工程学 (HFE)**的领域，这是一门将关于人类能力和局限性的知识应用于工具、任务和环境设计的学科[@problem_id:4882072]。其目标不是去完善人类，而是设计一个能包容我们不完美之处的世界。HFE的工作涵盖三个相互关联的领域：

-   **物理工效学**关注身体。它旨在设计一个能够减少劳损、使工作更轻松、更安全的物理环境。这可以很简单，比如提供高度可调的工作站以预防背痛，或者将常用物资放在伸手可及之处以减少不必要的走动和寻找[@problem_id:4882072]。

-   **认知工效学**关注心智。它致力于设计能够减轻我们有限的记忆力、注意力和决策能力负担的任务和工具。一些最有效的安全干预措施就源于此。我们不是告诉护士要“更小心”，而是可以重新设计计算机界面，让正确的选择成为轻松的选择。我们可以使用“高个子字母表示法”（例如，`hydrOXYzine` vs. `hydrALAZINE`）来防止外观相似的药物混淆。我们可以设计带有默认剂量和“硬停止”功能的药物医嘱界面，防止用户开出危险剂量。我们可以将自动的基于体重的计算器直接内置到工作流程中，从而消除容易出错的手动计算[@problem_id:4882072]。这些不仅仅是便利措施；它们是强有力的安全防御，消除了整类潜在条件。

-   **组织工效学**（或宏观工效学）关注社会系统。它专注于优化管理人与人之间协同工作的结构和流程。这包括设计有效的团队沟通协议，如**SBAR**（情境-背景-评估-建议），建立标准化的团队碰头会以确保每个人都有相同的心理模型，以及创建能够灵活应对工作量变化的排班模型。它旨在设计社会技术系统中的“社会”部分，使其与“技术”部分同样稳健[@problemid:4882072]。

### 安全文化：创建公正与学习型系统

即使设计得再精妙的系统也会有缺陷。发现并修复它们——即找到奶酪上的孔洞——的唯一方法，是让一线人员感到足够安全，能够坦率地谈论这些问题。这需要强大的**安全文化**，一种组织范围内共享的信念，即安全是重中之重，对失败的坦诚应受到珍视而非惩罰[@problem_id:4391543]。

健康安全文化的基石是**公正文化**。公正文化不是“无指责”文化，而是“公平问责”文化。它提供了一种结构化的方式来应对失败，通过区分三种截然不同的行为：

1.  **人为失误**：一次无意的滑移、疏忽或错误。我们都是人，都会犯错。公正的应对是安慰个人，并寻找方法修复导致错误发生的系统。

2.  **风险行为**：一种选择，其风险未被认识到或被错误地认为合理。这通常表现为“走捷径”或“权宜之计”。当一名护士在巨大的时间压力下，因为条形码扫描仪速度慢或有故障而绕过它时，她就在进行风险行为[@problem_id:4395185]。公正的应对不是惩罚，而是辅导，帮助她理解风险。更重要的是，我们必须问：*为什么需要这种权宜之计？*系统本身，由于设计不佳或资源不足，很可能正在鼓励这种行为。

3.  **鲁莽行为**：一种对重大风险有意识且不合理的漠视。这种情况很罕见。故意禁用安全警报或伪造记录的个人行为鲁莽。在此，且仅在此情况下，惩罚性措施才是合理的。

通过做出这些区分，公正文化创造了**心理安全感**。它向人们保证，他们不会因诚实的错误而受到惩罚，这反过来又释放出大量的关键安全信息。员工报告的“有惊无险”事件、权宜之计和微小故障，并非劳动力失败的迹象；它们是宝贵的**先行指标**——即早期预警，让我们能够在孔洞对齐并造成实际伤害之前修补防御系统中的孔洞[@problem_id:4395185]。一个只衡量伤害的组织总是在向后看；一个衡量“有惊无险”事件的组织则是在向前看。

### 超越避免失败：成功的科学

迄今为止的旅程已将我们从指责个人带到了设计更好的系统。但还有一个飞跃要做，一个与第一次同样深刻的视角转变。几十年来，安全科学几乎完全是关于失败的科学。我们关注的是哪里出了错。这就是**Safety-I**的世界，它将安全定义为没有不良事件[@problem_id:4961594]。

但想想医疗保健。这是一个极其复杂的系统，不断受到各种干扰的冲击——不可预测的病人到来、意料之外的并发症、资源短缺。如果安全仅仅是没有错误，那么系统会在几分钟内崩溃。令人惊讶的真相不是事情偶尔会出错，而是它们几乎总是能顺利进行。

这就引出了**Safety-II**，它将安全定义为系统在不同条件下取得成功的动态能力。这是关于成功的科学。为了理解它，让我们使用一个控制系统类比[@problem_id:4377513]。想象患者的状态是一个变量$y_t$，我们必须将其保持在安全边界$B$之内。系统不断受到干扰$w_t$的冲击，这些干扰有可能将$y_t$推出边界。是什么保证了患者的安全？是临床医生做出的持续、实时的调整$u_t$。这一系列的调整是“绩效变异性”，但它不是错误。它是控制的本质。它是护士注意到细微变化并调整滴速，或是外科医生根据意外的解剖结构改变技术。

从这个角度看，一线人员不是错误的来源；他们是**韧性**的主要来源——即系统预测、监控、响应和学习的能力。因此，一个组织的安全不仅在于其静态防御（清单、规则——Donabedian的**结构-过程-结果**框架的世界），还在于其适应能力。

这就是**高可靠性组织 (HROs)**的心态——像航空母舰和核电站这样的组织，它们在高危环境中取得了令人难以置信的安全记录[@problem_id:4397259]。HROs不仅擅長遵循规则（Safety-I）；他们更是自觉适应的大师（Safety-II）。他们专注于失败，不愿简化解释，并致力于韧性。最重要的是，他们奉行**尊重专业**，这意味着在危机中，权力会转移给对情况最了解的人，无论其级别如何。他们授权床边的护士暂停一个程序，因为他们明白，正是这位护士在做出保持系统安全的关键调整($u_t$)。

因此，最终的系统性方法是一个美丽的综合体。它使用Safety-I的原则来构建可靠的结构和流程作为基础。但它又将此与Safety-II的智慧相结合，培养文化和[适应能力](@entry_id:194789)，让人们能够发挥他们最擅长的一面：在真实世界 messy、不可预测且奇妙的复杂性中创造成功。

