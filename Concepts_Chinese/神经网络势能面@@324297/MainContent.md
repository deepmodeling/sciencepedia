## 引言
所有物质的行为，从单个水分子到最复杂的蛋白质，都由原子在一个被称为[势能面](@article_id:307856)（PES）的高维景观上错综复杂的舞蹈所决定。精确绘制这一景观是[分子模拟](@article_id:362031)领域的圣杯，它有望让我们能够从第一性原理出发，预测[化学反应](@article_id:307389)、设计新材料并揭示生命的奥秘。几十年来，科学家们一直面临一个艰难的权衡：量子力学的[高精度计算](@article_id:639660)对于大体系而言成本过高，而更快的[经典力场](@article_id:369501)通常缺乏必要的保真度。[神经网络势](@article_id:351133)（NNPs）作为一种革命性的方法应运而生，它弥合了这一差距，将量子数据的预测能力与机器学习的效率融为一体。本文将探索NNPs的世界，详细介绍它们的工作原理及其应用前景。

为了理解这场革命何以可能，我们将首先深入探讨赋予这些模型强大力量的基本思想。在第一章**原理与机制**中，我们将探究NNPs如何构建以遵循基本物理定律，以及它们如何学习支配化学的复杂量子力学相互作用。随后，关于**应用与跨学科联系**的章节将展示这些势的变革性影响，从预测材料性质、模拟生物系统，到在化学、物理和[机器学习理论](@article_id:327510)之间建立起令人惊奇的新联系。

## 原理与机制

想象你是一位徒步者，身处一片无限复杂的多山地形之中。在任何给[定点](@article_id:304105)，你脚下地面的高度是唯一决定你将滑向何方、山谷位于何处以及你可以选择哪些路径的因素。现在，想象这个景观不是三维的，而是成百上千维的，分子中的每个原子都对应一组坐标。这，本质上就是所有[化学反应](@article_id:307389)上演的舞台。

### 化学的景观

你脚下所站的地面就是**[势能面](@article_id:307856)**，即**PES**。在量子力学主宰的分子世界里，我们通常可以做出一个优美的简化，称为**Born-Oppenheimer近似**。由于原子核比电子[重数](@article_id:296920)千倍，它们移动缓慢，如同乌龟，而电子则像蜂鸟一样飞速掠过。这意味着我们可以想象原子核在某个构型 $\mathbf{R}$ 下被“固定”住，然后为*该特定排布*求解高速运动的[电子的基态](@article_id:300395)能量。如果我们对所有可能的原子核排布都这样做，我们就能描绘出一个连续的高维[势能景观](@article_id:304087)，$U(\mathbf{R})$ [@problem_id:2784636]。

这个景观就是一切。其中的山谷是稳定的分子，山隘是[化学反应](@article_id:307389)的[过渡态](@article_id:313517)。最重要的是，任何原子所受的力就是该景观在其位置上斜率的负值——即梯度：$\mathbf{F}_i = -\nabla_{\mathbf{r}_i} U(\mathbf{R})$。这意味着，如果你能绘制出这个景观，你就知道了力。而如果你知道了力，你就能预测原子将如何移动、[振动](@article_id:331484)和舞蹈——你就可以运行化学过程的动态模拟。以这种方式导出的[力场](@article_id:307740)被称为**保守的**，这带来一个极好的结果：一个孤立分子的总能量（势能加动能）在其运动过程中是完全守恒的，正如在真实世界中一样 [@problem_id:2952080]。因此，我们的任务就是创建一幅这个景观的完美地图。

### 不可违背的游戏规则

在我们开始绘制地图之前，必须认识到大自然有其规则。这些不是建议；它们是空间和物质的[基本对称性](@article_id:321660)。一张违反这些规则的地图不仅不准确，而且毫无意义。

首先，一个分子的能量不能取决于它在实验室中的位置或朝向。如果你把一个水分子从桌上移到架子上，或者把它倒置，它的内能不会改变。这意味着我们的势能函数 $U(\mathbf{R})$ 必须**对刚性平移和旋转不变**。它只能依赖于*内禀*几何结构，比如原子间的距离，而不是它们在空间中的绝对坐标。

其次，也是更深刻的一点，大自然不会给她的原子贴上标签。如果一个分子有两个氢原子，它们是完全、绝对相同的，你无法区分它们。如果你交换它们的位置，能量必须保持完全相同。这就是**[置换](@article_id:296886)不变性**。对于像甲烷 $\mathrm{CH}_4$ 这样的分子，有 $4! = 24$ 种方式可以[排列](@article_id:296886)四个相同的氢原子，能量在所有这些[排列](@article_id:296886)中必须是一个完美的常数。我们构建的任何模型都必须将这种对称性融入其结构本身 [@problem_id:2796818] [@problem_id:2952097]。

### 分而治之策略：局域性的力量

乍一看，绘制[势能面](@article_id:307856)似乎是不可能的。一个关于 $N$ 个原子的 $3N$ 个变量的函数，其复杂性超乎想象。试图用一个简单的多项式（如老式[经典力场](@article_id:369501)中使用的泰勒级数）来近似它，仅适用于单个平衡谷点附近的微小振动 [@problem_id:2456343]。

现代[神经网络势](@article_id:351133)（NNPs）的突破在于一个深刻的“分而治之”策略，其灵感来源于物理上的**[近视性原理](@article_id:368628)**。单个原子的化学环境和能量贡献主要由其近邻决定。一杯水中央的一个水分子并不太在意杯子另一边的某个分子。

这引出了一个既简单又强大的思想：体系的总能量就是每个独立原子能量贡献的总和：
$$
E(\mathbf{R}) = \sum_{i=1}^{N} E_i
$$
这里，$E_i$ 是原子 $i$ 的能量，它只依赖于其在某个截断距离 $r_c$ 内邻近原子的排布 [@problem_id:2648619]。

这种原子分解带来一个惊人的结果。想象两个分子 $\mathcal{A}$ 和 $\mathcal{B}$ 相距很远——远超截断距离。分子 $\mathcal{A}$ 中原子的能量贡献完全不受分子 $\mathcal{B}$ 中原子的影响，反之亦然。因此，组合体系的总能量就是 $E(\mathcal{A} \cup \mathcal{B}) = E(\mathcal{A}) + E(\mathcal{B})$。这个性质被称为**尺寸[广延性](@article_id:313063)**，而这种架构自然而然地满足了它！它正确地描述了非相互作用体系的能量，这是一个许多旧方法难以满足的基本物理要求 [@problem_id:2760129]。

### 架构师的工具箱：从原子到能量

因此，这个宏大的问题被简化为一个更易于处理的问题：我们如何基于单个原子的局域邻近环境，在遵循所有物理定律的前提下，计算其能量 $E_i$？这正是NNP中“神经网络”部分发挥作用的地方。

首先，我们必须以一种尊重对称性的方式来描述原子邻近环境。我们不能简单地将邻近原子的 $(x, y, z)$ 坐标输入神经网络，因为如果我们旋转分子，这些数字会改变。取而代之的是，我们为每个原子的环境计算一个指纹，即**描述符**。这个描述符是一个由其邻近原子的距离和角度导出的数值向量。例如，一个描述符可能包含这样的信息：“在2.1 Å的距离处有一个碳原子，在1.4 Å处有一个氧原子，它们之间的角度是$109^\circ$”，所有这些信息都被打包成一个固定长度的向量，该向量在旋转、平移或交换相同邻近原子时保持不变 [@problem_id:2952097]。

一旦我们有了这个[不变性](@article_id:300612)指纹 $\mathbf{G}_i$，就可以将它输入一个标准的[前馈神经网络](@article_id:640167)，该网络随后输出原子能量 $E_i = \mathcal{N}(\mathbf{G}_i)$。[神经网络](@article_id:305336)是一个[通用函数逼近器](@article_id:642029)。它不是一个简单的多项式；它是一个高度灵活的非线性机器，通过观察数千个量子力学计算的例子，*学习*原[子环](@article_id:314606)境几何结构与其能量贡献之间的复杂关系。它[实质](@article_id:309825)上是从数据中为化学相互作用构建了自己的、学得的高维“基” [@problem_id:2456343]。

即使是[神经网络](@article_id:305336)的内部机制也具有物理后果。“激活函数”引入了非线性，其作用至关重要。如果我们使用一个完全光滑的激活函数，如[双曲正切函数](@article_id:638603) $\tanh(x)$，得到的[势能面](@article_id:307856)也将是无限光滑的，从而产生连续的力，适合进行稳定的模拟。然而，如果我们使用一个带有“扭结”的函数，如流行的[修正线性单元](@article_id:641014) $\mathrm{ReLU}(x) = \max(0,x)$，得到的[能量景观](@article_id:308140)将会有尖锐的折痕。在模拟过程中跨越这些折痕之一，会导致原子受力发生不连续的跳跃——一种非物理的突变，这会破坏模拟，并使其无法计算[振动频率](@article_id:330258)等性质 [@problem_id:2456262] [@problem_id:2952080]。这是一个底层计算选择与高层物理原理直接相关的绝佳例子。

随着该领域的成熟，架构也在不断发展。第一代NNPs使用固定的、手工设计的描述符。下一代，通常基于**[图神经网络](@article_id:297304) (GNNs)** 或**[消息传递](@article_id:340415)神经网络 (MPNNs)**，则在训练过程中自己学习描述符，从而实现了更大的灵活性和[表达能力](@article_id:310282) [@problem_id:2648619]。一些先进模型更进一步，采纳了一种称为**[等变性](@article_id:640964)**的概念。它们不是在每一步都使所有东西保持不变，而是处理像向量和[张量](@article_id:321604)这样的几何对象，确保它们在旋转下正确变换，只在最后一步才将所有东西坍缩为一个不变的标量能量。这种更丰富的内部表示保留了更多的几何信息，这对于预测能量以外的性质至关重要 [@problem_id:2760132]。

### 超越局域：驯服[长程力](@article_id:361141)

局域的“分而治之”方法有一个主要的阿喀琉斯之踵：长程相互作用。物理学告诉我们，离子间的[静电力](@article_id:382016)以 $1/r$ 的形式缓慢衰减，而范德华[色散力](@article_id:313615)以 $1/r^6$ 的形式衰减。这些相互作用虽然微弱但可以累积，它们对于描述从盐晶体到蛋白质折叠的一切都至关重要。一个具有有限截断距离（比如 6 Å）的模型，根本无法感知这些长程力。它无法区分一个在10 Å处的离子和一个在100 Å处的离子 [@problem_id:2796824]。

这是否意味着整个方法注定要失败？完全不是。解决方案既实用又强大：创建一个混合模型。我们使用NNP，凭借其所有的灵活性和数据驱动的能力，来处理短程范围内混乱、复杂的量子力学相互作用。对于长程部分，我们重新引入明确的、有物理动机的静电和[色散](@article_id:376945)方程。

这种混合方法结合了两个世界的优点。NNP学习[化学键](@article_id:305517)的复杂细节，而解析公式确保了在长距离下的正确物理行为。这两个部分被平滑地混合以避免重复计算相互作用。这展示了一种成熟的科学方法：在我们的理[解模糊](@article_id:335597)但数据丰富的地方使用机器学习，而在我们的理解清晰且现象简单的地方使用已建立的物理定律。正是这种原理、数据和巧妙架构的综合，使得[神经网络势](@article_id:351133)能够以前所未有的准确度和范围创建化学景观的地图 [@problem_id:2796824]。