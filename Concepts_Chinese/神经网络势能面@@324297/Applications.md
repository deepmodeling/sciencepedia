## 应用与跨学科联系

现在我们已经熟悉了[神经网络势](@article_id:351133)的复杂机制——它们如何被构建以遵循[基本对称性](@article_id:321660)并领会量子力学的精髓——我们可以提出最激动人心的问题：我们能用它们来*做*什么？它们[能带](@article_id:306995)我们到达哪些新的理解前沿？你会看到，答案不仅仅是一系列技术成就。这是一个关于连接的故事，关于在微观与宏观、化学与生物学、物理与工程学之间，甚至在自然研究与学习研究本身之间架设桥梁的故事。NNP不仅仅是一个用于预测能量的黑箱；它是一种被赋予了物理规则的新型画布，我们可以在其上以惊人的保真度描绘原子复杂的舞蹈。

### 编织更精细的织锦：精炼和重建分子模型

在我们能跑之前，必须先学会走。NNPs一些最直接和最强大的应用来自于改进我们已有的工具。几十年来，经典分子模拟一直依赖于“[力场](@article_id:307740)”——描述原子间推拉作用的简化经验函数。这些模型速度快，但可能很粗糙。考虑围绕[化学键](@article_id:305517)的旋转，它由所谓的扭转势或[二面角](@article_id:314447)势控制。传统上，这是通[过拟合](@article_id:299541)一个原型分子的一维能量扫描来[参数化](@article_id:336283)的。但自然界并非如此简单。真实的能量取决于整个分[子环](@article_id:314606)境，并与其他运动耦合。

在这里，NNPs提供了一条通往更精确真理的道路。我们不再拟合单一的、理想化的扫描，而是可以在大量量子力学数据——能量以及至关重要的力——上训练模型，这些数据来自许多不同的分子和构象。通过整合这些丰富的数据并强制执行必要的物理对称性（例如，旋转整整 $360^\circ$ 会回到起点），我们可以学习到一种更准确、更具迁移性的有效扭转势。这种学得的势能隐含地考虑了复杂的环境耦合，然后可以被投影回我们旧的模拟程序所能理解的简单函数形式，有效地用量子力学的见解对其进行“升级” [@problem_id:2452448]。

除了精炼旧模型，我们还可以构建新模型。以[氢键](@article_id:297112)为例，这种谦逊而又至关重要的相互作用维系着我们的DNA，赋予水以维持生命的特性，并决定蛋白质的结构。捕捉其在静电、极化和量子效应之间的微妙平衡一直是一个长期挑战。NNP可以专门为此任务进行训练。通过向其输入大量[氢键](@article_id:297112)对的量子力学能量数据，并用具有物理意义的、对称性不变的描述符而不是原始坐标来描述每个几何构型，网络学会了以极高的精度预测[氢键](@article_id:297112)能量。它学习到了对距离和角度的关键依赖性，创造了一个专门的工具来正确模拟自然界中最重要的相互作用之一 [@problem_id:2456477]。

### 从原子之舞到宏观世界性质

也许物理学最深远的承诺是从支配其微观组分的基本规则出发，解释我们看到的世界——一杯[水的性质](@article_id:298432)，一块晶体的[熔点](@article_id:374672)。NNPs正在以前所未有的准确性将这一承诺变为现实。

考虑一种体相材料，如液态水。其最显著的特性之一是其高静态[介电常数](@article_id:332052)，$\varepsilon \approx 80$。这个数字告诉我们水屏蔽[电荷](@article_id:339187)的效率。但它从何而来？它源于数万亿个水[分子偶极矩](@article_id:313069)的集体关联涨落。在模拟中，为原子分配恒定[电荷](@article_id:339187)的固定[电荷](@article_id:339187)模型只捕捉到了故事的一部分——取向涨落。它们忽略了每个水分子的电子云会因其邻近分子的电场而变形，这种现象称为[电子极化](@article_id:305693)。这就是为什么这类模型会系统性地低估[介电常数](@article_id:332052)。更先进的“可极化”模型试图模仿这一点，但基于NNP的模型，经高级量子力学训练，能够更保真地、隐含地捕捉这些多体电子效应。通过更准确地表示偶极涨落的全部谱系，从单个分子的旋转到邻近分子间电子云的微妙晃动，NNPs为从微观之舞到我们在实验室中测量的宏观性质提供了更准确的桥梁 [@problem_id:2648568]。

我们还可以更进一步，从静态性质到像[相变](@article_id:297531)这样的动态过程。一种新的、通过[计算设计](@article_id:347223)的材料的[熔点](@article_id:374672)是多少？[熔点](@article_id:374672)是固相和液相具有完全相同的[吉布斯自由能](@article_id:307192)时的温度。有了NNP，我们可以进行直接共存模拟，将固相和液相置于接触状态，并找到界面保持稳定的温度。但在这里，NNPs提供了更深层次的东西。我们不仅可以训练一个NNP模型，还可以训练一个NNP模型的*系综*，每个模型都是对[量子数](@article_id:305982)据的略微不同但同样合理的拟合。通过用系综中的每个模型计算[熔点](@article_id:374672)，我们不仅可以预测一个单一的数字，还可以量化我们对该预测的信心。我们可以将来自模拟有限性带来的不确定性（[偶然不确定性](@article_id:314423)）与来自模型本身是现实不完美表示的不确定性（[认知不确定性](@article_id:310285)）分离开来。这种不仅能说“是什么”还能说“我们知道得多好”的能力，是成熟的预测性科学的标志 [@problem_id:2456317]。

在最深层次上，人们甚至可以问，神经网络内部的一个特定参数——单个权重或偏置——如何影响像自由能这样的宏观[热力学](@article_id:359663)性质。这听起来像一个不可能的问题，但[统计力](@article_id:373880)学的数学框架提供了一个直接的答案。推导宏观自由能对NNP中任何微观参数的梯度的精确表达式是可能的。这一卓越的联系形成了一条“影响链”，原则上允许我们不仅在低级的能量和力上训练NNP，而且可以直接对其进行优化，以重现已知的宏观[热力学](@article_id:359663)性质 [@problem_id:38398]。

### 深入未知：模拟生命与光的复杂性

有了这些强大且经过验证的机制，我们可以冒险进入令人眼花缭乱的复杂领域。思考一下生命的机器。蛋白质是一项工程奇迹，一串氨基酸折叠成精确的三维结构以执行其功能。一些蛋白质表现出一种称为[冷变性](@article_id:354927)的奇怪行为：它们不仅在加热时会解折叠，在变得非常冷时也会解折叠。这个反直觉的过程是由周围水分子[热力学](@article_id:359663)的微妙变化驱动的。为了模拟它，势能必须在广泛的构象范围内——折叠态、无数的解折叠态以及过渡路径——都保持准确，并且必须以近乎完美的保真度描述蛋白质-水的相互作用。

为这样的任务构建一个NNP是一项艰巨但有原则的工作。它需要生成探索所有这些状态的训练构象，通常使用[增强采样](@article_id:343024)方法。它需要使用显式水分子，因为它们是过程中的主要参与者。它需要用高级量子力学（通常是混合[QM/MM方法](@article_id:348074)）计算的参考能量和力。而且它通常需要一个“[主动学习](@article_id:318217)”循环，其中NNP被用来探索，识别它最不确定的构象，然后请求新的[量子计算](@article_id:303150)来填补其知识中的这些空白。其结果是一个能够探索以前第一性原理模拟无法企及的生物学现象的势能 [@problem_id:2456340]。

从蛋白质折叠的缓慢舞蹈，我们也可以转向[光化学](@article_id:301376)的超快世界。分子吸收一个[光子](@article_id:305617)后的最初飞秒内会发生什么？系统不再处于其电子[基态](@article_id:312876)[势能面](@article_id:307856)上。它可能被推到[激发态](@article_id:325164)，从那里它可以辐射光、发生反应，或者在一个“[锥形交叉](@article_id:323915)”——两个能量面接触的点——“跳跃”回[基态](@article_id:312876)。为了模拟这种[非绝热动力学](@article_id:324095)，我们需要的不仅仅是能量和力；我们还需要电子态之间的耦合。一种杰出的NNP策略已经出现：网络不直接学习能量面，而是学习底层的*准绝热哈密顿量*。这是一个小矩阵，其元素是核几何构型的[光滑函数](@article_id:299390)。通过在每一步[对角化](@article_id:307432)这个矩阵，我们以完全自洽和物理上严谨的方式获得了我们需要的所有量——多个能量面、它们的力，以及至关重要的、控制它们之间跳跃的[非绝热耦合](@article_id:371212)。这为NNPs模拟视觉、光合作用以及设计新型光敏材料打开了大门 [@problem_id:2456299]。

### 思想的循环：与力学和[机器学习理论](@article_id:327510)的联系

NNPs的影响不仅限于化学和生物学。这些思想本身正在为其他领域带来启发。例如，在固[体力](@article_id:353281)学中，工程师试图描述材料在应力下如何变形。[超弹性材料](@article_id:369306)的行为由[应变能密度函数](@article_id:378253)控制，[热力学](@article_id:359663)要求该函数必须是应变的[凸函数](@article_id:303510)。当试图从实验应力-应变数据中学习此函数时，我们如何确保我们的模型尊重这一基本物理定律？

答案在于物理学和机器学习架构的完美融合。通过使用一种称为输入凸[神经网络](@article_id:305336)（ICNN）的特殊类型网络，其结构在数学上保证其输出是其输入的[凸函数](@article_id:303510)，我们可以将[热力学一致性](@article_id:299334)直接构建到我们的模型中。然后，我们可以使用基于Legendre-Fenchel对偶性（[凸分析](@article_id:336934)的一个深刻原理，它将应变能与其对偶的余应力能联系起来）的[损失函数](@article_id:638865)来训练这个ICNN。这是一个深刻的例子，说明了如何根据物理定律量身定制网络的*架构*，从而产生更鲁棒和更具预测性的模型 [@problem_id:2629391]。

最后，在一个优美的转折中，我们用来[分析化学](@article_id:298050)中[势能面](@article_id:307856)的概念，为理解机器学习过程本身提供了一个强大的视角。神经网络的训练是一个优化问题：在一个极其高维的“[损失景观](@article_id:639867)”中寻找最小值，其中坐标是网络的权重，“能量”是训练数据的误差。就像在化学中一样，这个景观充满了最小值点和[鞍点](@article_id:303016)。我们可以使用黑塞矩阵，即二阶[导数](@article_id:318324)矩阵，来表征它们。其[特征值](@article_id:315305)告诉我们景观的曲率。

一个具有大的正[特征值](@article_id:315305)的“尖锐”最小值点是一个狭窄、陡峭的山谷。一个具有许多小的正[特征值](@article_id:315305)的“平坦”最小值点是一个宽阔、浅显的盆地。越来越多的证据表明，我们的优化算法找到的对应于这些平坦最小值点的解，往往能更好地泛化到新的、未见过的数据上。这种类比直接而有力：为理解[分子稳定性](@article_id:298195)而打造的数学工具，如今正帮助我们理解人工智能的鲁棒性 [@problem_id:2455291]。

从单个[化学键](@article_id:305517)的扭转到晶体的熔化，从蛋白质的解折叠到光化学反应的闪光，[神经网络势](@article_id:351133)正被证明是一种革命性的工具。它们不仅仅是强大的[插值器](@article_id:363847)；它们是一种新的理论[范式](@article_id:329204)，一座连接基本量子定律与世界复杂性的多功能桥梁，在此过程中揭示了科学內在的美和统一性。