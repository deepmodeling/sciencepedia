## 应用与跨学科联系

在理解了如何将类别的定性、描述性本质转化为数学的定量语言的原理之后，我们现在准备好开始一段真正的旅程了。这才是真正乐趣的开始。正如我们所见，创建[虚拟变量](@article_id:299348)这个简单的行为就像是打开了一扇门。但门后的景象并非一条简单、笔直的走廊，而是一个迷宫，充满了引人入胜的挑战、令人惊奇的联系以及贯穿整个现代数据科学领域的、优美而巧妙的解决方案。让我们穿过这扇门，一探究竟。

### 世界并非平坦：几何、距离与千类诅咒

我们的第一站是一个既需谨慎又充满奇迹的地方：由[独热编码](@article_id:349211)创造的高维空间。想象你有一个类别特征，比如一个人的居住城市。如果有一千个可能的城市，我们这个简单的特征就会突然爆炸成一个由多个零和一个一组成的一千维向量。现在，如果我们有几个这样的特征会怎样？我们数据的维度会急剧飙升。

这不仅仅是一个计算上的麻烦；它从根本上改变了我们数据空间的几何结构。思考一下像DBSCAN这样基于距离的[算法](@article_id:331821)，它通过寻找点的密集邻域来发现簇。在一个被[独热编码](@article_id:349211)膨胀的空间里，所有东西看起来都离其他东西很远。任意两个随机点之间的平均距离急剧增加，“密集邻域”的概念开始消解。对于一个固定的半径 $\varepsilon$，本应是邻居的点发现自己漂浮在一个巨大、空旷的空间里，我们的[聚类算法](@article_id:307138)可能无法找到任何有意义的结构，而将大多数点视为噪声[@problem_id:3114649]。

这种“[维度灾难](@article_id:304350)”也给[主成分分析](@article_id:305819)（PCA）等方法施加了一种奇怪的魔咒。当我们用独热[向量表示](@article_id:345740)类别时，我们给数据施加了一种僵硬而人为的结构。单个特征的[虚拟变量](@article_id:299348)并非独立的；在中心化之后，它们是完全[负相关](@article_id:641786)的。例如，如果一个人不是来自城市A，也不是来自城市B，那么他必须来自城市C（如果这是仅有的选项）。PCA旨在寻找最大方差的方向，很容易被这种人为的结构所欺骗。它可能会发现一个“主成分”，仅仅是对比最常见的类别和最稀有的类别，这是一个高方差的方向，与底层的科学问题毫无关系，而仅仅是编码方式和类别频率不平衡的产物。这种无监督方法，对我们的最终目标一无所知，可能会抛弃其他特征中真正具有预测性的信息，而去解释由我们自己的编码方案产生的、响亮但无趣的方差[@problem_id:3160819]。

那么，我们该怎么办？我们把简单的类别变成了高维向量，却发现新空间是一个扭曲而难以导航的土地。答案不是放弃编码，而是成为更聪明的制图师。我们必须学会绘制更好的地图。

### 绘制更好的地图：从原始坐标到有意义的[嵌入](@article_id:311541)

处理[定性预测变量](@article_id:640949)的艺术在于创建*[嵌入](@article_id:311541)*——即低维、连续的[向量表示](@article_id:345740)，它们以对当前任务有意义的方式捕捉类别的*本质*。

#### 统计学家的地图：对应分析

最优雅的经典方法之一来自统计学。我们可以使用一种名为对应分析（CA）的姊妹技术，而不是天真地将[PCA应用](@article_id:311282)于计数表。PCA试图在欧几里得世界中解释方差，而CA则试图在列联表的世界中，使用基于卡方统计量的更合适的几何结构，来解释*与[统计独立性](@article_id:310718)的偏差*。

想象一个[交叉](@article_id:315017)列表，它记录了两个类别变量，比如病人的组织亚型和特定突变的存在。CA不仅仅看原始计数。它会问：“与组织亚型和突变完全不相关时我们[期望](@article_id:311378)的计数相比，这些计数有多令人意外？”然后它对这个“意外”矩阵执行奇异值分解（SVD）。结果是一张低维地图，其中两个组织亚型点之间，或一个组织亚型与一个突变点之间的邻近度，反映了它们关联的强度，并且已经恰当地考虑了一些组织亚型或突变本身就更常见这一事实。这提供了一种有原则的方法，将类别水平[嵌入](@article_id:311541)到一个连续空间中，这个空间充满了关于它们相互关系的丰富意义[@problem_id:3173904]。

#### 计算机科学家的地图：图的光谱力量

一个更现代且极其灵活的方法是将我们的类别视为网络中的节点。假设我们的类别是电子商务网站上的产品。如果两种产品经常出现在同一个购物车中，我们就可以在它们之间画一条边。边的权重可以表示这种共现的强度。现在我们有了一个代表我们类别之间关系的图。

我们如何将这个图变成向量[嵌入](@article_id:311541)？在这里，我们借用了物理学和[图论](@article_id:301242)中一个强大的思想：[谱分析](@article_id:304149)。通过分析图的拉普拉斯矩阵——一个编码了[图连通性](@article_id:330538)信息的矩阵——的[特征向量](@article_id:312227)，我们可以为我们的类别获得一个[坐标系](@article_id:316753)。对应于最小非零[特征值](@article_id:315305)的[特征向量](@article_id:312227)（通常称为[Fiedler向量](@article_id:308619)）具有一个显著的特性：它们以一种尊重图的簇和结构的方式[排列](@article_id:296886)节点。在图中[紧密连接](@article_id:349689)的类别将被映射到新[向量空间](@article_id:297288)中的邻近点。这种“谱[嵌入](@article_id:311541)”提供了一种强大的非线性方法，将关系结构转化为几何结构，从而创建出可以与任何下游机器学习任务的其他连续数据相结合的丰富特征[@problem_id:3117810]。

#### 生物学家的地图：森林的智慧

让我们转向[计算生物学](@article_id:307404)，寻找另一个绝妙的想法。假设我们有一个癌症患者的数据集，由基因表达水平（连续）和临床变量（类别）混合描述，我们希望在没有任何预先存在的标签的情况下发现患者的亚型。这是一个[无监督聚类](@article_id:347668)问题。[随机森林](@article_id:307083)，一个监督[算法](@article_id:331821)，怎么可能帮上忙呢？

诀窍在于：我们创建一个“假的”监督问题。我们取原始的患者数据（称之为“类别1”），然后通过独立地打乱每个特征列中的值来创建一个同样大小的合成数据集。这种打乱破坏了相关结构，我们将这个合成数据称为“类别0”。现在，我们训练一个[随机森林](@article_id:307083)来区分真实的、有结构的数据和合成的、随机的数据。

森林本身并不是我们的最终目标。魔力在于它在此过程中学到的东西。对于任意两个*真实*的患者，比如患者A和患者B，我们现在可以问：在森林中的多少棵树里，他们最终落在了同一个终端叶节点上？这个比例就是他们的“邻近度”。如果他们经常被分到同一个叶节点，这意味着[决策树](@article_id:299696)的集合认为他们在复杂的特征组合上非常相似。这个邻近度矩阵为我们提供了一种强大的、非线性的、数据驱动的相似性度量，它自然地处理了混合数据类型和交互作用。然后我们可以使用这个相似性度量来对患者进行聚类，揭示那些对于像PCA这样的线性方法来说是不可见的亚型[@problem_id:2384488]。这是多么巧妙的构思——通过发明一个监督问题来解决一个无监督问题！

### 现代世界的生活：前沿[算法](@article_id:331821)中的[定性预测变量](@article_id:640949)

掌握了这些思考类别的复杂方法之后，让我们来看看当今一些最强大的机器学习[算法](@article_id:331821)的内部机制。

#### [梯度提升](@article_id:641131)的效率

[梯度提升](@article_id:641131)机（GBM）是[预测建模](@article_id:345714)领域的巨头，尤其是在表格数据上。它们面临的一个关键挑战是处理具有数千个水平的类别特征（例如，邮政编码）。完整的[独热编码](@article_id:349211)在计算上将是一场灾难。它们是否使用了我们那些花哨的[嵌入](@article_id:311541)方法之一？答案是一个辉煌的[算法工程](@article_id:640232)杰作。

在提升过程的每一步，模型都试图纠正前一步的错误（即“伪[残差](@article_id:348682)”或梯度）。对于一个类别特征，[算法](@article_id:331821)会计算每个类别水平的*平均伪[残差](@article_id:348682)*。然后它根据这个平均值对这些水平进行排序。现在，在类别特征上找到最佳分割点的问题，就简化为在这个一维有序列表中找到最佳分[割点](@article_id:641740)——一个可以在线性时间内完成的任务。这里的优雅令人叹为观止。模型利用它正试图最小化的误差信号，来动态高效地构建其寻找最佳决策规则的搜索过程。它动态地创建了一个临时的、针对特定任务的类别排序，从而绕过了任何固定的、高维编码的需求[@problem_id:3125598]。

#### 深度学习的线性特性

现在让我们探索深度学习的前沿。像“mixup”这样的[数据增强](@article_id:329733)技术，通过对真实样本进行加权平均来创建合成训练样本，在[计算机视觉](@article_id:298749)领域取得了巨大成功。我们能将这种方法应用于具有混合特征的表格数据吗？

想象我们有两个数据点。我们可以很容易地混合它们的连续[特征和](@article_id:368537)目标标签：$\tilde{x} = \lambda x_1 + (1-\lambda)x_2$ 和 $\tilde{y} = \lambda y_1 + (1-\lambda)y_2$。但是混合类别A和类别B意味着什么呢？我们是抛硬币决定吗？答案在于模型本身的深层线性结构。一个类别特征通过一个[嵌入](@article_id:311541)层输入到[神经网络](@article_id:305336)中，这只是一个线性的矩阵乘法。由于这种线性特性，我们发现我们可以（1）先混合独热向量，然后将结果通过[嵌入](@article_id:311541)层，或者（2）先将独热向量通过[嵌入](@article_id:311541)层，然后混合得到的[嵌入](@article_id:311541)向量。结果在数学上是完全相同的！这种美妙的一致性使我们能够以一种有原则的方式将mixup扩展到类别数据，确保模型对混合输入的预测是其对原始输入预测的完美线性插值，正如mixup原则所要求的那样[@problem_id:3151922]。

### 我们能信任机器吗？对[可解释性](@article_id:642051)的追求

我们已经构建了功能强大的模型，能够从复杂的混合数据类型中学习。但强大的能力也需要深刻的理解。我们能否解释一个模型*为什么*做出某个特定的决定，尤其是当它涉及类别特征之间错综复杂的交互作用时？

这就把我们带到了[可解释性](@article_id:642051)人工智能（XAI）的领域。一种强大的技术，LIME（局部[可解释模型](@article_id:642254)无关解释），正面解决了这个问题。其思想是通过在该实例的局部邻域内，用一个简单的、可解释的[代理模型](@article_id:305860)（如[线性模型](@article_id:357202)）来近似复杂的“黑箱”模型，从而解释单个实例的预测。

为了解释一个涉及例如 `(X1=B, X2=E, X3=G)` 的实例的预测，我们可以通过一次翻转一个类别来生成附近的的数据点（例如 `(X1=A, X2=E, X3=G)`）。然后我们向我们的黑箱询问它对所有这些扰动点的预测。最后，我们用[虚拟变量](@article_id:299348)（可能还有一些简单的交互作用）拟合一个简单的[线性模型](@article_id:357202)到这个局部数据上，并根据与原始实例的邻近度进行加权。这个简单模型的系数为我们提供了一个局部解释：“预测值增加了0.8，因为`X1`是`B`且`X2`是`E`共同作用的结果，这个效应并不能被它们各自的[主效应](@article_id:349035)所捕捉。”通过系统地增加我们[代理模型](@article_id:305860)的复杂性，我们可以探索解释的简单性与它对原始黑箱的保真度之间的权衡，从而揭示驱动模型行为的类别特征之间复杂的相互作用[@problem_id:3140803]。

一个最初简单的问题——如何表示一个类别——引领我们进行了一场穿越几何学、统计学、[图论](@article_id:301242)和算法设计的宏大旅行。我们看到，卑微的类别变量并非[现代机器学习](@article_id:641462)故事中的一个注脚，而是一个核心角色，推动我们开发出更聪明、更强大，并最终更易于理解的模型。