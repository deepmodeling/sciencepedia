## 应用与跨学科联系

我们花了一些时间来理解损失函数的机制——它们如何充当向导、教师，告诉神经网络其表现是好是坏。在传统观念中，这位教师只有一个信息来源：一个包含正确答案的数据集。网络尝试猜测，教师将其判为错误，网络再试一次，慢慢学会模仿标准答案。这是一种强大的方法，但有点像只通过查看一长串实验结果来学习物理，却从未被告知牛顿定律。你最终可能会发现物体会下落，但你会错过支配它们*为何*以及*如何*下落的那些优雅而强大的原理。

如果我们能做得更好呢？如果我们能给我们的神经网络一份宇宙的“小抄”呢？如果在向它们展示数据的同时，我们还能教它们游戏规则——物理学的基本定律、化学的约束、经济学的原理，那会怎样？这不是一个异想天开的想法。这是一种正在改变科学和工程实践的革命性方法，而使其成为可能的工具，正是我们一直在研究的东西：[损失函数](@article_id:638865)。通过设计自定义[损失函数](@article_id:638865)，我们可以创造出一位更为复杂的教师，他不仅会告诉网络“你错了”，还会说“你错了，*并且*你的答案违反了[能量守恒](@article_id:300957)定律。”

本章将带领我们穿越这片激动人心的领域。我们将看到这个单一而优雅的思想——将领域知识编码到损失函数中——如何联合不同的领域，并使我们能够构建不仅更准确，而且在物理上更真实、更可解释、更强大的模型。

### 教网络学习物理定律

教机器了解世界最直接的方法，就是让它尊重我们用来描述世界的语言：[偏微分方程](@article_id:301773) (PDE)。这些方程，从热流定律到[流体动力学](@article_id:319275)，是现代科学的基石。一种被恰当地命名为[物理信息神经网络](@article_id:305653) (PINN) 的新型模型，正是这样做的。

想象一下，你想预测一块薄金属板上的[稳态温度分布](@article_id:355252)。你知道边缘的温度是固定的（边界条件），并且你知道在板的内部，温度必须服从[拉普拉斯方程](@article_id:304121) $\nabla^2 u = 0$。与其仅仅用一个包含大量已解温度点的数据集来训练网络，我们可以直接针对问题本身进行训练。我们构建一个包含两部分的[损失函数](@article_id:638865)。第一部分如我们所料：它检查网络的预测是否与边界上的已知温度相匹配。第二部分，也是至关重要的部分，检查网络的输出是否在板内部*满足[拉普拉斯方程](@article_id:304121)*。这部分的损失是 PDE 的“[残差](@article_id:348682)”——即网络的输出距离使方程等于零有多远。网络因此必须学习一个既能正确匹[配边](@article_id:335865)界条件，又能在其他任何地方都遵守物理定律的温度图 [@problem_id:2126359]。

这是一个非常灵活的思想。你的问题是随时间变化的吗，比如热量随时间在杆中传播？没问题。我们只需将时间变量添加到网络的输入中，并使用热方程 $u_t = \alpha u_{xx}$ 来定义[残差](@article_id:348682)损失。我们甚至可以在边界上指定不同类型的规则，例如一端是固定温度（[狄利克雷条件](@article_id:297547)），另一端是固定的热流率（[诺伊曼条件](@article_id:344812)，这涉及到解的[导数](@article_id:318324)）。这些规则中的每一个都只是我们复合[损失函数](@article_id:638865)中的另一个项，是网络必须满足的检查清单 [@problem_id:2403429]。

而且这并不仅限于物理学。[量化金融](@article_id:299568)中的“游戏规则”也常常以 PDE 的形式表达。著名的 Black-Scholes 方程描述了金融期权的价值如何随时间演变。为了给期权定价，我们可以训练一个 PINN，其[损失函数](@article_id:638865)包括一个针对 Black-Scholes PDE 本身的项，一个针对期权在其到期日已知价值的项（终端条件），以及针对其在极端资产价格下行为的项（边界条件）。通过最小化这个损失，网络无需看到复杂的解析公式，就能直接从金融模型的基本原则中学习，从而找到期权的公允价值 [@problem_id:2126361]。

### 从学生到科学家：发现未知定律

到目前为止，我们一直扮演着教师的角色，为网络提供已知的物理定律。但我们能反转这个剧本吗？网络能成为一名科学家，从实验数据中发现未知的定律吗？答案惊人地是肯定的。

假设我们有一项新实验的数据——比如，一种化学浓度随时间演变的数据——但我们不知道支配它的确切 PDE。我们可能有一个假设，即该定律是几个可能物理过程的组合：一些扩散 ($c_5 u_{xx}$)，一些输运 ($c_4 u_x$)，以及一些反应 ($c_1 u + c_2 u^2 + \dots$)。系数 $c_1, c_2, \dots$ 是未知的。我们可以设置一个神经网络来近似浓度，但这一次，我们让未知系数 $c_i$ 成为可训练的参数，就像网络自身的权重一样。

现在的[损失函数](@article_id:638865)变成了一场有趣的平衡游戏。一个项推动网络去拟合实验数据点。另一个项，即 PDE [残差](@article_id:348682)，推动网络去遵守假设的方程。至关重要的是，随着网络权重的调整，系数 $c_i$ 也在调整。优化器试图找到系数的最佳值，以使网络*既能*拟合数据，*又能*满足方程结构。如果某个项是不需要的，它的系数将被驱使为零。通过这种方式，该过程可以执行“[模型选择](@article_id:316011)”，直接从数据中发现最可信的控制方程 [@problem_id:2094871]。这将[损失函数](@article_id:638865)的作用从一个简单的误差度量提升为科学发现的引擎。

### 编织现实的纤维：材料与分子中的约束

自然法则并不总是以整洁的 PDE 形式写就。有时它们是更广泛的原则，是对物理上可能与不可能的约束。我们多功能的损失函数也能编码这些原则，充当模型预测的“现实检验”。

这在[材料科学](@article_id:312640)中是一个巨大的挑战。机器学习模型可能会预测一种具有惊人性能的新合金，但如果你尝试制造它，它可能就散架了。一种材料要稳定，其基本要求之一是它的自由能面必须是凸的。非凸区域意味着不稳定性，材料会自发地从这种状态改变。所以，当我们训练一个[神经网络](@article_id:305336)来预测材料的自由能时，我们可以在其[损失函数](@article_id:638865)中加入一个惩罚项。该项会“扫描”网络输出的二阶[导数](@article_id:318324) $\frac{d^2G}{dx^2}$。只要该[导数](@article_id:318324)为负（违反凸性），就会增加一个惩罚。因此，网络被训练来避开这些不稳定区域，不仅学习预测能量值，还学习尊重[热力学](@article_id:359663)的基本定律 [@problem_id:90246]。

我们甚至可以做得更具体。对于任何固体，我们知道在其稳定、平衡的体积 $V_0$ 处，压力 $P = -\frac{dE}{dV}$ 必须为零，其抗压缩性由一个特定值——体模量 $B_0$ 给出。当训练网络预测材料的能量-体积曲线时，我们可以在损失函数中加入两个简单但强大的项。一个项惩罚在 $V_0$ 处的任何非零能量梯度（压力），另一个项惩罚任何与已知体模量 $B_0$ 的偏差。这些基于物理的惩罚引导网络生成一条不仅能很好地拟合数据点，而且在曲线上最重要的点具有物理意义的曲线 [@problem_id:90090]。

同样的设计理念可以延伸到原子尺度。在[药物发现](@article_id:324955)中，一个关键任务是预测药物分子（配体）将如何与目标蛋白结合。一个朴素的模型可能会预测一个原子间距离不符合物理现实的结合姿态，从而产生巨大的空间[位阻排斥](@article_id:348494)。我们可以通过在损失函数中添加一个基于物理的能量项来引导模型。使用标准的[分子力学力场](@article_id:354543)，如 Lennard-Jones 和库仑势，我们可以计算网络预测的原子坐标的势能。这个能量项充当一个惩罚。如果网络提出的姿态中原子发生碰撞，势能会变得巨大，损失也会巨大，优化器就会学会避免这种情况。因此，模型被训练来寻找低能量、物理上合理的结合构型 [@problem_id:1426745]。

要一睹这种方法威力的惊人展示，可以考虑对现代[半导体器件](@article_id:323928)进行建模。电子的行为由耦合的薛定谔-泊松方程的复杂相互作用所支配。可以构建一个 PINN 来求解这个系统，方法是创建一个庞大的复合[损失函数](@article_id:638865)。它包含针对每个电子态的薛定谔方程[残差](@article_id:348682)的项，一个针对泊松方程[残差](@article_id:348682)的项，所有边界条件的项，甚至还有强制执行量子力学规则（如[波函数](@article_id:307855)的[归一化](@article_id:310343)和正交性）的项。每一个约束，每一条物理原理，都被翻译成一个优化器试图最小化的数学表达式，从而让网络解开这个极其复杂的耦合系统 [@problem_id:90141]。

### 贯穿各学科的统一线索

这个思想的美妙之处在于其普适性。它是一种超越任何单一领域的思维方式。

在控制理论中，工程师可能会设计一个[神经网络](@article_id:305336)来控制一个[磁悬浮](@article_id:339464)系统。主要目标是让物体遵循一个参考轨迹，因此需要一个标准的跟踪误差损失。但这里有一个问题：电磁铁会消耗能量。一个激进的控制器可能跟踪得完美无瑕，但会消耗大量能源。解决方案？在损失函数中添加一个惩罚大控制输入的项。优化器现在被迫寻找一种平衡——一个既能良好跟踪又高效运行的控制器。这与添加约束性惩罚的原理完全相同，只是应用于工程上的权衡，而非物理定律 [@problem_id:1595332]。

这种思维方式甚至帮助我们解决纯粹[算法](@article_id:331821)领域的问题。在[自然语言处理](@article_id:333975)中，一个常见的任务是拼写纠错。衡量一个拼写错误的单词和正确单词之间误差的一个好方法是“[编辑距离](@article_id:313123)”——即所需单字符插入、删除或替换的次数。但这个度量是通过一个涉及不可微的 `min` 操作的[动态规划](@article_id:301549)[算法](@article_id:331821)计算的，这会使基于梯度的训练停滞不前。解决方案要么是用一个平滑、可微的版本（使用“soft-min”函数）来近似这个损失，要么是使用[强化学习](@article_id:301586)中的技术（如 REINFORCE [算法](@article_id:331821)）来处理不可微的奖励。无论哪种情况，我们都是在创造性地修改或处理损失函数，以便直接优化我们真正关心的那个度量，这展示了这种理念的广度 [@problem_id:3231081]。

也许最深刻的联系在于统计物理与[神经网络架构](@article_id:641816)本身之间。Ising 自旋玻璃（物理学中一个基础的磁性模型）的能量函数形式为 $E = - \sum J_{ij} s_i s_j$。一个基础的[神经网络](@article_id:305336)模型——玻尔兹曼机，其“能量”或损失函数形式为 $L = - \frac{1}{2} \sum w_{ij} s_i s_j$。除了一个简单的因子 2 外，它们是相同的函数。物理耦合 $J_{ij}$ 直接映射到网络权重 $w_{ij}$。在这里，我们甚至不需要*添加*一个基于物理的损失；[损失函数](@article_id:638865)*本身*就是一个物理系统的能量。这种美妙的对应关系暗示了支配自然界和人工智能中集体行为的原则之间存在着深刻而富有成果的统一性 [@problem_id:2373926]。

从求解 PDE 到发现它们，从强制执行热力学稳定性到寻找药物结合的正确方式，自定义[损失函数](@article_id:638865)是我们与模型谈论世界的语言。它将模型从简单的模仿者转变为可以被教授规则的学生。它证明了这样一个思想：最强大的学习不仅来自数据，更来自数据与对底层原理深刻理解的结合。