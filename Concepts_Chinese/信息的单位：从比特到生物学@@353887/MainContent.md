## 引言
什么是信息？虽然我们日常使用这个词，但它的科学含义要精确和强大得多。它关乎的不是消息的语义内容，而是它所提供的不确定性的减少。本文旨在探讨我们如何量化信息这一根本问题，将其从一个抽象概念转变为一个可测量的物理量。我们将首先探索其核心原理和机制，定义信息的基本“原子”——比特，并建立熵这一关键概念。然后，我们将跨越科学领域，见证这些思想出人意料且深刻的应用，揭示信息论如何为物理学、生物学和计算机科学等不同领域提供一种通用语言。通过理解信息的单位，我们解锁了一个观察宇宙本身的新视角。

## 原理与机制

想象一下，你是一部老电影里的间谍。一封密信抵达。它可能是十六个可能计划中的一个。在你阅读之前，你的世界是一个由十六种可能性、十六个笼罩在迷雾中的未来构成的景象。你打开信封，信中写着“G计划”。瞬间，迷雾散去，十五种未来消失，一个现实清晰起来。你刚刚获得了多少清晰度？多少不确定性被消除了？这是信息论的核心问题。它关乎的不是“G计划”的*含义*，而是可能性数量的纯粹减少。

### 信息的原子：比特

让我们简化一下。忘掉十六个计划。想象一枚被遮住的硬币，正在抛掷。是正面还是反面？两种可能性。当你看到结果的那一刻，你就解决了一个根本性的不确定性。这种对一个“是/否”问题的解答，一个在两个[等可能结果](@article_id:323895)之间的选择，就是信息的基本原子。我们称之为**比特**（bit）。

现在，让我们回到间谍的场景。对于十六个等可能的计划，消息“G计划”包含多少比特的信息？你可以把它想象成一个“20问”游戏。你的第一个问题可能是：“它是A到H计划中的一个吗？” 回答这个问题会将可能性减半，给你一比特的信息。如果答案是肯定的，你再问：“它是A到D计划中的一个吗？” 又是一比特。你需要问四个这样的问题才能精确定位到“G计划”。所以，收到那条消息消除了**4比特**的不确定性 [@problem_id:1629825]。

注意其中的数学规律：$16 = 2^4$。比特数就是[2的幂](@article_id:311389)指数，其结果是选项的总数。这就是对数的本质。从$N$个等可能的结果中识别出一个所获得的[信息量](@article_id:333051)$I$为：

$$ I = \log_{2}(N) $$

这个对数尺度是关键。它意味着信息以一种极其简单的方式累加。解决一次抛硬币的不确定性给你$\log_2(2) = 1$比特。解决两次独立抛硬币的不确定性给你$\log_2(4) = 2$比特。[信息量](@article_id:333051)直接相加。

### 惊奇，惊奇！

但如果各种可能性不是等概率的呢？想象你在读一本英文小说。如果你看到的下一个字母是'E'，你不会很惊讶，因为它是最常见的字母。但如果下一个字母是'Z'，你可能会停顿一下。这是一个罕见的事件。常见的事件是可预测的，因此携带的信息很少。罕见、令人惊奇的事件则携带大量信息。

信息论之父Claude Shannon完美地捕捉了这一直觉。一个概率为$p(x)$的特定结果$x$的信息内容，或称**惊奇度**（surprisal），定义为：

$$ I(x) = -\log_{2}(p(x)) $$

注意，对于一个很小的概率$p(x)$，它的对数是一个很大的负数，所以$-\log_{2}(p(x))$是一个很大的正数。一个非常不可能的事件携带很多比特的信息。例如，字母'Z'在英语中出现的概率大约是$0.00074$。因此，观察到一个'Z'的信息内容是$-\log_{2}(0.00074) \approx 10.4$比特 [@problem_id:1632010]。这单个字母所消除的不确定性，比连续抛掷十多次硬币还多！

### 不确定性的度量：熵

现在我们可以衡量任何单个事件的惊奇度。但如果我们想描述信息源本身的特性呢？一个只产生字母'A'的信源是完全可预测的，平均产生零信息。一个以等概率随机吐出字母的信源则高度不可预测。我们需要一种方法来衡量一个信源的*平均*惊奇度，或其固有的不确定性。这个度量被称为**熵**（entropy），用$H$表示。

我们通过计算每个可能结果的信息$I(x_i)$，用该结果的概率$p(x_i)$进行加权，然后将它们全部相加来得到熵：

$$ H = \sum_{i} p(x_i) I(x_i) = -\sum_{i} p(x_i) \log_{2}(p(x_i)) $$

熵是整个科学领域最深刻和最有用的概念之一。对于[通信工程](@article_id:335826)师来说，数据源的熵代表了一个根本的极限。Shannon的**[信源编码定理](@article_id:299134)**证明，你无法将一个信源的数据平均压缩到每个符号少于该[信源熵](@article_id:331720)的比特数 [@problem_id:1657635]。这是[数据压缩](@article_id:298151)的绝对、不可打破的极限。熵告诉你正在产生的“真实”[信息量](@article_id:333051)。

在这里，我们偶然发现了科学中最美丽的统一之一。在19世纪，像Ludwig Boltzmann和J. Willard Gibbs这样的物理学家为物理系统（如一容器气体）的熵建立了一个公式。**[吉布斯熵](@article_id:314565)**（Gibbs entropy）是：

$$ S = -k_{B} \sum_{i} p_{i} \ln(p_{i}) $$

看起来熟悉吗？它与Shannon的公式完全相同，只是用自然对数代替了以2为底的对数，并乘以一个物理常数，即**[玻尔兹曼常数](@article_id:302824)**（Boltzmann constant）$k_B$ [@problem_id:1967993]。这并非巧合。两个公式都在测量同一个基本的东西：观察者对系统状态的不确定性。无论是气体分子的微观状态，还是消息中的下一个符号，熵都量化了我们的无知。[信息是物理的](@article_id:339966)。

### 三种对数的故事：比特、纳特和哈特利

不同对数底（$\log_2$ 对比 $\ln$）的出现，引出了单位的问题。就像我们可以用米或英尺来测量距离一样，我们也可以用不同的单位来测量信息。对数底的选择仅仅是设定了单位：

*   **以2为底**：单位是**比特**（bit）。这是计算机和二进制逻辑的自然语言。
*   **以e为底**：单位是**纳特**（nat，来自“natural unit”）。这是微积分和理论物理的自然语言，其中数字$e$无处不在。
*   **以10为底**：单位是**哈特利**（Hartley，或ban，或decimal digit）。这对应于单个十进制数字中的信息。

一个系统的不确定性量是固定的；其熵的数值仅仅取决于我们用来测量的单位 [@problem_id:2472839]。它们之间的转换就像任何其他单位转换一样简单，使用对数的换底公式：$H_{\text{bits}} = H_{\text{nats}} / \ln(2)$。

这不仅仅是一个学术练习。在[生物信息学](@article_id:307177)领域，科学家使用像BLAST这样的工具来寻找DNA或蛋白质序列之间的相似性。比对的原始分数通常使用以$e$为底最自然的公式计算，得出以纳特为单位的分数。然而，为了使结果易于解释，这个分数会通过除以$\ln(2)$转换为比特。这个**比特分数**（bit score）有一个非常直观的含义：比特分数每增加1点，比对的统计显著性就增加一倍 [@problem_id:2375700]。这个简单的单位转换将原始的数学输出变成了一个强大的发现工具。

### 信息定律

信息，就像能量一样，不仅仅是一个记账工具；它遵循基本定律。

首先，**你不能无中生有地创造信息**。想象你有一份丰富的病人健康记录数据集$Y$。你处理它以创建一个更小的、匿名的数集$Z$，也许是为了与研究人员共享。**[数据处理不等式](@article_id:303124)**（Data Processing Inequality）指出，匿名数据集$Z$包含的关于原始病人状况$X$的[信息量](@article_id:333051)，永远不会大于原始数据集$Y$所包含的信息量。处理只能保存或丢失信息；它永远不能创造信息 [@problem_id:1613394]。在一个处理链$X \to Y \to Z$中，必须始终满足$I(X;Y) \ge I(X;Z)$。这是信息流的一个基本定律。

其次，信息传输有速度限制。当我们通过嘈杂的[信道](@article_id:330097)发送数据时，比如深空探测器通过太阳等离子体传输信号，一些比特可能会丢失。一个简单的模型是**[二进制删除信道](@article_id:330981)**（Binary Erasure Channel），其中每个比特要么完美到达，要么以一定的概率$\epsilon$被删除。你能可靠地发送数据的最大速率是多少？[Shannon的信道编码定理](@article_id:338714)给出了答案：**信道容量**（channel capacity）$C$。对于这个[信道](@article_id:330097)，容量非常直观和简单：$C = 1-\epsilon$ [@problem_id:1604518]。最大可靠速率就是成功通过的比特的比例。你不可能做得更好。

最后，我们回到起点，再次看到信息是如何深深地编织在物理世界的结构中。考虑一个现代工程问题，涉及从嘈杂的测量$Y$中估计信号$X$。一个称为I-MMSE恒等式的深刻结果将[互信息](@article_id:299166)$I(X;Y)$与最佳可能估计的误差联系起来。乍一看，这个方程似乎违反了量纲分析的规则——无量纲信息的[导数](@article_id:318324)等于一个单位为伏特平方的量！这个悖论只有在你意识到为了使方程成立，抽象的“[信噪比](@article_id:334893)”参数本身必须带有能使一切保持一致的物理单位时才能解决 [@problem_id:1654327]。信息，当在物理系统中测量时，不是一个脱离实体的幽灵。它受到与能量、动量和[电荷](@article_id:339187)同样严格的定律的约束。它是一个真正的物理量，其原理支配着从秘密消息的低语到宇宙的轰鸣的一切。