## 应用与跨学科联系

我们花了一些时间来理解[小批量随机方法](@article_id:641137)的机制——即注入一定剂量的随机性如何让我们在巨大的优化问题上取得快速进展。人们可能很容易将其视为一种聪明但狭隘的技巧，一个专属于机器学习领域的计算工具。但这就像只看到一个齿轮，却未能看到整个宇宙的时钟装置。小批量方法背后的原理——信号与噪声之间的权衡、采样的艺术，以及通过部分信息在复杂景观中导航的动力学——并不仅仅是关于训练神经网络。它们是基本思想，在众多科学学科中产生共鸣。现在，让我们踏上一段旅程，看看这个“简单”的采样理念[能带](@article_id:306995)我们走多远。

### 在机器学习内部精炼技艺

在我们涉足太远之前，让我们先欣赏一下小批量方法在其原生环境中的深度和精妙之处。即便在这里，天真的应用也是不够的；真正的精通需要对所涉及的权衡有更深的理解。

考虑一下常见且关键的[类别不平衡](@article_id:640952)问题。想象一下，你正在构建一个模型来检测一种罕见疾病，而你的数据中只有0.01%是患病患者的样本。如果你[随机抽样](@article_id:354218)数据，你的小批量中几乎永远不会包含该疾病的例子，你的模型将学会一直预测“健康”。对抗这种情况的一个常用策略是平衡采样：在每个小批量中，我们刻意从稀有类别中过采样。但这个解决方案本身也带来了一些有趣的后果。通过从一个非常小的少数类样本池中反复抽取，我们引入了很高的“重复率”。模型在一个 epoch 内会一遍又一遍地看到同样的几个疾病样本。这种强烈的重复增加了模型简单地记住这些特定样本而不是学习疾病可泛化特征的风险，这是[过拟合](@article_id:299541)的一个典型案例。它还使优化过程缺乏多样化的梯度，使得学习路径的探索性降低 [@problem_id:3127134]。这揭示了一种美妙的[张力](@article_id:357470)：用一种采样策略解决一个问题（[类别不平衡](@article_id:640952)），却引入了一个我们同样必须注意的新问题（[过拟合](@article_id:299541)）。

小批量方法与其驱动的[算法](@article_id:331821)之间的关系并非单向的。有时，[算法](@article_id:331821)本身会进化以在小批量创造的随机环境中茁壮成长。这种[共同进化](@article_id:312329)的一个绝佳例子是[深度神经网络](@article_id:640465)中的[批量归一化](@article_id:639282)（Batch Normalization）层。想象一下，你正试图合并来自两个不同实验室的细胞信息数据集。由于设备和协议的差异，一个实验室的数据可能系统性地比另一个实验室的更亮或有偏移——这种现象被称为“[批次效应](@article_id:329563)”。如果我们混合这些数据，我们的优化器在由一个或另一个实验室主导的小批量之间跳跃时，会不断失去平衡。[批量归一化](@article_id:639282)提供了一个优雅的解决方案。在每个小批量内部，它计算激活值的均值和方差，并用它们来标准化数据，有效地将该特定批次的所有数据点置于一个共同的尺度上。它将小批量从一个潜在的不稳定源泉转变为稳定化的工具，使下游网络对困扰现实世界科学数据（如基因组学数据）的各种偏移和缩放具有鲁棒性 [@problem_id:2373409]。

### 将速度与复杂性相结合

最简单的随机方法，[随机梯度下降](@article_id:299582)（SGD），就像一个蒙着眼睛的徒步者，大致朝着下坡的方向走。它出奇地有效，但如果我们想使用更复杂的工具——比如一张地形图和一个指南针——来更快地找到谷底呢？在优化中，这对应于使用那些近似损失[曲面](@article_id:331153)曲率的二阶信息的方法，比如著名的 [L-BFGS](@article_id:346550) [算法](@article_id:331821)。

但是，当我们试图将来自小批量的原始、充满噪声的梯度与 [L-BFGS](@article_id:346550) 的精密机制配对时，我们遇到了一个严重的问题。[L-BFGS](@article_id:346550) 更新的一个关键部分涉及计算梯度的变化量，$y_k = \nabla f(x_{k+1}) - \nabla f(x_k)$。对此最直接的随机版本将是 $y_k = g(x_{k+1}, \zeta_{k+1}) - g(x_k, \zeta_k)$，其中梯度是在两个*独立*的小批量 $\zeta_k$ 和 $\zeta_{k+1}$ 上计算的。因为这两个批次的噪声是独立的，它们的方差会相加。得到的向量 $y_k$ 噪声如此之大，以至于常常无法满足一个基本的稳定性条件，即曲率条件（$s_k^T y_k \gt 0$）。[算法](@article_id:331821)对景观曲率的近似变得不稳定，优化过程可能会朝着一个完全错误的方向飞去 [@problem_id:2184532]。

这是否意味着速度和复杂性注定要分道扬镳？完全不是！稍加思索就会发现一种更聪明的采样方式。如果我们不用两个不同的小批量，而是用*同一个*小批量来评估 $x_k$ 和 $x_{k+1}$ 处的梯度会怎么样？这种被称为使用“共同随机数”的技术，在两个[梯度估计](@article_id:343928)的噪声之间引入了正相关。现在，当我们计算差值时，一大部分噪声会相互抵消。这个简单的技巧极大地降低了 $y_k$ 的方差，有助于保持曲率条件，并使得像 [L-BFGS](@article_id:346550) 这样的方法能够成功地适应于随机环境 [@problem_id:3142813]。这种智能地构建随机估计的主题延伸到许多其他高级优化框架，包括 Gauss-Newton 方法 [@problem_id:3232822] 和[交替方向乘子法](@article_id:342449)（ADMM）[@problem_id:3096694]，展示了一个普适原则：要驯服随机性，必须首先理解其结构。

最近的进展已将这一思想推向其逻辑结论。虽然噪声有助于逃离糟糕的局部最小值，但它的持续存在会阻止[算法](@article_id:331821)精确地稳定在谷底。一类新的“[方差缩减](@article_id:305920)”[算法](@article_id:331821)，如 SVRG 和 SAGA，应运而生。这些方法使用巧妙的方式来估计随机梯度的噪声并将其减去，从而有效地创造出一个“[去噪](@article_id:344957)”的梯度。SVRG 通过周期性地计算一个完整的、确定性的梯度作为参考点来实现这一点，而 SAGA 则巧妙地为每个数据点维护一个过往梯度的运行表，以在完全不需要计算完整梯度的情况下达到类似的效果 [@problem_id:3167437]。这些[算法](@article_id:331821)实现了两全其美：既有 SGD 每步的低[计算成本](@article_id:308397)，又有全批量方法的快速[线性收敛](@article_id:343026)率。

### 在更广阔的科学世界中的回响

我们一直将“批次”数据视为图像或用户记录的子集，但这个概念远比这更通用。当我们超越传统机器学习的界限时，它的力量和局限性就变得最为清晰。

在[计算化学](@article_id:303474)中，科学家通过寻找使其势能 $E(\mathbf{R})$ 最小化的原子构型 $\mathbf{R}$ 来优化分子的几何结构。这个能量的梯度 $\nabla E(\mathbf{R})$ 对应于作用在原子上的力。有人可能会想：我们能否通过只计算一个“小批量”原子的力来加速这个过程？答案是响亮的“不”。分子的势能是其*所有*原子位置的高度纠缠函数；它不是独立的、每个原子贡献的简单总和。计算一部分原子上的力会得到一个毫无希望的、有偏的且物理上无意义的总[梯度估计](@article_id:343928)。这给我们一个关键的教训：小批量采样的有效性建立在我们的全局目标是准独立部分的一个平均或总和的假设之上。它提醒我们，没有工具是万能的，我们必须始终尊重我们试图解决问题的底层结构 [@problem_id:2463012]。

然而，在其他科学领域，这个类比却完美成立。考虑一下前沿领域[物理信息神经网络](@article_id:305653)（[PINNs](@article_id:305653)），其中[神经网络](@article_id:305336)被用来[求解微分方程](@article_id:297922)。为了训练这些网络，通常需要在一个连续域上评估网络在多大程度上满足了底层的物理定律（例如，[能量守恒](@article_id:300957)）。这通常通过使用一组求积点来近似域上的积分来完成。这些成千上万甚至数百万的求积点就成了我们的“数据集”。每个点都对总[损失函数](@article_id:638865)贡献一小部分。在这种情况下，小批量方法的所有逻辑都完全适用。一次性使用所有求积点进行训练（全批量）可能太慢或需要太多内存。取而代之的是，我们可以在每一步中采样一个小批量的求积点。其权衡是完全相同的：更小的批次减少了内存使用并允许更快的硬件吞吐量，但代价是梯度带有噪声 [@problem_id:2668923]。在这里，我们看到“数据点”这个抽象概念被优美地转化为物理问题中的“空间中的一个点”。

这就引出了我们最后一个，或许也是最深刻的联系。科学已知的最宏大的优化过程是什么？有人可能会说是[达尔文进化论](@article_id:297633)本身。一个[生物种群](@article_id:378996)在一个“[适应度景观](@article_id:342043)”中探索，其中适应度对应于[繁殖成功率](@article_id:346018)。突变引入随机变异，而自然选择倾向于将种[群平均](@article_id:368245)推向更高适应度的山峰。[随机梯度下降](@article_id:299582)的数学模型是否可能作为进化的模型？

在某些方面，这个类比非常诱人。在一个大型、简单的种群中，可以证明平均基因型的预期变化会沿着适应度的梯度进行，很像 SGD 更新沿着损失的梯度进行 [@problem_id:2373411]。进化中变化的环境类似于机器学习中变化的数据分布，两者都呈现出非平稳的优化问题。然而，这个类比也有深刻的局限性，这些局限性教会了我们很多。进化中的“噪声”不仅仅是采样方差；它还包括[遗传漂变](@article_id:306018)，一种根本不同的随机力量。更重要的是，进化作用于一个并行探索景观的解*群体*，而有性重组通过混合来自不同成功个体的性状，允许巨大的飞跃。单轨迹的 SGD 路径没有与这些现象直接对应的东西。从这个角度看，进化也许更类似于基于群体的优化算法，如[遗传算法](@article_id:351266)或[集成方法](@article_id:639884)，而不是简单的 SGD [@problem_id:2373411]。

于是，我们的旅程就此结束。我们从一个加速计算的简单技巧开始，最终思考生命的根本机制。[小批量随机方法](@article_id:641137)的故事证明了一个单一、统一思想的力量：在一个过于广阔和复杂以至于无法一览无余的世界里，最有效的前进道路往往是通过迈出微小、快速且时而随机的步伐来找到的。