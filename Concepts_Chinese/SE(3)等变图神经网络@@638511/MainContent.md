## 引言
支配我们宇宙的物理定律在根本上是对称的；无论观察者在三维空间中的位置或朝向如何，这些定律都保持不变。对于旨在模拟分子或材料等物理系统的机器学习模型而言，理解这种对称性不仅有益，而且至关重要。标准的[神经网](@entry_id:276355)络通常难以从数据中学习这些规则，它们需要海量数据集，并且无法保证物理上的一致性。这在人工智能的数据驱动方法与物理学的原则性世界之间造成了巨大的知识鸿沟。

SE(3) [等变图神经网络](@entry_id:749065)（GNNs）通过将三维空间的对称性直接构建到其架构中，提供了一种优雅的解决方案。这些模型并非去学习几何规则，而是生来就知晓这些规则。本文将深入探讨这类强大的模型。在第一部分“原理与机制”中，我们将解析不变性和[等变性](@entry_id:636671)的核心概念，并探讨使这些网络能够以几何上一致的方式“思考”的架构组件。随后，在“应用与跨学科联系”部分，我们将游历从化学和药物发现到[材料科学](@entry_id:152226)和粒子物理学的不同科学领域，见证这一统一的原则如何释放出新的预测能力和洞见。

## 原理与机制

想象一下，你正在尝试向一台非常智能但又非常刻板的计算机描述物理定律。你想让它理解一个分子如何行为——它的原子如何[振动](@entry_id:267781)，它们如何相互推拉，以及它的总能量是多少。你必须教给它的最基本规则，是一条深刻而优美的对称性原理：物理定律不依赖于你的观察视角。无论你身处伦敦还是东京，无论你是正着、倒着还是以任何其他朝向观察分子，这些定律都是相同的。这是所有现代物理学的基石。我们的目标是将这种对对称性的深刻理解直接构建到机器学习模型的架构之中。

### 对称性的语言：[不变性](@entry_id:140168)与[等变性](@entry_id:636671)

让我们从一个简单的思想实验开始。想象一个水分子 $\text{H}_2\text{O}$。它具有一定的[势能](@entry_id:748988)，这是一个描述其稳定性的单一数值。如果我们把这个分子在空间中旋转，它的能量会发生什么变化？什么都不会变。能量是一个**标量**，它在[旋转操作](@entry_id:140575)下保持不变，即具有**[不变性](@entry_id:140168)**。这是分子内部几何结构的属性，而非其在空间中的朝向。

但现在考虑作用在原子上的力。每个力都是一个**矢量**——它既有大小又有方向。如果我们旋转分子，作用在原子上的力也会随之旋转，在我们的实验室坐标系中指向新的方向。这些力不会保持不变，它们与分子本身同步变换。这种性质被称为**[等变性](@entry_id:636671)**。

这种区别不仅是语义上的，它既是核心挑战，也是关键洞见。
*   **不变性（对标量而言）：** 当系统变换时，性质不发生改变。$f(R\mathcal{X}) = f(\mathcal{X})$。能量是不变的。
*   **[等变性](@entry_id:636671)（对矢量/张量而言）：** 性质以与系统*相同的方式*进行变换。$f(R\mathcal{X}) = R f(\mathcal{X})$。力是等变的。分子的[永久偶极矩](@entry_id:163961)，一个从负电荷中心指向正电荷中心的矢量，也是等变的 [@problem_id:2903829]。

任何旨在模拟物理世界的模型都必须遵循这种基本的对称性语法。它必须能够预测不变的能量和等变的力。

### 与其教网络学习物理，不如让它生而知之？

有人可能会问：“一个足够大且复杂的[神经网](@entry_id:276355)络难道不能仅从数据中学习这些规则吗？”理论上，我们可以向模型展示一百万个不同随机朝向的水分子，并希望它能找出其潜在的模式。这种被称为[数据增强](@entry_id:266029)的方法，有点像教孩子乘法时，不教他们乘法表，而是给他们看数百万个例子。这种方法效率极低，更糟糕的是，它不能保证成功。模型可能在与其见过相似的朝向上表现良好，但在新的朝向上仍可能犯下奇怪的、不符合物理规律的错误 [@problem_id:2479740]。

SE(3) [等变网络](@entry_id:143881)的哲学是选择一条更优雅的路径。我们不是强迫模型去学习对称性规则，而是将这些规则构建到其结构本身之中。我们设计的机器，其本质就决定了它无法违反这些物理定律。它生来就懂得[三维几何](@entry_id:176328)的语言。

### 构建一台等变机器

我们如何构建这样一台机器？其奥秘在于几个巧妙的架构原则，它们共同确保了网络能以几何上一致的方式进行“思考”。

#### 用相对关系表达

首先，为了使我们的模型对空间中的绝对位置不敏感（即具有[平移不变性](@entry_id:195885)），我们让它只考虑原子的相对位置。它处理的是原子间的[位移矢量](@entry_id:262782) $\mathbf{r}_{ij} = \mathbf{r}_j - \mathbf{r}_i$，而不是它们的绝对坐标。如果整个系统被一个矢量 $\mathbf{t}$ 平移，这些相对矢量保持不变，模型的预测也同样保持不变。这个简单的技巧优雅地解决了平移对称性问题 [@problem_id:2765008]。

#### 作为几何对象的特征

在标准的[神经网](@entry_id:276355)络中，信息被处理为一串普通的数字。而在一个等变 GNN 中，特征本身就是几何对象，在旋转下具有明确定义的属性。我们可以将它们视为具有“类型”：
*   **0 型特征：** 这些是标量，如原子质量或[原子电荷](@entry_id:204820)的学习表示。它们在旋转下是不变的。
*   **1 型特征：** 这些是矢量，如相对位置矢量 $\mathbf{r}_{ij}$。它们是等变的，意味着它们随系统一起旋转。
*   **更高类型特征（张量）：** 这些表示更复杂的几何信息，如[四极矩](@entry_id:157717)或化学键的朝向。

网络在其各层中维护和更新这些几何特征，始终跟踪每个特征应该如何变换。

#### 等变的“对话”

网络的核心由多层“[消息传递](@entry_id:751915)”组成，原子通过与邻居交换信息来构建其局部环境的图像。为了使这个过程具有[等变性](@entry_id:636671)，这些“对话”必须遵循严格的几何规则。

这是通过使用具有明确几何意义的操作来组合特征实现的。最基本的操作是**[张量积](@entry_id:140694)**。虽然其数学可能很复杂，但直观理解很简单：它是一本关于如何组合几何对象以创造新对象的规则手册。例如，两个矢量（1 型）的[张量积](@entry_id:140694)可以产生：
*   一个标量（0 型），通过计算它们的**[点积](@entry_id:149019)**。网络就是通过这种方式生成像能量这样的[不变性](@entry_id:140168)量。
*   另一个矢量（1 型），通过计算它们的**叉积**。这会创建新的等变矢量。
*   一个更高阶的张量（2 型），捕捉它们耦合的朝向信息。

为了描述邻居的角向[排列](@entry_id:136432)，这些网络使用一种强大的数学工具，称为**球谐函数**（$Y_l^m$）。你可以将球谐函数看作是球面上的一套完备的“几何[基函数](@entry_id:170178)”。它们就像是三维空间中的正弦和余弦，使网络能够表示中心原子周围邻居的任何可能的角向[分布](@entry_id:182848)。至关重要的是，这些[球谐函数](@entry_id:178380)在旋转下以可预测的方式变换，从而允许网络将局部原子环境的形状编码到其几何特征中 [@problem_id:3455800] [@problem_id:2479740]。通过将网络的带类型特征与相对位置矢量的球谐函数相结合，模型可以进行丰富的、感知朝向的对话，并严格遵守[旋转对称](@entry_id:137077)定律。

### 对称性的美妙推论

将对称性嵌入到网络架构中，不仅是一种理论上的纯粹之举，它还带来了深刻而实际的益处，使得这些模型异常强大。

#### 梯度的优雅与[能量守恒](@entry_id:140514)

其中一个最优雅的结果关乎能量与力之间的关系。在物理学中，[力是势能的负梯度](@entry_id:168705)（$\mathbf{F}_i = -\nabla_{\mathbf{r}_i} E$）。可以这样表示的[力场](@entry_id:147325)称为**保守**[力场](@entry_id:147325)，意味着[孤立系统](@entry_id:159201)的总能量是恒定的。如果我们设计一个[等变网络](@entry_id:143881)来预测一个单一的、物理上正确的**不变**标量能量 $E$，我们就可以通过[自动微分](@entry_id:144512)免费获得力。由此产生的力在数学上保证是**等变的**，并且该[力场](@entry_id:147325)也保证是保守的 [@problem_id:2765008]。该模型不仅仅是学习力，它学习的是一个[势能面](@entry_id:147441)，并从该[势能面](@entry_id:147441)推导出一致且[能量守恒](@entry_id:140514)的力。

#### 分辨左右：手性之谜

考虑两个互为镜像的分子，就像你的左手和右手。它们被称为**[对映异构体](@entry_id:149008)**。它们拥有完全相同的原子，以完全相同的方式连接，并且它们原子间的所有距离和角度也都相同。一个只使用这些不变距离作为输入的模型，在根本上对**手性**是盲视的；它无法区分左手分子和右手分子 [@problem_id:2903829]。

然而，镜像反射是一种“非正常”旋转。它不是你在三维空间中通过任何连续旋转可以实现的。因为我们的网络是专门为对[正常旋转](@entry_id:141831)群 SE(3) 等变而构建的，所以它们不受限于必须同等对待一个分子及其镜像。它们可以学会区分它们。例如，它们可以计算像[标量三重积](@entry_id:177480) $\mathbf{r}_i \cdot (\mathbf{r}_j \times \mathbf{r}_k)$ 这样的**[伪标量](@entry_id:196696)**，这个数值在旋转下不变，但在反射时会改变符号。这使得模型能够正确地学习到手性分子可以具有非零偶极矩等性质，而它们的[非手性](@entry_id:194107)对应物（如 1,2-二氯乙烷的*反式*构象异构体）则不能 [@problem_id:3468358]。

#### 一瞥即学

也许最显著的实际好处是数据效率。因为[旋转对称](@entry_id:137077)性是硬编码的，所以模型不需要被展示一个分子成千上万种不同的朝向。如果它学会了如何预测单一构象的能量和力，其[等变性](@entry_id:636671)就能自动确保它知晓该构象*所有可能旋转版本*的答案。它能免费地泛化到所有朝向，而这对于一个非等变模型来说，将需要海量的数据 [@problem_id:3468358]。

### 超越局部视野

尽管这些等变 GNN 功能强大，但它们有一个內在的局限性：它们是局部的。每个原子的预测都基于一个有限邻域内的信息，这个邻域通常由一个[截断半径](@entry_id:136708) $r_c$ 定义。经过几层[消息传递](@entry_id:751915)后，一个原子所知的范围被限制在它周围几纳米的球体内。

这对于描述短程量子力学相互作用来说完全没问题，但对于[长程力](@entry_id:181779)，尤其是静电（库仑）相互作用，则会完全失效。[库仑力](@entry_id:174598)以 $1/r$ 的形式缓慢衰减，这意味着晶体中一个离子所受的力取决于系统中*所有*其他离子，即使是那些相隔数千个原子的离子。根据定义，局部模型对这种非局部物理现象是盲视的 [@problem_id:3449555]。

这是否意味着整个方法都有缺陷？完全不是。这仅仅意味着我们需要更巧妙一些。研究的前沿在于创建**[混合模型](@entry_id:266571)**，以结合两者的优点。其策略是让等变 GNN 做它最擅长的事情——模拟复杂、纠缠的短程量子效应——并使用另一种解析方法来处理长程部分。

一种优美的途径是训练 GNN 预测依赖于环境的原子属性，比如每个原子上的[部分电荷](@entry_id:167157)。然后将这些学习到的[电荷](@entry_id:275494)输入到一个经典的、高度优化的物理求解器（如 [Particle-Mesh Ewald](@entry_id:140744) 或 PME 方法）中，该求解器能高效地计算整个周期性系统的长程[静电能](@entry_id:267406)。通过使整个流程可[微分](@entry_id:158718)，计算出的力能够正确地包含短程学习到的分量和长程物理分量 [@problem_id:3449555]。这种[深度学习](@entry_id:142022)与[经典物理学](@entry_id:150394)的结合，展示了科学家如何在基本原则上进行构建，承认一种工具的局限性，并将其与另一种工具无缝集成，从而描绘出一幅更完整的世界图景。

