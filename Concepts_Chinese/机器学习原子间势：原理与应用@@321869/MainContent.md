## 引言
模拟原子和分子的动力学是现代科学的核心挑战，它弥合了基本定律与可观测现象之间的鸿沟。虽然量子力学为原子相互作用提供了精确的描述，但其计算成本对于除最小系统之外的所有系统来说都高得令人望而却步。这造成了知识上的差距，限制了我们模拟化学和[材料科学](@article_id:312640)中复杂过程的能力。[机器学习原子间势](@article_id:344521)（MLIPs）作为一种革命性的解决方案应运而生，有望以极低的计算成本实现量子级别的精度。本文旨在全面概述这项强大的技术。在接下来的章节中，我们将首先探讨其基础的“原理与机制”，从[势能面](@article_id:307856)的底层物理学到使这些模型得以实现的架构和训练策略。随后，我们将展示 MLIPs 在一系列“应用与跨学科联系”中的变革性影响，演示它们如何被用于预测[材料性质](@article_id:307141)、实现混合模拟，甚至探索原子的量子本性。

## 原理与机制

要构建一个像物理学家一样思考的机器，我们必须首先教会它游戏规则。[原子间势](@article_id:356603)不仅仅是任意一个数学函数；它是量子力学定律的紧凑表示，专为宏大的原子之舞而设。我们的使命是为复杂的量子世界创建一个快速、精确的[代理模型](@article_id:305860)，为此，我们必须深入理解它必须遵循的原理以及赋予其生命的机制。

### 一个[曲面](@article_id:331153)上的世界：玻恩-奥本海默近似

想象一下原子的宇宙。它们永不停歇的运动——[振动](@article_id:331484)、旋转、碰撞——并非随机的混沌。它受一片由丘陵和山谷构成的地貌所支配，这是一个错综复杂的多维[势能面](@article_id:307856)。系统的状态总是倾向于向山下滚动，朝向能量最低的山谷。这片地貌就是**[势能面](@article_id:307856)（PES）**，它是我们希望机器学习模型学习的核心对象。

但这片地貌从何而来？它源于量子力学中一个深刻的简化，即**玻恩-奥本海默近似**。其关键思想源于一个巨大的质量差异：原子核比环绕它们的电子[重数](@article_id:296920)千倍。因此，与缓慢、笨重的原子核相比，轻盈、敏捷的电子运动速度几乎是无限快。对于任何固定的原子核排布，电子都有充足的时间安顿到它们的最低能量[量子态](@article_id:306563)，即[基态](@article_id:312876)。

PES，记为 $V(\mathbf{R})$，是对于每一种可能的原子核构型 $\mathbf{R}$ 的电子基态能量。原则上，要获得这个能量，我们必须在原子核被“钳位”的情况下求解[电子薛定谔方程](@article_id:356914)，然后加上带正电的原子核之间的经典排斥力。这个过程需要对无数种构型重复进行，才能绘制出整个[势能面](@article_id:307856)。至关重要的是，这个势能 $V(\mathbf{R})$ 只取决于原子的位置，而与它们的运动或系统温度无关。它决不能与[热力学](@article_id:359663)中的**[亥姆霍兹自由能](@article_id:296896)**相混淆，后者包含了原子核动能和熵的效应——即原子在有限温度下的晃动和摆动。

这整个美丽的图景依赖于几个假设：电子足够快以至于能瞬时调整（玻恩-奥本海默近似本身），以及系统的动力学过程足够温和，不会将[电子激发](@article_id:363044)到更高能量的[激发态](@article_id:325164)（**[绝热近似](@article_id:303509)**）。对于绝大多数化学和材料现象，这些假设都成立得非常好，为我们构建[机器学习势](@article_id:362354)提供了坚实的基础[@problem_id:2784636]。

### 游戏规则：物理对称性与局域性

物理定律通常是关于什么*不变*的陈述——即关于对称性的陈述。一个水分子的能量，无论它是在这里还是在月球上，都是相同的（[平移不变性](@article_id:374761)）；无论它在空间中如何取向，能量也是相同的（[旋转不变性](@article_id:298095)）。此外，如果我们交换两个氢原子，它仍然是同一个水分子，具有相同的能量（[置换](@article_id:296886)[不变性](@article_id:300612)）。任何一个有价值的势能模型都*必须*遵守这些基本对称性。

这立即带来了一个挑战。如果我们试图以最朴素的方式向计算机描述一个分子，即简单地列出每个原子的 $(x, y, z)$ 坐标，我们会彻底失败。想象一个在空间中旋转的甲烷分子。从计算机的角度看，代表其原子坐标的数字列表在不断变化。一个基于这种表示训练的机器学习模型将会 hopelessly confused，对同一个分子的每一种可能取向都学习到一个不同的能量[@problem_id:2457461]。

解决这个问题的技巧，正如物理学中常常出现的那样，是使用本身就具有[不变性](@article_id:300612)的量来描述原[子环](@article_id:314606)境。我们不向模型输入原始坐标，而是为每个原子的局域环境构建一个数学“指纹”。这个指纹，被称为**描述符**，是根据邻近原子的相对位置（例如，距离和角度）构建的，其构建方式保证了无论整个系统如何旋转或平移，其值都保持不变。

游戏的下一个规则是**局域性**。正如物理学家 Walter Kohn 所说，物质是“[近视](@article_id:357860)”的。特定原子上的能量和力绝大多数由其近邻决定，而不是由一英里外的原子决定。这使我们能够定义一个**[截断半径](@article_id:297161)**，即每个原子周围的一个小影响范围球。我们可以做出一个非常好的近似，认为这个截断范围之外的原子没有直接影响。一个简单的思想实验可以阐明这一点：如果你在一个晶体中轻微摆动一个原子，扰动会通过材料传播，但远处的原子不会立即感受到它。为了计算局域能量，我们可以简单地忽略远处的原子[@problem_id:2457450]。

局域性原理直接导出了一个至关重要的性质：**尺寸[广延性](@article_id:313063)**。通过假定系统的总能量是每个原子局域能量贡献的总和，我们保证了大系统的能量能随其尺寸正确地标度。例如，对于两个不相互作用的分子，当它们相距大于[截断半径](@article_id:297161)时，其总能量被正确地预测为它们各自能量之和。这听起来显而易见，但从一开始就将其构建到架构中，是构建稳健势的基石[@problem_id:2805720]。

### 数字炼金术士的蓝图：MLIP 架构

掌握了物理原理——对称性、局域性和广延性——我们现在可以为我们的模型设计蓝图了。核心的架构选择是将总[能量分解](@article_id:372528)为原子贡献的总和：
$$
E_{\text{total}} = \sum_{i} E_i
$$
这个简单的求和保证了广延性。创造性的部分在于如何计算每个原子的局域能量贡献 $E_i$。在这里，已经出现了两大思想流派[@problem_id:2648619]。

**1. “固定指纹”方法（例如 Behler-Parrinello 网络）**

在这种方法中，物理学家扮演建筑师的角色，预先定义一组捕捉局域几何的描述符。这些是固定的、手工制作的数学表达式，称为**[对称函数](@article_id:356066)**，例如，它们可以编码涉及一个原子及其[截断半径](@article_id:297161)内邻居的所有二体距离和三体角度。这些函数在设计上就是旋转不变和[置换](@article_id:296886)不变的。这个[对称函数](@article_id:356066)值的向量——即“指纹”——然后被输入到一个标准的[前馈神经网络](@article_id:640167)中。网络的工作不是要弄清对称性，而仅仅是学习从已经对称化的指纹到单个数字（原子能量 $E_i$）的复杂[非线性映射](@article_id:336627)。这种方法将强大的物理知识或**[归纳偏置](@article_id:297870)**注入到模型中，这可以使其数据效率更高。

**2. “可学习指纹”方法（例如[图神经网络](@article_id:297304)）**

这个思想流派采用了一种更灵活、数据驱动的方法。分子被看作一个图，其中原子是节点，邻近关系（在[截断半径](@article_id:297161)内）是边。每个原子开始时带有一个代表其身份（例如，“我是一个碳原子”）的基本向量。然后，一个**信息传递**的过程开始。原子与它们的邻居“交谈”，发送依赖于自身状态和相对位置的信息。每个原子随后通过聚合它收到的信息来更新自己的状态向量。经过几轮这种学习到的通信后，每个原子的最终向量成为其局域环境的丰富、依赖于上下文的表示。这个最终向量*就是*学习到的指纹，然后用它来预测原子能量 $E_i$。这种端到端的学习具有很强的表达能力，因为模型能从数据本身中发现最相关的特征。

在这些哲学之间的选择，涉及在手工特征的强指导性与学习表示的强大灵活性之间进行权衡。

### 教导的艺术：基于量子数据的训练

我们有了一个模型；现在我们需要一个“答案钥匙”来训练它。这个钥匙来自我们拥有的最精确的理论：量子力学计算，最常见的是**密度泛函理论（DFT）**。对于一组选定的原子构型，我们运行昂贵的 DFT 模拟来计算“基准真相”的能量，以及同样重要的，每个原子上的力。

一个至关重要且常常很微妙的点是，当我们在 DFT 数据上训练 MLIP 时，我们是在教它成为那个*特定 DFT 模型*的快速而忠实的模仿者，并继承其所有内在的近似。妙处在于，一个收敛良好的 DFT 计算所得的力，正是 DFT 能量面的精确解析梯度。这意味着训练数据是内部一致的：相对于能量而言，力是保守的。这为训练一个保守的 MLIP 提供了完美的数据集[@problem_id:2837976]。

力是 MLIP 训练的秘密武器。对于一个结构，一次能量计算只给我们一个数据点，而对于一个有 $N$ 个原子的系统，力提供了 $3N$ 个数据点——每个原子的每个[笛卡尔坐标](@article_id:323143)一个。这是巨大的信息财富。力告诉我们[势能面](@article_id:307856)各处的*斜率*，为模型提供了关于地貌形状的丰富、多维的图像，而不仅仅是其在几个点处的高度。这就是**力匹配**的精髓[@problem_id:2759514]。

为了训练模型，我们编写一个**[损失函数](@article_id:638865)**来衡量 MLIP 的预测与 DFT 参考数据之间的不匹配程度。一个稳健的损失函数结合了能量和力的误差。但如何结合呢？简单地将能量误差的平方与力误差的平方相加是错误的——这就像把米加到千克上！这些项具有不同的物理单位。一种有原则的方法，其动机源于最大似然估计的统计理论，是通过将每个量的平方误差除以其预期的方差或噪声水平（能量为 $\sigma_E^2$，力为 $\sigma_F^2$），从而使每项都无量纲化。此外，为了平衡一个能量值与 $3N$ 个力值的信息含量，损失函数通常将能量误差的平方与*平均*力误差的平方进行比较。这为模型创造了一个平衡的、量纲一致的、并且在统计上有意义的学习目标[@problem_id:2648589]。

### 衡量我们的无知：[不确定性量化](@article_id:299045)

一个伟大的科学家不仅知道他们知道什么，也知道他们知识的局限。一个真正智能的 MLIP 也应该如此。这属于**[不确定性量化](@article_id:299045)**的范畴，它要求我们区分两种根本不同的不确定性[@problem_id:2648582]。

**[认知不确定性](@article_id:310285)**是可减少的无知。这是模型在说：“我不确定这个预测，因为我在训练数据中没有见过类似的东西。”这种不确定性是模型及其有限经验的一个属性。它在构型空间的未探索区域很高。我们可以通过训练一个具有不同随机初始化的模型集成来估计它；如果模型对一个预测的意见[分歧](@article_id:372077)很大，我们的认知不确定性就很高。这种不确定性的美妙之处在于它是可以减少的。它像一个向导，准确地告诉我们需要在哪里收集更多数据来使我们的模型更智能。这就是**[主动学习](@article_id:318217)**背后的原理。

另一方面，**[偶然不确定性](@article_id:314423)**是数据本身固有的、不可减少的随机性。这是模型在说：“我所依据的‘基准真相’本身就是模糊的。”如果我们的参考数据来自像[量子蒙特卡洛](@article_id:304811)（QMC）这样的随机方法，这种情况就会发生，因为 QMC 具有固有的统计噪声。在[粗粒化模型](@article_id:640967)中也会出现这种情况，我们有意地平均掉了细粒度的细节；一个[粗粒化](@article_id:302374)珠子上的力会自然地根据它所代表的原子的精确、隐藏状态而波动。这种不确定性是系统或测量过程的一个特征，而不是我们模型的缺陷，即使有无限的数据，它也依然存在。

在形式上，一个概率模型的总预测方差可以分解为这两部分之和。随着我们收集越来越多的数据，认知（模型）不确定性会消失，但偶然（数据）不确定性会作为一个基本衡量标准，代表我们正在建模的系统中[固有噪声](@article_id:324909)的大小而保留下来[@problem_id:2648582]。理解这两种不确定性，不仅能让我们构建更精确的模型，还能以一种有原则的、科学的方式信任它们的预测。