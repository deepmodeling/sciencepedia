## 引言
机器学习与分子科学的交叉融合，有望彻底改变我们发现材料、设计药物以及理解自然基本规律的方式。通过教会计算机化学的复杂语言，我们可以绕过长期以来制约科学进步的许多计算瓶颈。然而，这项事业也带来了一个深远的挑战：我们如何将分子动态的、量子力学的现实转化为机器能够理解的格式？又如何教会它掌握支配其行为的物理定律？本文旨在通过全面概述驱动该领域发展的核心概念，来填补这一知识鸿沟。

我们的探索始于“原理与机制”一章，该章节深入探讨了分[子表示](@entry_id:141094)的基础问题。我们将探索如何构建尊重基本物理对称性的描述符，并了解模型如何被训练以学习至关重要的[势能面](@entry_id:147441)。本节还将强调数据效率和[不确定性量化](@entry_id:138597)在创建稳健可靠模型中的关键作用。随后，“应用与跨学科联系”一章将展示这些强大工具如何应用于解决现实世界的问题。我们将看到，机器学习不仅在加速计算，更在成为一种新的理论视角，为从生物学到[量子化学](@entry_id:140193)等领域开辟新前沿，并推动我们朝向为所有物质建立一个通用模型的宏伟挑战迈进。

## 原理与机制

要教会机器化学这门精妙而深奥的语言，我们必须首先解决两个基本问题。第一，我们如何以计算机能够理解的方式，来描述一个分子——这个由[原子核](@entry_id:167902)和电子构成的动态三维实体？第二，一旦计算机理解了结构，我们又该如何教它掌握支配其行为，特别是其能量的底层物理定律？这些问题的答案位于物理学、计算机科学和化学三者美妙的交汇点，构成了这个激动人心领域的核心原理与机制。

### 分子的通用语言

分子不是一个静态的物体。它会[振动](@entry_id:267781)、旋转和扭曲。因此，第一个挑战便是创造一种数学表示，即**描述符**（descriptor），它既能捕捉[分子结构](@entry_id:140109)的本质，又能尊重基本的物理定律。毕竟，自然法则不依赖于我们任意选择的观测方式。

这些定律中最关键的是对称性。例如，一个孤立水分子的能量，无论是在你的实验室里，还是漂浮在半人马座阿尔法星附近，都是相同的；它对于其在空间中的位置和朝向是保持不变的。这意味着我们的描述符必须具备**平移和[旋转不变性](@entry_id:137644)**[@problem_id:2784640]。此外，水分子中的两个氢原子是完全相同、不可区分的粒子。如果我们偷偷交换它们的标签，分子的能量和性质将保持不变。因此，我们的描述符还必须对相同原子的**[置换](@entry_id:136432)**（permutation）保持不变[@problem_id:2784640] [@problem_id:2952097]。一个不尊重这种对称性的模型将是荒谬的——它会因为我们给原子编号的方式不同而预测出同一分子的不同能量，这明显违反了量子力学[@problem_id:2456264]。

早期创建此类描述符的尝试虽然直观但存在缺陷。例如，可以构建一个**库仑矩阵**（Coulomb matrix），其中每个条目代表两个[原子核](@entry_id:167902)之间的静电排斥力[@problem_id:2479765]。对角线上的条目可以表示原子自身能量的缩放版本。虽然这种方法捕捉了一些基本的[物理信息](@entry_id:152556)，但它在[置换检验](@entry_id:175392)上却彻底失败了。交换两个原子意味着交换矩阵的行和列，这会完全改变矩阵。有人可能尝试通过使用[矩阵特征值](@entry_id:156365)的排序列表作为描述符来解决这个问题。这个列表*确实*具有[置换不变性](@entry_id:753356)，但这个解决方案就像只用音符集合来重建一首乐曲一样——你丢失了所有的结构信息，并且不同的分子可能会意外地得到相同的描述符[@problem_id:2479765]。

现代方法从一开始就将这些基本对称性构建到其架构中。主流策略是创建**以原子为中心的描述符**（atom-centered descriptors），其中每个原子都由其局部化学环境的“指纹”来描述。总能量通常被建模为每个原子贡献的总和。这种方法具有一种奇妙的优雅。如果每个原子的指纹对其邻居的标记是不变的，并且总能量是所有原子的总和，那么最终的预测就自动地、精确地具有[置换不变性](@entry_id:753356)。

这种局部指纹是如何构建的呢？一种强大的方法是使用**[对称函数](@entry_id:177113)**（symmetry functions）。对于每个原子，我们通过对其邻居的贡献求和来计算一组值。例如，一个函数可能会对到某种类型的所有邻近原子的经高斯加权的距离求和，而另一个函数则可能捕捉角度信息。因为我们是对邻居进行*求和*，所以我们考虑它们的顺序无关紧要，这优雅地解决了[置换](@entry_id:136432)问题[@problem_id:2952097]。另一种日益流行的方法是将分[子表示](@entry_id:141094)为一个图，其中原子为节点，[化学键](@entry_id:138216)为边。**图神经网络（GNNs）**可以通过在相邻原子之间传递“消息”并在每个节点上进行聚合来学习原子指纹。同样，使用像求和这样的[置换](@entry_id:136432)不变聚合器可以确保整个模型尊重所需的对称性[@problem_id:2952097]。

通过将物理学直接构建到表示中，我们使机器学习模型不必从零开始学习这些基本对称性这一不可能的任务。它反而可以将其全部能力集中在学习化学相互作用的复杂细节上。

### 学习能量景观

一旦我们有了描述分子的语言，我们该教给机器什么呢？核心概念是**[势能面](@entry_id:147441)（PES）**。想象一个广阔的高维景观，其中每一点都对应于分子中原子的特定[排列](@entry_id:136432)，而该点的高度就是其势能。这个景观的谷底代表稳定的分子及其不同的构象。连接这些谷底的山隘代表[化学反应](@entry_id:146973)的过渡态。一个分子的整个生命故事——它的结构、稳定性、反应性——都写在了这个景观的地形之中。

[机器学习势](@entry_id:183033)的宏伟目标是学习整个函数 $E(\mathbf{R})$，该函数将任意一组原子坐标 $\mathbf{R}$ 映射到一个标量能量 $E$。模型接收我们精心构建的、具有对称性感知的[分子描述符](@entry_id:164109)，并输出这个单一的数值。

在这里，我们发现了机器学习和[统计力](@entry_id:194984)学之间一个惊人而美妙的相似之处。假设一个模型试图决定几种分子形状（构象异构体）中哪一种最可能出现。模型可能会输出一组未归一化的分数 $\{s_i\}$。要将这些分数转换为概率，一个标准的工具是 **softmax 函数**：
$$
q_i = \frac{\exp(s_i/\tau)}{\sum_j \exp(s_j/\tau)}
$$
现在，让我们转向物理学。在物理温度 $T$ 下，一个物理系统处于能量为 $E_i$ 的状态的概率由基本的**[玻尔兹曼分布](@entry_id:142765)**（Boltzmann distribution）给出：
$$
p_i = \frac{\exp(-E_i/(k_B T))}{\sum_j \exp(-E_j/(k_B T))}
$$
这两个方程在数学上是完全相同的！[@problem_id:2463642]。模型的分数 $s_i$ 扮演了[负能量](@entry_id:161542) $-E_i$ 的角色。更高的分数对应于更低的能量，也就是更可能的状态。这并非巧合，它揭示了一种深层次的统一性。一个训练良好的模型会隐式地学习一个有效的[能量景观](@entry_id:147726)。

softmax 函数中的温度参数 $\tau$ 控制着模型预测的“置信度”。当 $\tau \to 0$ 时，[概率分布](@entry_id:146404)会在得分最高（能量最低）的状态上形成尖峰，就像物理系统在绝对[零度](@entry_id:156285)下冻结到其[基态](@entry_id:150928)一样。随着 $\tau$ 的增加，[概率分布](@entry_id:146404)变得平坦，代表着更大的不确定性，就像一个高温系统会探索许多不同的能量状态一样。这个可调参数使我们不仅能将模型的输出解释为预测，还能将其解释为反映其确定性的[概率分布](@entry_id:146404)[@problem_id:2463642]。

### 教学的艺术：能量、力与数据效率

为了教会我们的模型[势能面](@entry_id:147441)，我们需要向它提供示例。这些示例来自于求解量子力学方程，这些方程能为特定的原子[排列](@entry_id:136432)提供高精度但计算成本高昂的能量。训练过程是一个循环：模型预测一个能量，我们将其与真实的量子力学能量进行比较，然后调整模型的内部参数以最小化差异，即**损失**（loss）。

一个关键问题随之而来：我们应该只用能量来训练模型，还是有更多信息可以利用？物理学给了我们一个强有力的提示。作用在原子上的力就是能量景观的负梯度（斜率）：$\mathbf{F} = -\nabla E$。

想象一下你正在尝试绘制一幅山脉地图。你可以在许多不同地点徒步并记录海拔（能量）。这会让你对地形有一个大致的了解。但如果，在每个地点，你还测量了坡度的陡峭程度和方向（力）呢？这种梯度信息对于重建景观的详细形状要强大得多。

学习[势能面](@entry_id:147441)也是如此。一个单一的能量值只是一个数字。而作用在原子上的力是一个具有三个分量 $(F_x, F_y, F_z)$ 的矢量。对于一个有 $N$ 个原子的分子，利用力进行训练可以提供 $3N$ 条梯度信息，而能量信息只有一条[@problem_id:2903774]。这使得训练的**数据效率**（data-efficient）大大提高。通过在训练中包含力，我们可以更快地教会模型[能量景观](@entry_id:147726)的形状，从而用少得多的昂贵量子力学计算就能达到期望的精度水平[@problem_id:2648589] [@problem_id:2903774]。将能量误差（例如，单位为焦耳）和力误差（例如，单位为牛顿）合并到单一损失函数中的挑战，可以通过[量纲分析](@entry_id:140259)和统计学原理优雅地解决，从而得到一个能够利用所有可用信息的平衡目标函数[@problem_id:2648589]。

### 知其所不知：不确定性的关键作用

一个真正智能的模型，就像一位优秀的科学家一样，不仅应提供答案，还应诚实地评估其置信度。在机器学习中，这被称为**[不确定性量化](@entry_id:138597)（UQ）**。理解模型的不确定性不仅仅是一项学术活动，它对于构建可靠高效的科学发现工具至关重要。

我们必须考虑两种主要的不确定性类型[@problem_id:2760138]。第一种是**[偶然不确定性](@entry_id:154011)**（aleatoric uncertainty），这是数据本身固有的噪声或随机性。例如，我们用于训练的量子力学计算可能没有完全收敛，这会在能量标签中引入一个微小但不可避免的误差。这就像用一把稍有瑕疵的尺子测量物体一样，你能达到的精度是有限的。一个复杂的模型可以被设计来考虑这一点，从而有效地学会更多地信任“干净”的数据点，而不是“嘈杂”的数据点。

第二种，也往往是更关键的类型，是**认知不确定性**（epistemic uncertainty）。这是模型因其知识有限而产生的不确定性。一个只在水分子上训练过的模型，如果你突然让它预测乙醇分子的能量，它会非常不确定。这种不确定性在广阔的“化学空间”中模型训练时未曾见过的区域会很高。与偶然不确定性不同，认知不确定性是可约减的：我们可以通过在那些未探索的区域为模型提供更多数据来减少它。

这种量化[认知不确定性](@entry_id:149866)的能力开启了一种称为**主动学习**（active learning）的强大[范式](@entry_id:161181)。[机器学习模型](@entry_id:262335)可以被用来运行模拟，探索新的化学构型。当它遇到一个其[认知不确定性](@entry_id:149866)很高的结构时，它基本上可以举起一面旗帜说：“这超出了我的能力范围！”然后，它可以暂停模拟，并请求对这个特定结构进行一次新的、高精度的[量子计算](@entry_id:142712)。这个新的、信息量大的数据点随后被添加到训练集中，使模型更加稳健。这个智能的、由反馈驱动的过程使得模型能够高效地自主学习和探索化学空间。

UQ的实际重要性在**[分子动力学](@entry_id:147283)（MD）模拟**中最为明显，在这些模拟中，模型被用来模拟原子随时间的运动[@problem_id:2908464]。MD模拟需要在数百万个连续的时间步长上计算所有原子受到的力。如果在任何时刻，机器学习模型给出了一个灾难性的错误力预测，模拟就可能“崩溃”——原子可能会以不切实际的速度飞散开来。对预测的*力*进行良好校准的[不确定性估计](@entry_id:191096)，就像一个至关重要的安全网。通过监测模型的力不确定性，我们可以检测到模拟何时变得不可靠并采取纠正措施。像**[深度集成](@entry_id:636362)**（deep ensembles）这样的方法，即我们训练多个模型并使用它们之间的[分歧](@entry_id:193119)作为不确定性的度量，为实现这一目标提供了一种稳健的方式，从而能够对复杂的化学过程进行长期、稳定且可信的模拟[@problem_-id:2908464]。

