## 引言
像 [Transformer](@article_id:334261) 这样的现代人工智能模型已经彻底改变了从[自然语言处理](@article_id:333975)到科学发现的各个领域。然而，其核心组件——[自注意力机制](@article_id:642355)，存在一个根本性的盲点：它天生无法感知序列顺序。对于它来说，“人咬狗”和“狗咬人”这样的句子看起来是完全相同的。本文旨在通过探讨[位置编码](@article_id:639065)这一概念来弥补这一关键缺陷。[位置编码](@article_id:639065)是赋予这些强大模型“之前”与“之后”感的关键要素。我们将首先深入探讨其基本**原理与机制**，剖析从绝对正弦编码到优雅的相对旋转等不同策略如何向模型传递序列结构信息。随后，我们将在**应用与跨学科联系**部分展开讨论，揭示这一思想如何被巧妙地应用于编码生物学、物理学和音乐中的对称性，从而将[位置编码](@article_id:639065)从一个简单的序列计数器转变为一种描述世界的深刻语言。

## 原理与机制

想象一下，你有一堆 Scrabble 拼字游戏中的瓷片，每片上都有一个单词。如果你把它们扔进一个袋子里，你就得到了一个“词袋”——你知道里面有哪些词，但完全不知道它们的原始顺序。“人咬狗”和“狗咬人”这两个句子变得无法区分，尽管它们的含义天差地别。这正是纯粹的[自注意力机制](@article_id:642355)所面临的根本挑战。[自注意力](@article_id:640256)的核心是在一个项目*集合*（一个“向量袋”）上进行操作。它被巧妙地设计用来衡量每个项目相对于所有其他项目的重要性，但它天生对序列顺序是盲目的。这种特性被称为**[置换](@article_id:296886)[等变性](@article_id:640964)**（permutation equivariance），意味着如果你打乱输入序列，注意力输出也只是以同样的方式被打乱，但模型本身并没有“之前”或“之后”的内在概念 [@problem_id:3154475]。为了构建能够理解语言、音乐或[时间序列数据](@article_id:326643)的模型，我们必须明确地教会它们顺序的概念。这正是**[位置编码](@article_id:639065)**（positional encodings）的作用。

### 为序列编号：绝对[位置编码](@article_id:639065)

我们如何告知模型每个词元（token）的位置呢？最直接的方法是为每个词元的[嵌入](@article_id:311541)向量附加一个独特的“位置标签”，就像在[流水线](@article_id:346477)上为每个物件盖上序列号一样。这便是**绝对[位置编码](@article_id:639065)**（absolute positional encodings）的精髓。

我们可以设想让模型从头*学习*这些标签。对于一个长度为 $L$ 的序列，我们会创建 $L$ 个不同的向量，每个位置一个，并像训练其他任何参数一样训练它们。这种方法可行，但存在一个重大缺陷：当模型遇到比训练时见过的任何句子都长的句子时，会发生什么？对于任何超出 $L$ 的位置，都没有预先学习好的标签。模型将束手无策，这严重限制了其对更长序列的泛化能力——这个问题被称为糟糕的**外推**（extrapolation）能力 [@problem_id:3173696]。

一个远为优雅的解决方案是使用确定性的数学规则来生成这些位置标签。这便是经典的**[正弦位置编码](@article_id:642084)**（sinusoidal positional encodings）背后的思想。想象一下，你想编码一个位置，比如 $t=3$。你可以不使用单个数字，而是使用一组时钟，每个时钟的秒针以不同的固定速度旋转。第一个时钟可能每10秒转一圈，下一个每100秒转一圈，以此类推。位置 $t=3$ 就对应于所有时钟指针的一个独特组合。在数学上，我们使用正弦和余弦对来表示每个指针的方向。对于每个位置 $t$ 和每个频率 $\omega_i$，我们生成一对值 $(\sin(\omega_i t), \cos(\omega_i t))$ [@problem_id:3181505]。通过将不同频率下的这些值对拼接起来，我们为每个位置创建了一个独特的向量签名。由于正弦和余弦函数对任何数字都有定义，我们可以为任何可以想象的位置生成签名，从而在原则上立即解决了[外推](@article_id:354951)问题。

### 意外的礼物：从绝对编码中获得的相对位置

这里，一点数学上的魔力开始发挥作用。我们最初为模型提供了绝对位置标记，但[点积](@article_id:309438)注意力机制却自然而然地发现了一种更为强大的东西：一种*相对*位置感。

位于位置 $t$ 的查询（query）与位于位置 $u$ 的键（key）之间的注意力分数取决于它们各自向量的[点积](@article_id:309438)。加入[位置编码](@article_id:639065)后，这个[点积](@article_id:309438)包含一个形如 $\langle p_t, p_u \rangle$ 的项，即它们位置向量的内积。让我们更仔细地看看这一项。对于单个频率 $\omega$，其对[点积](@article_id:309438)的贡献是 $\sin(\omega t)\sin(\omega u) + \cos(\omega t)\cos(\omega u)$。一个基本的[三角恒等式](@article_id:344424)告诉我们，这恰好等于 $\cos(\omega(t-u))$！

当我们将编码中使用的所有频率的这一项加起来时，完整的位置[点积](@article_id:309438)就变成了 $\sum_i \cos(\omega_i(t-u))$ [@problem_id:3193493] [@problem_id:3172436]。这个分数不再独立地取决于绝对位置 $t$ 和 $u$，而是取决于它们的*差值* $t-u$。模型获得了一份意外的礼物：一把内置的、用于测量词元之间距离的尺子。

这种涌现属性是通往一个强大概念——**[平移等变性](@article_id:640635)**（translation equivariance）的关键。它意味着注意力模式对于全局平移是不变的。位置2的词元关注其邻居{1, 2, 3}的方式，将与位置102的词元关注其邻居{101, 102, 103}的方式完全相同。学习到的[注意力机制](@article_id:640724)会随着序列的平移而平移。这一点可以被严格验证：一个查询的注意力分布与一个平移后的查询对其相应平移后的键的注意力分布之间的差异恰好为零 [@problem_id:3193493]。这就是为什么正弦编码虽然是绝对的，却能泛化到训练期间从未见过的序列长度 [@problem_id:3173696] [@problem_id:3193561]。模型学习的是关于相对距离的规则，这些规则是普适的，而不是关于特定绝对位置的规则。

### 直接方法：为相对性而设计

如果模型最终使用的是相对位置，那为什么不直接设计提供相对位置的编码呢？这一思路催生了几种强大的**相对[位置编码](@article_id:639065)**（relative positional encoding）方案。

#### 作为加法的相对位置：与卷积的联系

也许最直接的方法是在注意力分数上增加一个特殊的偏置（bias），该偏置仅取决于查询和键之间的相对距离 $t-u$。例如，在一个被称为**线性偏置注意力（ALiBi）**的方案中，这个偏置就是一个与距离成线性关系的可学习的小惩罚项，比如 $-\alpha|t-u|$ [@problem_id:3193561]。这会温和地鼓励模型关注附近的词元，这对于许多任务来说是一个合理的默认设置。

这个看似简单的添加相对偏置的技巧，揭示了其与计算机科学中一个经典概念的深刻联系。在一个简化的设定下，带有这种偏置的[注意力机制](@article_id:640724)在数学上等同于**[循环卷积](@article_id:308312)**（circular convolution）[@problem_id:3180923]。学习到的偏置构成了一个滤波器（filter）或核（kernel），它在输入序列上“滑动”以产生输出。这表明，作为现代[深度学习](@article_id:302462)巅峰之作的 Transformer，其根源深植于信号处理这个早已被充分理解的领域。[平移等变性](@article_id:640635)不再是一种涌现属性，而是卷积结构中一个内置的设计特性。

#### 作为旋转的相对位置：一种优雅的变换

一种更为精巧的方法见于**旋转位置[嵌入](@article_id:311541)（RoPE）**。RoPE 不是添加位置信息，而是*旋转*查询和键向量。每个向量被划分为成对的维度，每一对都按照一个与其位置成正比的角度 $\theta_t = \omega t$ 进行旋转。

RoPE 的数学优雅性在其[点积](@article_id:309438)中得以体现。通过根据位置 $t$ 的查询向量和位置 $u$ 的键向量各自的位置进行旋转，它们的内积变成了一个仅依赖于其内容和*相对位置* $t-u$ 的函数 [@problem_id:3185332] [@problem_id:3193561]。所有绝对位置信息都消失了，只剩下相对偏移量。这是一种注入相对位置信息的优美方式，因为旋转是保长的；它们不改变内容[向量的大小](@article_id:366769)，只改变其方向。

当然，没有哪个解决方案是完美无瑕的。由于旋转具有周期性，对于非常大的相对距离，RoPE 无法区分距离 $d$ 和距离 $d + P$，其中 $P$ 是旋转的周期。这种“[混叠](@article_id:367748)”（aliasing）在极长序列中可能成为一个影响因素，但其对相对位置的基本依赖性仍然是实现[外推](@article_id:354951)的一个强大特性 [@problem_id:3193561]。

### 调优的艺术：幅度和频率

拥有一个编码位置的机制只是成功了一半。这种位置信号的“音量”必须被恰到好处地调整。

如果[位置编码](@article_id:639065)的**幅度**太低（接近于零），注意力分数会变得几乎完全相同。由此产生的注意力图谱是一片均匀的“大杂烩”，其中每个词元都同等地关注所有其他词元，模型实际上仍然是顺序盲的。这种最大不确定性的状态可以用高**香农熵**来量化。另一方面，如果幅度太高，位置信号可能会完全压倒内容信息。注意力分数会变得极其尖锐，导致注意力几乎完全基于位置而“坍缩”到单个词元上。这是一种接近确定性（低熵）的状态，它阻止了模型学习基于内容的关系 [@problem_id:3180897]。找到恰当的平衡至关重要。

此外，在正弦和旋转方案中选择频率对学习动态有深远影响。对[反向传播算法](@article_id:377031)的仔细分析揭示了一种“[频谱](@article_id:340514)偏置”（spectral bias）：[损失函数](@article_id:638865)相对于输入的梯度对于高频位置分量呈指数级增大 [@problem_id:3181505]。这意味着模型学习拟合高频细节（如非常局部的词间互动）的速度和积极性，远超学习低频信息（如长程句子结构）。这一洞见为使用对数范围的频率这一常见做法提供了理论依据，确保模型能够接触到均衡的[信号频谱](@article_id:377210)进行学习，范围从最局部的细节一直延伸到最全局的依赖关系。

总而言之，为[神经网络](@article_id:305336)提供顺序感是一个内容丰富且引人入胜的问题。从简单、有缺陷的标签到优雅、相对的旋转，这一历程揭示了直觉、数学严谨性和涌现属性之间的美妙相互作用，展示了赋予现代人工智能力量的深层原理。

