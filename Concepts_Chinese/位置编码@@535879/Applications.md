## 应用与跨学科联系

我们已经看到，[位置编码](@article_id:639065)是一种巧妙的技巧，它使得像 Transformer 这样的架构能够理解顺序。但如果仅止于此，就好比学会了国际象棋的规则却从未欣赏过大师的对局。[位置编码](@article_id:639065)的真正美妙之处不仅在于它们能为事物排序，更在于它们提供了一种描述问题*几何*与*对称性*的语言。它们是连接[神经网络](@article_id:305336)的抽象、[置换](@article_id:296886)无关世界与我们希望它理解的丰富、结构化现实之间的桥梁。通过明智地选择编码方式，我们可以将物理学、生物学甚至音乐的基本原理直接“烘焙”到我们的模型中。让我们踏上征程，看看这个强大的思想如何在科学与工程的各个领域中绽放光彩。

### 信息的几何学：相对与绝对

想象你有一份长文档，由多个段落组成。机器的一项关键任务是理解指代关系——一个句子中的代词指向另一个句子中的名词。现在，我们来玩个小游戏。我们把这些段落像一副牌一样打乱。什么样的“寻址系统”能让我们的机器追踪这些内部链接呢？

一种方法是**绝对**[位置编码](@article_id:639065)。这就像给原始、未打乱的文档中的每个词分配一个永久的街道地址。索引为105的词永远带有“105”的标签。但当我们打乱段落后会发生什么呢？原本在地址105的词现在可能位于地址502，但它仍然带着旧的“105”标签。为了找到它的先行词，模型现在必须在整个打乱的文档中搜索一个*当前*地址最接近*原始*地址105的词。这简直一团糟！这种策略的成功纯粹变成了运气问题；随着文档变长，找到正确链接的几率会急剧下降。

现在，考虑**相对**[位置编码](@article_id:639065)。这就像是指路。模型不是学习一个固定的地址，而是学习类似于“该指代的锚点位于同一段落内、它*之前*的15个词处”这样的信息。这种局部指令非常强大。当我们打乱段落时，每个段落的内部结构保持不变。“之前15个词”这个指令在新的、打乱的文档中和在旧文档中一样有效。这种策略不会失败。它*总是*能找到正确的链接，因为它的知识是局部的、相对的，而非全局的、绝对的 [@problem_id:3191208]。

这个简单的思想实验揭示了一个深刻的原理。相对[位置编码](@article_id:639065)提供了对抗全局[重排](@article_id:369331)的鲁棒性，这一特性在从文档分析到计算机视觉等任务中极为有用，因为我们希望模型能够识别一个物体，而无论它出现在图像的哪个位置。

### 编码自然的对称性

世界不仅仅是一系列项目的序列，它受规律和对称性支配。[位置编码](@article_id:639065)最优雅的应用，莫过于那些将这些对称性教给机器的应用。

#### 生命的对称性：DNA 的双螺旋

思考一下生命的蓝图——DNA序列。它是由字母A、C、G、T组成的字符串。但这不仅仅是任意的字符串。DNA的一个基本特性是其双[螺旋结构](@article_id:363019)，这意味着存在**反向互补对称性**。一个基因通常可以在一条链上读取，或者在另一条链上反向并以互补碱基（A↔T, G↔C）的方式读取。一个识别[序列基序](@article_id:356365) `AGT` 的生物过程，在许多情况下，也应该能识别其反向互补序列 `ACT`。

我们如何构建一个尊重这种对称性的模型呢？如果我们使用简单的[位置编码](@article_id:639065)，仅仅从左到右为碱基编号，那么位置 $p$ 和 $L-1-p$（一对相对于序列中心对称的位置）将被视为完全不相关。模型将不得不从头开始学习，去发现这两个截然不同的位置在某种程度上是对称的。

一个更优美的解决方案是设计一种内置了这种对称性的[位置编码](@article_id:639065)。我们可以不编码绝对位置 $p$，而是编码相对于序列*中心*的位置，并使用一个偶函数，比如余弦函数。这样，两个相对于中心对称的位置就保证会获得相同的位置值。有了这样的编码，模型天生就知道序列开头的基序应该与序列末尾其反向互补的基序被类似地对待。它不需要学习这种对称性；对称性是其“本能”的一部分 [@problem_id:2479929]。

#### 音乐的节奏

对称性不仅是空间的，也可以是时间的。想想音乐。一个关键元素是节拍（meter）——构成一首乐曲节奏骨架的强弱拍的循环模式。一个小节中的第‘1’拍与下一个小节中的第‘1’拍感觉相似。这是一种时间上的周期性。

使用[正弦位置编码](@article_id:642084)，我们可以直接教会 [Transformer](@article_id:334261) 这种音乐时间。如果一首乐曲的节拍周期为 $T$ 拍，我们可以使用诸如 $\sin(2\pi t/T)$ 和 $\cos(2\pi t/T)$ 的编码。根据定义，这些函数是以 $T$ 为周期的。当模型使用这些编码时，它的注意力机制也可以学会具有周期性。它可以学会，从时间 $t$ 的音符到时间 $u$ 的音符所付出的注意力，应该与从时间 $t+T$ 到 $u+T$ 的注意力相同。这使得模型能够捕捉到对我们感知音乐至关重要的重复性节奏结构 [@problem_id:3193549]。

#### 编码物理学本身

也许这一理念最深刻的应用是在物理学启发的机器学习中。想象一下，我们想构建一个神经网络来预测一根杆的温度如何随时间演变，这一过程由[热方程](@article_id:304863)所控制。我们可以将其视为一个黑箱问题，向模型输入数千个例子，并希望它能学习到底层的物理规律。

但是我们已经*知道*了物理规律！[热方程](@article_id:304863)的解可以表示为一系列基本模式（[傅里叶级数](@article_id:299903)）的和，其中每个模式都以其自身的特征速率呈指数衰减。为什么不直接把这些信息给神经网络呢？

我们可以构建一个[位置编码](@article_id:639065)，其中每个分量都对应于这些物理模式中的一个。一个点 $(x, t)$ 的编码将是一个向量，包含[空间模式](@article_id:360081)的值（如 $\cos(n\pi x/L)$）乘以其对应的时间衰减因子（$\exp(-\alpha(n\pi/L)^2 t)$），并由初始温度分布加权。这样，网络就不再需要从头开始发现[热方程](@article_id:304863)。它的任务被简化为学习这些物理上有效的[基函数](@article_id:307485)的正确*叠加*。这就像给学生一张完整的积分表，而不是要求他们从第一性原理推导每一个积分。这种方法不仅能产生更准确的模型，而且模型也更鲁棒、需要的数据更少，因为它们已经被教会了宇宙的基本法则 [@problem_id:2502931]。

### 实际应用中的[位置编码](@article_id:639065)：现代架构

这种编码结构的思想不仅仅是理论上的好奇心；它处于当今最先进人工智能系统的核心。

#### 用 Transformer 看世界

在**视觉 [Transformer](@article_id:334261) (ViTs)** 中，一张图像首先被切成一系列小图像块（patch）。然后，[Transformer](@article_id:334261) 在这个序列上自由发挥。但如果没有[位置信息](@article_id:315552)，模型只会看到一堆图像块，完全不知道某个图像块是来自图像的左上角还是中心。[位置编码](@article_id:639065)的作用就是将图像重新“缝合”起来，赋予每个图像块位置感。这就引出了一些实际问题：如果我们的图像尺寸各不相同（这在[医学成像](@article_id:333351)中很常见）怎么办？固定的绝对位置网格将不起作用。解决方案要么是巧妙地插值已学习的位置向量以适应新的网格尺寸，要么，更好的方法是，使用相对[位置编码](@article_id:639065)，这种编码天生对图像块数量的变化更具灵活性 [@problem_id:3199220]。

#### 图的特殊情况

那么，对于那些不是简单序列的数据，比如社交网络或分子，情况如何呢？这些数据被表示为图。对于**[图神经网络](@article_id:297304) (GNNs)**，出现了一个有趣的二分法。一方面，像[图卷积网络](@article_id:373416) (GCN) 这样的标准 GNN 架构，通常不需要显式的[位置编码](@article_id:639065)。这是因为其核心操作——[消息传递](@article_id:340415)——直接构建在图的连通性之上。一个节点通过聚合其直接邻居的信息来学习。这个过程在局部意义上是天生“位置感知”的，并且至关重要的是，它是**[置换](@article_id:296886)等变的**：如果你重新标记图中的节点，节点的输出特征也会以完全相同的方式被重新标记。这是一种优美的、内建的结构感知能力 [@problem_id:3106158]。

相比之下，Transformer 缺乏这种内置结构。如果你将图的节点输入给它，它会将它们视为一个无序集合。这就是为什么一个*没有*[位置编码](@article_id:639065)的 Transformer 是[置换](@article_id:296886)等变的，而添加绝对[位置编码](@article_id:639065)则是一种刻意*打破*该对称性并施加顺序的行为 [@problem_id:3106158]。

然而，GNN 的完美对称性有时也可能是一种诅咒。在一个高度规则的图（如一个简单的环）中，两个节点在结构上可能无法区分。仅靠[消息传递](@article_id:340415)无法将它们区分开。这时，我们必须重新引入[位置信息](@article_id:315552)。一个聪明的策略是选择几个“锚点”节点——可以把它们想象成地标。然后，我们可以为其他每个节点创建一个[位置编码](@article_id:639065)，该编码基于它到每个锚点的[最短路径](@article_id:317973)距离。只需几个精心挑选的锚点，我们就能给图中的每个节点一个独特的位置签名，从而打破对称性，让 GNN 能够区分它们 [@problem_id:3131899]。

### 一点警示：[混叠](@article_id:367748)的危险

尽管[位置编码](@article_id:639065)功能强大，但它们也伴随着一个关键的警告，这呼应了信号处理中的一个经典问题：混叠（aliasing）。你可能在电影中见过这种效应：汽车飞速旋转的车轮看起来变慢、停止甚至倒转。这是因为相机的帧率太低，无法捕捉车轮真实的高速运动，因此记录下了一个低速的“伪像”。

同样的情况也可能发生在神经网络中。想象一下，我们想要建模一个高频信号，比如一个快速[振荡](@article_id:331484)的波。我们的[位置编码](@article_id:639065)是一组基函数，如不同频率的正弦和余弦。如果我们编码中的最高频率低于我们试图建模的信号的频率，那么我们的模型就相当于在以过慢的速度“采样”世界。当在一些点上进行训练时，它可能会找到一个完美的拟合——但不是拟合真实的高频信号，而是拟合一个在采样点上恰好与真实信号匹配的低频*[混叠](@article_id:367748)*信号。模型会报告接近零的[训练误差](@article_id:639944)，让我们相信它已经学会了。但是，当我们要求它对新的点进行预测时，它所产生的低频幻象将与高频现实大相径庭 [@problem_id:3136712]。

这个警示性的故事强调，[位置编码](@article_id:639065)的力量——即通过提供丰富的特征集来增强模型表达能力的能力 [@problem_id:3098829]——关键取决于选择一个对于手头问题的复杂性而言足够丰富的基。

### 编码的艺术

归根结底，[位置编码](@article_id:639065)与其说是一种单一技术，不如说是一种深刻的设计哲学。它是一门艺术，要求我们深入审视一个问题，识别其基本结构——无论是语言的线性顺序、音乐的周期性节奏、DNA 的反向互补对称性，还是物理定律的本征模式——并将这种结构编织进机器学习模型的肌理之中。它证明了这样一个观点：最强大的模型并非那些从零开始学习的模型，而是那些被赋予了它们所要理解的世界之智慧的模型。