## 引言
训练真正的深度神经网络面临一个根本性挑战：当信息穿过数十甚至数百个层时，它往往会衰减至无，或爆炸成混乱状态。这一现象被称为信号消失与爆炸问题，在很长一段时间里，它都是构建更强大、更深模型的硬性障碍。我们如何能确保一个信号及其对应的误差梯度能够在网络的巨大深度中无失真地传播？

本文将通过探索最优雅且有效的解决方案之一：He（或 Kaiming）初始化，来解答这个关键问题。我们将揭示支撑这项技术的核心原理：信号方差保持。在接下来的章节中，我们首先将在“原理与机制”一节中从[第一性原理](@article_id:382249)出发推导该方法，剖析它是如何专门为流行的 ReLU [激活函数](@article_id:302225)量身定制的。然后，在“应用与跨学科联系”一节中，我们将探索其深远影响，展示这个简单的规则如何成为训练从 [ResNet](@article_id:638916) 到 Transformer 等现代架构的基石。这段探索之旅将从审视网络内信号流动的基本物理学开始。

## 原理与机制

想象一个极深的[神经网络](@article_id:305336)是一长串放大器。你将一个信号——你的输入数据——送入第一个放大器。它的输出成为第二个放大器的输入，依此类推，沿着一条由数百个层组成的长[链传递](@article_id:361648)下去。为了使最终的信号有用，每个放大器必须满足什么条件？如果每个放大器都略微减弱信号，经过一百个阶段后，你将只得到一片沉寂。如果每个放大器都略微增强信号，你将很快得到刺耳、失真的尖叫声——无用的噪声。要传输清晰的信号，每个放大器平均而言必须保持其强度。

在神经网络中，信号的“强度”就是其**方差**。如果每层激活值的方差都向零收缩，我们就遇到了**信号消失问题**。如果它不受控制地增长，我们就遇到了**信号爆炸问题**。[权重初始化](@article_id:641245)的艺术和科学，就在于设置网络放大器的初始“增益”，使得信号方差能在这条钢丝上保持平衡，在深入网络传播时既不消失也不爆炸。这个原理是训练真正深度架构的关键。

### 线性放大器：热身

让我们从最简单的情况开始：一个没有任何非线性激活函数的网络层。它只是一个[线性变换](@article_id:376365)。对于单个[神经元](@article_id:324093)，其输出（激活前）$z$ 是其输入 $a_j$ 的加权和：

$$
z = \sum_{j=1}^{n} w_j a_j
$$

这里，$n$ 是输入数量，即**[扇入](@article_id:344674)（fan-in）**。让我们为网络在诞生时（即初始化时）做一些合理的假设：权重 $w_j$ 和输入激活值 $a_j$ 都是均值为零的[独立随机变量](@article_id:337591)。那么输出的方差 $\mathrm{Var}(z)$ 是多少？

因为这些变量是独立的且均值为零，所以和的方差等于方差的和。两个独立的零均值变量乘积的方差等于它们各自方差的乘积。这给了我们一个非常简单的关系 [@problem_id:3167810]：

$$
\mathrm{Var}(z) = \sum_{j=1}^{n} \mathrm{Var}(w_j a_j) = \sum_{j=1}^{n} \mathrm{Var}(w_j) \mathrm{Var}(a_j)
$$

如果所有权重都从同一个方差为 $\mathrm{Var}(w)$ 的分布中抽取，并且所有输入都有相同的方差 $\mathrm{Var}(a)$，那么方程变为：

$$
\mathrm{Var}(z) = n \cdot \mathrm{Var}(w) \cdot \mathrm{Var}(a)
$$

为了保持方差不变——即 $\mathrm{Var}(z) = \mathrm{Var}(a)$——我们需要乘法因子为 1：

$$
n \cdot \mathrm{Var}(w) = 1 \quad \implies \quad \mathrm{Var}(w) = \frac{1}{n}
$$

这是初始化理论的基石。它告诉我们，每个权重的方差必须与[神经元](@article_id:324093)的输入数量成反比。这一洞见构成了著名的 **Xavier (或 Glorot) 初始化**的一半，该方法是为在原点附近近似线性的激活函数（如[双曲正切函数](@article_id:638603)）设计的。

### 守门员登场：ReLU 激活函数

现代深度学习因一个看似微小的改变而发生了革命：用简单、尖锐的**[修正线性单元](@article_id:641014) (Rectified Linear Unit, ReLU)** 替换了平滑、对称的激活函数。ReLU 定义为 $\phi(z) = \max\{0, z\}$。ReLU 就像一个门：如果输入 $z$ 是正数，它将原封不动地通过；如果是负数，它将被阻断，输出为零。

这种[门控机制](@article_id:312846)对我们的信号方差有什么影响？我们假设输入的预激活值 $z$ 的分布在零点附近是对称的（例如高斯分布），当 $z$ 是许多[独立随机变量之和](@article_id:339783)时，这是一个合理的假设。ReLU 函数随后会将恰好一半的信号置零。这不是一个温和的调整；而是一种粗暴的截断。我们的放大器不再是线性的了。我们该如何调整它的增益？

### 为 ReLU 重新校准：“2 倍因子”的秘密

为了找到适用于 ReLU 的正确权重方差，我们必须重新计算信号方差的变化方式。我们希望输出激活值的方差 $\mathrm{Var}(\phi(z))$ 等于前一层输入激活值的方差。输出激活值为 $\phi(z) = \max\{0, z\}$。

在对这些网络的分析中，从[第一性原理](@article_id:382249)推导出的关键洞见是信号的二阶矩会受到怎样的影响 [@problem_id:3166688] [@problem_id:3167810]。对于一个均值为零的对称变量 $z$，ReLU 函数会将其二阶矩减半：

$$
\mathbb{E}[(\max\{0, z\})^2] = \frac{1}{2} \mathbb{E}[z^2]
$$

这是因为计算二阶矩的积分 $\int z^2 p(z) dz$ 被分成了两半——定义域的负半部分贡献为零。在许多实际推导中，ReLU 之后激活值的[方差近似](@article_id:332287)于其二阶矩，因为 ReLU 引入的均值偏移影响较小。在这种简化下，我们有：

$$
\mathrm{Var}(\text{output}) \approx \frac{1}{2} \mathrm{Var}(z) = \frac{1}{2} \left( n \cdot \mathrm{Var}(w) \cdot \mathrm{Var}(\text{input}) \right)
$$

现在，为了保持方差稳定（$\mathrm{Var}(\text{output}) = \mathrm{Var}(\text{input})$），我们必须将新的乘法因子设为 1：

$$
\frac{1}{2} \cdot n \cdot \mathrm{Var}(w) = 1 \quad \implies \quad \mathrm{Var}(w) = \frac{2}{n}
$$

这就是著名的 **He (或 Kaiming) 初始化**。数字 2 的出现并非偶然；它正是对 ReLU 门控作用引入的 $\frac{1}{2}$ 因子的精准数学解药。这是一个绝佳的例子，说明了对机制——ReLU 的“截断”效应——的深刻理解如何直接导出一个使训练深度网络成为可能的实用解决方案。在某些网络中，如果出于某种原因，激活[神经元](@article_id:324093)的比例不是 $\frac{1}{2}$ 而是某个其他值 $\pi$，这个原理可以优美地推广：方差应设置为 $\frac{1}{n\pi}$ [@problem_id:3134394]。

### 回声之旅：为什么梯度也同样重要

[信号传播](@article_id:344501)只是故事的一半。另一半是**反向传播**，误差梯度从输出层向后传播到输入层，告诉每个权重如何调整自己。这个“回声”信号同样容易消失或爆炸。我们的初始化方案能经受住考验吗？

让我们看看梯度的方差在反向通过一层时是如何变化的。一个严谨的推导 [@problem_id:3125165] [@problem_id:3154467] 揭示了一个非常对称的结果。梯度方差的乘法因子是：

$$
\text{Backward Factor} = n \cdot \mathrm{Var}(w) \cdot \mathbb{E}[(\phi'(z))^2]
$$

这里，$\phi'(z)$ 是[激活函数](@article_id:302225)的[导数](@article_id:318324)。对于 ReLU，当 $z > 0$ 时[导数](@article_id:318324)为 1，当 $z  0$ 时[导数](@article_id:318324)为 0。所以，它的平方也只是 1 或 0。[期望](@article_id:311378) $\mathbb{E}[(\phi'(z))^2]$ 就是 $z > 0$ 的概率。对于我们假设的对称变量 $z$，这个概率是 $\frac{1}{2}$。

将此代入，后向因子变为：

$$
\text{Backward Factor} = n \cdot \mathrm{Var}(w) \cdot \frac{1}{2}
$$

为了防止[梯度消失](@article_id:642027)或爆炸，我们需要这个因子为 1。将其设为 1，我们得到条件 $n \cdot \mathrm{Var}(w) \cdot \frac{1}{2} = 1$，这再次得出：

$$
\mathrm{Var}(w) = \frac{2}{n}
$$

这是一个深刻而优雅的结果。对于一个 ReLU 网络，稳定激活值[前向传播](@article_id:372045)的初始化方法，同样也稳定了梯度的[反向传播](@article_id:302452)。正是这种完美的对称性使得 He 初始化如此有效。在 ReLU 网络中使用 Xavier 初始化 ($\mathrm{Var}(w) \propto 1/n$) 会导致后向因子约等于 $\frac{1}{2}$，随着网络加深，将导致梯度呈指数级消失 [@problem_id:3125165]。这一点可以由理论和经验模拟共同证实 [@problem_id:3199598]。

### 统一的初始化理论

我们发现的原理比仅仅适用于 ReLU 更具普适性。它为推导*任何*[激活函数](@article_id:302225)的正确初始化方法提供了一套方案。关键在于将权重方差与非线性函数的统计特性相匹配。

考虑**参数化 ReLU ([PReLU](@article_id:640023))**，其定义为 $f(z) = \max\{z, \alpha z\}$，其中 $\alpha$ 是用于负半部分的一个小参数 [@problem_id:3197644] [@problem_id:3199537]。遵循同样的逻辑，我们可以发现 [PReLU](@article_id:640023) 如何转换一个零均值对称信号的方差。作用于二阶矩的乘法因子结果为 $\frac{1+\alpha^2}{2}$。

为了在[前向传播](@article_id:372045)中保持方差，我们必须满足：

$$
n \cdot \mathrm{Var}(w) \cdot \left(\frac{1+\alpha^2}{2}\right) = 1 \quad \implies \quad \mathrm{Var}(w) = \frac{2}{n(1+\alpha^2)}
$$

这一个公式统一了我们之前的发现：
-   对于标准的 ReLU，$\alpha=0$，我们恢复了 He 初始化：$\mathrm{Var}(w) = \frac{2}{n}$。
-   对于一个“线性”[激活函数](@article_id:302225)，我们设置 $\alpha=1$，该公式得出 $\mathrm{Var}(w) = \frac{2}{n(1+1)} = \frac{1}{n}$。这就是基于[扇入](@article_id:344674)的 Xavier 初始化。

这表明 He 和 Xavier 不是两个独立的、临时的规则，而是一个强大原理的两个具体实例：**为了保持信号方差，权重的增益必须经过缩放，以精确抵消所选非线性函数的统计效应。**

这个原理也帮助我们创建一种平衡的初始化方法，比如原始的 Xavier 公式 $\mathrm{Var}(w) = \frac{2}{n_{\text{in}}+n_{\text{out}}}$，它平均了[前向传播](@article_id:372045)（依赖于[扇入](@article_id:344674) $n_{\text{in}}$）和后向传播（依赖于[扇出](@article_id:352314) $n_{\text{out}}$）的需求，以找到一个稳健的折衷方案 [@problem_id:3154467]。

### 关于偏置项的最后一点说明

在整个讨论中，我们忽略了偏置项 $b$。它重要吗？是的，很重要。如果我们从一个带方差的随机分布中初始化偏置，该方差将被加到预激活方差上，从而破坏我们精心校准的平衡 [@problem_id:3199849]：

$$
\mathrm{Var}(z) = n \cdot \mathrm{Var}(w) \cdot \mathrm{Var}(a) + \mathrm{Var}(b)
$$

额外的 $\mathrm{Var}(b)$ 项会将该层推向爆炸状态。最简单、最有效且几乎被普遍采用的解决方案，就是将所有[偏置初始化](@article_id:639166)为零。这是一个小细节，但对于让[权重初始化](@article_id:641245)的方差保持魔法发挥作用至关重要。

