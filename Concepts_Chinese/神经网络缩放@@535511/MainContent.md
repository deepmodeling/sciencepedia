## 引言
在构建更强大、更有能力的人工智能的探索中，最有效的策略之一就是简单地把模型做得更大。然而，简单粗暴地增加神经网络的规模往往会导致[收益递减](@article_id:354464)和天文数字般的计算成本。这就提出了一个关键问题：如何以一种有原则且高效的方式缩放[神经网络](@article_id:305336)，以释放其全部潜力？答案不仅在于把模型做得更大，更在于*更聪明地*把它们做得更大。

本文深入探讨[神经网络缩放](@article_id:641804)的科学，为其核心概念和实际意义提供了一份全面的指南。它致力于解决如何从“暴力”方法转向一种更优雅、更具预测性的模型设计框架的挑战。

本文首先涵盖缩放的**原理与机制**，探索深度、宽度和分辨率这些基本维度。文章揭示了为何一种被称为[复合缩放](@article_id:638288)的平衡方法至关重要，并解释了能够预测性能的缩放定律的可预测性。随后，**应用与跨学科联系**部分探讨了这些原理如何作为强大的工程工具被应用于处理现实世界的权衡问题，从针对特定硬件的优化到创建更符合人类感知的模型，甚至将其与自然界的缩放现象进行类比。

## 原理与机制

缩放一个基础[神经网络](@article_id:305336)使其更智能，可以类比为建筑师设计一个人工心智而非一栋建筑的任务。有三个基本的“权力杠杆”可以拉动。可以通过增加更多层使网络**更深**，通过在每层增加更多[神经元](@article_id:324093)使其**更宽**，或者可以给它输入更高**分辨率**的图像，让它从一开始就有更多细节可供处理。

这就是[神经网络缩放](@article_id:641804)的核心问题：如何操纵这三个维度——深度（$d$）、宽度（$w$）和分辨率（$r$）——来为给定的计算预算创造出最有能力的模型。探寻这个问题的答案揭示了现代人工智能中一些最美妙、最强大的原理。

### 雄心的代价

在我们梦想拥有大规模网络之前，必须面对一个非常实际的现实：计算不是免费的。每一个[神经元](@article_id:324093)、每一个连接、每一个像素都会增加成本。这种成本通常用 **FLOPs**（浮点运算次数）来衡量——这是网络所需进行的总算术量的一个代理指标。

那么，拉动我们这三个杠杆如何影响成本呢？让我们考虑一个典型的[卷积神经网络](@article_id:357845)。如果你将层数（深度）加倍，成本大约会加倍。很简单。但如果你将每层的通道数（宽度）加倍，成本不仅仅是加倍，而是变成了四倍！通道间的交互呈二次方增长。分辨率也是如此：将图像高度和宽度加倍意味着像素数是原来的四倍，因此成本也大约是四倍。

简而言之，我们称之为 $F$ 的[计算成本](@article_id:308397)，其缩放规律大致如下：
$$ F \propto d \cdot w^2 \cdot r^2 $$
这种关系是一个基本约束。它告诉我们，缩放宽度和分辨率远比缩放深度“昂贵”。这似乎令人望而生畏，但也正是人类创造力发挥作用的地方。研究人员发明了更高效的网络构建模块。一个典型的例子是**[深度可分离卷积](@article_id:640324)（Depthwise Separable Convolution, DSC）**，它巧妙地将[空间滤波](@article_id:324234)任务与通道混合任务分开。这种看似微小的架构变化，与标准卷积相比，可以在性能没有显著损失的情况下，将[计算成本降低](@article_id:349827)近10倍。通过如此大幅度地降低基线成本，这些高效的构建模块使得雄心勃勃的缩放成为一种现实的可能性 [@problem_id:3120081] [@problem_id:3119519]。

### 单维思维的谬误

有了我们的三个杠杆以及对其成本的清晰理解，最简单的策略就是选择其中一个并尽可能地推动它。想要一个更好的模型？那就让它更深。或者更宽。或者给它输入更大的图像。然而，这是一个经典的陷阱。

自然似乎钟爱平衡。将一个维度推向极致而忽略其他维度，会导致**收益递减**现象迅速出现。让我们思考一下原因。

想象你有一个非常非常深但极其狭窄的网络。它有数百层，但每层只有几个[神经元](@article_id:324093)。这样的网络无法学会识别复杂的特征；它在任何给定层中都没有足够的能力来表示它们。这就像试图只用三种颜色画一幅杰作。

现在，考虑相反的情况：一个浅而极宽的网络。它的少数几层中每一层都可能有数千个[神经元](@article_id:324093)。当给它输入一个小的、低分辨率的图像时，会发生什么？大量的[神经元](@article_id:324093)都试图在有限的信息中寻找模式。它们中的许多最终学会了完全相同的简单特征，比如边缘或角落。网络变得极度冗余和低效 [@problem_id:3119519]。

最直观的不匹配来自于单独缩放分辨率。如果你给一个浅层网络输入一张巨大的、高分辨率的图像，你就是在给它设下失败的圈套。网络的**感受野**——即每个[神经元](@article_id:324093)能“看到”的输入图像区域的大小——随着其深度的增加而增长。一个浅层网络只有一个小[感受野](@article_id:640466)。它也许能在一幅巨大的肖像中识别出鼻子或眼睛，但它永远无法拥有看到整张脸的上下文。要理解更大的图像，你需要更深的网络 [@problem_id:3119519]。

教训是明确的：这三个维度不是独立的。它们是相互交织的。最优的网络不是最深的，也不是最宽的，更不是分辨率最高的那个。它是那个*平衡*的网络 [@problem_id:3119640]。

### [复合缩放](@article_id:638288)的和谐之美

这就引出了**[复合缩放](@article_id:638288)**这一优雅的思想。与其摆弄三个独立的旋钮，我们能否创造一个单一的主旋钮，以[同步](@article_id:339180)、和谐的方式缩放所有三个维度？

这正是 [EfficientNet](@article_id:640108) 系列模型的创造者们所提出的。他们用一个单一的复合系数 $\phi$ 来定义深度、宽度和分辨率的缩放：
$$ d = \alpha^{\phi}, \quad w = \beta^{\phi}, \quad r = \gamma^{\phi} $$
在这里，$\alpha$、$\beta$ 和 $\gamma$ 分别是深度、宽度和分辨率的恒定缩放因子。现在，通过简单地增加 $\phi$，我们就可以沿着所有三个维度同时扩展网络，保持我们发现至关重要的架构平衡。

但是，这些“神奇数字”$\alpha, \beta$ 和 $\gamma$ 从何而来？它们并非凭空捏造。它们是通过一种有原则的、经验性的搜索找到的。可以从一个小型、高效的基线网络和一个固定的资源预算（例如，“我想让网络的成本增加一倍”）开始。然后，在此约束下搜索能使准确率最大化的 $\alpha, \beta$ 和 $\gamma$ 的组合。这个细致的过程揭示了特定架构家族的最佳缩放配方 [@problem_id:3119552]。例如，对于最初的 [EfficientNet](@article_id:640108)，发现的常数大约是 $\alpha \approx 1.2$, $\beta \approx 1.1$ 和 $\gamma \approx 1.15$，这告诉我们，对于那个特定的架构，最好是稍微更积极地缩放分辨率，其次是深度，然后是宽度。

### 水晶球：缩放定律的魔力

到目前为止，我们一直像建筑师一样，试图找出建造更大模型的最佳方法。现在，我们戴上物理学家的帽子，问一个不同的问题：我们获得的智能是否存在一种可预测的模式？如果我们将计算预算加倍，我们是获得双倍的智能，还是减半，或是其他什么？

答案是深度学习历史上最惊人、最深刻的发现之一。神经网络的性能遵循**幂律**。

这意味着模型的错误率会随着投入资源的某个可预测的幂次而降低。如果你在一个[对数-对数图](@article_id:337919)上绘制模型的错误率与用于训练它的计算量，你得到的不是一堆杂乱无章、不可预测的点，而是一条直线。

这种关系可以用一个惊人简单的公式来捕捉：
$$ e(C) \approx \kappa C^{-\alpha} $$
其中 $e$ 是错误率，$C$ 是计算量，而 $\kappa$ 和 $\alpha$ 是取决于模型架构和任务的常数 [@problem_id:3115194]。指数 $\alpha$ 告诉你你的网络将计算量“转化”为性能的效率。一个更高的 $\alpha$ 意味着你提升得更快、更好。

这意味着的意义是惊人的。它意味着你可以训练几个小型模型，绘制它们的性能，然后画一条直线。接着你可以延伸这条线——进行外推——以惊人的准确性预测一个规模大一百倍甚至一千倍的模型的性能，而训练这样一个模型可能需要数月时间和数百万美元。**缩放定律**已将构建大规模[神经网络](@article_id:305336)的艺术变成了一门科学。这就像拥有一个水晶球，让你在真正运行实验之前就能看到你最雄心勃勃的实验结果。

### 普适的交响曲

这种美妙的可预测性不仅仅是图像识别模型的一个怪癖。它似乎是一个普遍的原则。缩放定律最令人兴奋的前沿领域是在语言模型中，这些模型是像 ChatGPT 这样的系统背后的引擎。在这里，定律变得更加错综复杂且富有启发性。模型的损失（其错误的一种度量）不仅取决于用于训练它的计算量，还取决于模型本身的大小（参数数量 $N$）和训练数据集的大小 ($D$)。

这些人工智能巨头的缩放定律看起来像这样 [@problem_id:3193533]：
$$ \mathcal{L}(N, D) \approx A \cdot N^{-\beta} + B \cdot D^{-\delta} + \mathcal{L}_{\infty} $$
这个方程是一张藏宝图。它告诉我们，扩大模型规模和扩大数据规模会分别带来可预测的回报。指数 $\beta$ 和 $\delta$ 量化了这些回报。它甚至包含一个项 $\mathcal{L}_{\infty}$，代表一个不可约损失——这是无论如何缩放都无法突破的理论性能下限。这表明缩放不是单一的事情；它是一首由多个活动部分组成的交响曲，而每个部分都遵循其自身可预测的规则。

### 地图的边界：缩放的极限

有了这门强大的新科学，我们很容易相信可以通过缩放达到无限的智能。但每张地图都有边界。虽然准确率在提高，但这种提高的*成本*却呈指数级增长。最终，我们会撞上一堵收益递减的墙。

我们可以定义效率指标，比如每参数准确率或每 FLOP 准确率。起初，当我们从一个小型基线向上扩展时，这些效率通常会增加。一个稍大的模型不仅更准确，而且就其规模而言*更有效率*。但这并不会永远持续下去。随着模型变得巨大，准确率的增益开始饱和，而成本继续爆炸式增长。效率指标达到峰值，然后开始下降 [@problem_id:3119621]。

存在一个最佳的停止点 $\phi^{\star}$，超过这个点，每增加一大块计算量所带来的边际准确率增益就变得微乎其微。确定这个点是当今最关键的工程挑战之一。缩放定律给了我们地图和指南针，但它们没有给我们无限的资源。它们指引我们创造出尽可能强大的人工智能心智，直到我们雄心和预算的现实极限。

