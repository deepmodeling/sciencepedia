## 引言
什么是信息？它仅仅是屏幕上的数据、书本里的文字，还是某种更根本的、交织在现实结构中的东西？虽然我们日常使用这个词，但一个精确、科学的定义似乎难以捉摸。关键在于一个简单的直觉：一条告诉你已知信息的消息不包含任何信息，而一条报道罕见且惊人事件的消息则信息丰富。因此，核心问题是如何将这种“意外程度”的概念形式化和量化。本文通过引入强大的信息量概念来解决这一问题。

本次探索分为两个关键部分。在第一章“原理与机制”中，我们将奠定数学基础，从如何用“比特”衡量单个事件的[信息量](@article_id:333051)开始，逐步构建到著名的[香农熵](@article_id:303050)公式，该公式可以计算任何系统的平均[信息量](@article_id:333051)。我们将通过[热力学](@article_id:359663)揭示信息与物理世界之间深刻的联系。随后的“应用与跨学科联系”一章将带领我们巡礼现代科学。我们将看到这同一个概念如何让我们能够计算 DNA 的数据密度，解决物理学中的悖论，并量化我们数字通信的安全性。准备好去发现信息不仅是一个抽象概念，更是一种统一了科学世界不同角落的通用货币。

## 原理与机制

想象一下你收到一条消息。一条消息说：“今天早上太阳升起了。”另一条说：“一颗巨型小行星刚刚与地球擦肩而过。”哪条消息携带更多的*信息*？直觉上，是第二条。第一条是完全预料之中的事件，而第二条则是一个惊人、影响巨大的意外。这种简单的直觉正是我们量化信息的核心：**信息是对意外程度的度量**。

一个确定会发生的事件（$p=1$）完全不出所料，因此不提供任何新信息。一个极其罕见的事件（概率 $p$ 接近于零）则是一个巨大的意外，携带大量信息。我们如何构建一种数学语言来捕捉这一点？我们需要一个当概率趋于1时函数值趋于0，而当概率趋于0时函数值趋于无穷大的函数。对数函数完美地完成了这项工作。这引导我们得出单个事件的**[自信息](@article_id:325761)**（self-information）或**信息量**（information content）的基本定义：

$$I(p) = -\log_{b}(p)$$

负号的存在仅仅是因为概率（一个0到1之间的数）的对数总是负数，而我们更倾向于用正数来表示信息。对数的底数 $b$ 是我们选择的单位。如果我们使用底数2，我们的单位就是**比特**（bit），数字世界中熟悉的货币。如果我们使用底数10，单位是**哈特利**（hartley），如果我们使用自然对数（底数 $e$），单位是**奈特**（nat）[@problem_id:1666613] [@problem_id:2002076]。在接下来的讨论中，我们将坚持使用比特，因为它是思考选择和计算时最自然的单位。

这个定义有一个绝佳的性质。如果你有两个*独立*事件，两者都发生的概率是它们各自概率的乘积，$p_{1} \times p_{2}$。由于对数的性质，信息量变成了它们各[自信息](@article_id:325761)量的*和*：$I(p_{1}p_{2}) = I(p_{1}) + I(p_{2})$。这正是我们想要的！

如果我们尝试为一个概率大于1的事件（比如 $p=1.6$）计算信息量会发生什么？1.6的对数是一个正数，这意味着信息量 $I(1.6)$ 会是负数。这在概念上是荒谬的。获取信息意味着减少我们的不确定性。一个负值将意味着观察该事件*增加*了我们的不确定性，就好像我们比之前知道的更少了！这个简单的数学检验揭示了关于这个概念的一个深刻真理：[信息增益](@article_id:325719)总是一个非负量[@problem_id:1666609]。

### 最简单的情况：计算可能性

让我们从最直接的场景开始。想象一个系统可以处于 $N$ 个可能的状态之一，并且每个状态都是等可能的。这可能是一个公正的八面骰子[@problem_id:1868009]，一个有2500种等可能配置的专用医疗设备[@problem_id:1913643]，或者一个可以报告120种不同状况之一的环境传感器[@problem_id:1629226]。

如果有 $N$ 个等可能性，任何一个发生的概率是 $p = \frac{1}{N}$。与确定系统具体处于哪个状态相关的[信息量](@article_id:333051)是多少？我们只需将这个概率代入我们的公式：

$$I = -\log_{2}\left(\frac{1}{N}\right) = \log_{2}(N)$$

这个优美简洁的结果被称为**[哈特利熵](@article_id:326312)**（Hartley entropy）。它告诉我们，从 $N$ 个等可能选项中指定一个结果所需的信息量就是 $N$ 的以2为底的对数。例如，要从2500种可能性中识别出唯一正确的配置，你需要 $\log_{2}(2500) \approx 11.29$ 比特的信息[@problem_id:1913643]。这相当于在一个完美的“20个问题”游戏中，为了精确找出正确状态，你平均需要提出的“是/否”问题的数量。如果你有8种可能性，你需要 $\log_2(8) = 3$ 个问题（它在前四个吗？它在那组的前两个吗？它是那对中的第一个吗？）。

同样的逻辑直接应用于[统计力](@article_id:373880)学领域。考虑一个由 $N$ 个位点组成的磁性存储设备的简化模型。如果我们约束其净磁性为零，这意味着恰好有 $N/2$ 个位点必须是“上”，$N/2$ 个位点必须是“下”。可访问的微观构型总数 $\Omega$ 是选择哪 $N/2$ 个位点朝上的方式数，由二项式系数 $\binom{N}{N/2}$ 给出。如果所有这些微观状态都是等可能的，指定具体某一个所需的[信息量](@article_id:333051)就是 $\ln(\Omega)$ 奈特，或 $\log_2(\Omega)$ 比特[@problem_id:2002076]。这是我们将微观状态与宏观信息联系起来的根本基础。

### 现实世界：对不[均匀概率](@article_id:331880)进行平均

当然，世界很少如此整齐划一。在大多数真实情况下，不同的结果*并非*等可能。字母'E'在英文文本中出现的频率远高于'Z'。一个通信协议可能50%的时间使用一个符号，而其他三个符号的使用频率则低得多[@problem_id:1629789]。一个纳米机械开关可能由于热波动而以概率 $p$ 处于'ON'状态，以概率 $1-p$ 处于'OFF'状态[@problem_id:1604159]。

在这些情况下，我们如何讨论*整个系统*的信息量？从任何单次测量中获得的信息量将取决于结果。如果我们观察到一个非常罕见的结果，我们获得大量信息。如果我们观察到一个非常常见的结果，我们获得的信息就很少。为了描述这个系统，我们需要知道我们[期望](@article_id:311378)从一次测量中获得的*平均*[信息量](@article_id:333051)。这就是我们使用强大的**[期望值](@article_id:313620)**概念的地方。

让我们以简单的双稳态开关为例。观察到'ON'的信息是 $I_{ON} = -\log_{2}(p)$，观察到'OFF'的信息是 $I_{OFF} = -\log_{2}(1-p)$。为了找到平均信息量，我们将每个信息值乘以它发生的概率进行加权：

$$\text{Average Information} = p \times I_{ON} + (1-p) \times I_{OFF} = -p\log_{2}(p) - (1-p)\log_{2}(1-p)$$

这个表达式就是著名的**[二元熵函数](@article_id:332705)**（binary entropy function），它给出了任何具有两种结果的过程的平均信息量，单位是比特[@problem_id:1604159]。

将其推广到一个有 $N$ 种可能结果的系统，每种结果都有其自身的概率 $p_i$，平均[信息量](@article_id:333051)由**香农熵**（Shannon entropy）给出，记为 $H$：

$$H = -\sum_{i=1}^{N} p_i \log_{2}(p_i)$$

[香农熵](@article_id:303050)是所有可能结果的[自信息](@article_id:325761)的[期望值](@article_id:313620)。它告诉你对来自该源的消息进行编码所需的平均比特数，或者当你观察其状态时将体验到的平均意外程度。

至关重要的是要看到，如果所有结果都是等可能的（对所有 $i$ 都有 $p_i = 1/N$），[香农熵](@article_id:303050)就精确地简化为[哈特利熵](@article_id:326312)：$H = -\sum (1/N)\log_2(1/N) = -N \times (1/N)\log_2(1/N) = \log_2(N)$ [@problem_id:1622974]。这表明香农的公式是一个更通用、更强大的工具，它包含了更简单的情况。

此外，[香农熵](@article_id:303050)向我们展示了关于不确定性的一些深刻道理。对于固定数量的结果 $N$，当分布是均匀的（$p_i=1/N$）时，熵 $H$ 最大化。这是系统最不可预测的时候。随着概率变得更加倾斜，熵会减少。如果一个四符号字母表中的一个符号有50%的出现机会，那么这个系统比四个符号各有25%机会时更可预测。这种可预测性的增加意味着平均而言，意外程度更低，因此每个符号的[信息量](@article_id:333051)也更少。一个简单的计算表明，一个非均匀四符号系统的[香农熵](@article_id:303050)可能是 $1.75$ 比特，而仅仅计算四种可能性会让你高估信息量为 $\log_2(4) = 2$ 比特[@problem_id:1629789]。这 $0.25$ 比特的差异是“无知的代价”——即在分布不均匀时假设其均匀所付出的代价。

### [信息是物理的](@article_id:339966)：通往[热力学](@article_id:359663)的桥梁

很长一段时间里，信息似乎纯粹是一个数学抽象，是生活在代码和符号世界里的思维产物。但20世纪最惊人的发现之一是，信息是深刻且不可动摇地物理的。

线索是一个惊人的相似之处。[香农熵](@article_id:303050)的公式 $H = -\sum p_i \log_2 p_i$ 与[统计力](@article_id:373880)学中的[吉布斯熵](@article_id:314565) $S = -k_B \sum p_i \ln p_i$ 看起来惊人地相似，后者描述了一个[粒子系统](@article_id:355770)的[热力学](@article_id:359663)无序度。这仅仅是巧合吗？

并非如此。两者是成正比的。通过使用对数的换底公式，我们可以看到 $S = (k_B \ln 2)H$ [@problem_id:2462930]。这不仅仅是一个形式上的操作；它是连接两个世界的桥梁。比例常数 $k_B \ln 2$ 是抽象的“比特”信息与物理熵单位（[焦耳](@article_id:308101)/开尔文）之间的基本转换因子。它告诉我们一比特信息的最小[热力学](@article_id:359663)代价。

这种物理现实最著名的表述是**[朗道尔原理](@article_id:307021)**（Landauer's principle）。该原理指出，擦除信息是一个[热力学](@article_id:359663)不可逆的过程，必须向环境中耗散最小量的热量。在温度 $T$ 下，擦除一比特信息，至少需要 $Q_{min} = k_B T \ln 2$ 焦耳的能量。

想象一个纳米级设备，它随机地稳定在8个[量子态](@article_id:306563)中的一个。了解其状态所需的信息是 $\log_2(8) = 3$ 比特。这个信息被记录在一个内存寄存器中。现在，为了准备下一个周期，我们必须重置那个寄存器——我们必须擦除那3个比特。根据[朗道尔原理](@article_id:307021)，这种擦除行为不是免费的。它必须伴随着至少 $3 \times (k_B T \ln 2)$ [焦耳](@article_id:308101)的热量耗散到周围环境中[@problem_id:1868009]。这不是我们当前技术的限制；这是一个基本的自然法则。

所以，信息不仅仅是一个想法。它具有[质能等价](@article_id:306676)性。它有[热力学](@article_id:359663)成本。思考、计算甚至遗忘的行为都受到与支配恒星和引擎相同的物理定律的约束。比特的抽象世界和原子的有形世界，最终是同一个世界。