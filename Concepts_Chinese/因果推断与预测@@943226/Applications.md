## 应用与跨学科联系

区分影子的轮廓与投射影子的光线，区分预测风暴与知晓如何平息风浪，这不仅仅是哲学上的细枝末节，而是现代科学核心的一项根本选择。在我们迄今的探索中，我们已经研究了将预测与因果推断分离开来的原理。现在，让我们看看这些思想在实践中的应用，因为正是在医学、生态学和人工智能这些丰饶的土壤中，这些概念才真正焕发生机，塑造着我们如何疗愈、如何理解我们的星球以及如何构建我们的未来。

每一项科学探索，无论是在国家公园广袤的荒野中，还是在实验室无菌的环境里，都始于一个问题。而问题的性质决定了整个发现的路径。我们可以将这些问题看作一个自然的层级结构。首先，我们有*描述性*问题：“世界是什么样子的？”接着是*机制性*或因果性问题：“它为什么是那个样子？控制它的杠杆是什么？”最后是*预测性*问题：“根据我们现在所见的，明天它会是什么样子？”一位研究湖泊的生态学家可能首先调查营养物质和藻类的水平（描述），然后进行一项添加磷的实验，看它是否引起藻类大量繁殖（机制），最后建立一个模型，根据今年的土地使用情况来预测明年夏天的[水质](@entry_id:180499)（预测）[@problem_id:2538633]。每个问题都需要不同的工具、不同的思维模式和不同种类的证据。

### 预测的艺术：由数据制成的水晶球

让我们从预测的艺术开始。其目标简单而务实：利用一切可用的信息做出尽可能好的预报。想象一下，医生们希望识别出在未来五年内有高风险患上心脏病的患者。他们可以建立一个临床风险评分模型，这是一个精美的预测引擎，以年龄、胆[固醇](@entry_id:173187)和血压等数据为养料。这样的模型在它的本职工作上可以表现得异常出色，能够标记出需要更密切关注的个体[@problem_id:4578301]。

但这颗水晶球有其局限性。模型中的数字——即“系数”——并非健康良方。它们不会告诉你，如果你将胆[固醇](@entry_id:173187)强制降低10个点，你的风险就会下降某个特定的数值。它们只反映了用于构建模型的人群中所见的复杂关联网络。这是一个关键点：预测模型是其环境的产物。如果你将一个在某个国家开发的风险评分模型应用于另一个生活方式和遗传背景不同的国家，其准确性可能会骤降。模型本身没有错，只是它的上下文改变了。因此，预测的艺术也包含着*重新校准*的谦逊——温和地将模型调整到一个新的现实中，就像将收音机调到一个新电台，而不是假装你发现了一个普适的自然法则[@problem_id:4578301]。

预测事业也为粗心者布下了微妙的陷阱。设想一个团队正在构建一个人工智能来预测住院患者的急性肾损伤。他们用成千上万的患者记录训练了一个强大的模型，并取得了惊人的准确率。但当在医院部署时，这个模型却毫无用处。为什么？在他们对数据的狂热追求中，他们让模型偷窥了未来。他们用于训练一个下午3点发生肾损伤的患者的数据中，包含了下午4点的实验室结果。模型变得非常擅长于*检测*肾损伤的*后果*——即医生在做出诊断*后*开出的一系列检查和治疗——而不是*预测*肾损伤本身[@problem_id:5219460]。它学会了识别影子，而不是投下影子的事件。这揭示了一个深刻的教训：即使在看似直接的预测世界里，我们也必须尊重[时间之箭](@entry_id:143779)。好的预测需要一定程度的因果思维。

### 探寻“为什么”：杠杆、反事实与混淆

预测告诉我们可能会发生什么。因果关系告诉我们我们能*改变*什么。提出一个因果问题，就是询问宇宙的杠杆在哪里。如果我拉动这个杠杆——给予这种药物，实施这项政策，改变这种饮食——会发生什么？

这是一个难度无限大的问题。世界是一个由相互关联的变量组成的纠缠不清的混乱体。一个简单的相关性可能具有深度欺骗性。想象两个平行宇宙，它们的构造使得在统计学家的仪器看来，它们在观测上是完全相同的。在这两个宇宙中，变量 $X$ 都以完全相同的方式与结果 $Y$ 相关。在任一宇宙中训练的预测模型都会学到相同的简单规则：$Y$ 倾向于等于 $X$。在宇宙A中，这是因为 $X$ 是 $Y$ 的直接原因。但在宇宙B中，存在一个隐藏的、未被观察到的“混淆因子” $U$，它同时导致了 $X$ 和 $Y$ [@problem_id:3178830]。

现在，假设你进行干预。你伸手进入这些宇宙，将 $X$ 的值设定为5。在宇宙A中，$X$ 是一个原因，于是 $Y$ 尽职地变成了5。但在宇宙B中，你的干预打破了混淆因子 $U$ 和 $X$ 之间的联系。混淆因子对 $Y$ 的影响依然存在，但 $X$ 和 $Y$ 之间的相关性消失了。你的干预对 $Y$ 毫无影响。一个预测模型，无论多么强大，只要它仅仅在观测数据上训练，就无法区分这两个宇宙。将预测模型误认为因果模型，就像只看后视镜开车一样。

这种挑战在医学领域无处不在。医生们在评估一种新的癌症影像生物标志物时，需要知道基于该标志物给予治疗是否真的能帮助患者[@problem_id:4531880]。一个简单的预测模型可能会发现，那些生物标志物评分为“高风险”且接受治疗的患者预后很差。但这可能是因为临床医生凭其智慧，已经在给病情最重的患者施用该疗法——这种现象被称为“适应症混淆”。要回答这个因果问题，“如果我给某个特定患者用药，相对于不用药，会发生什么？”，我们必须进入*[潜在结果](@entry_id:753644)*的世界。我们必须尝试估计那些接受治疗的患者假如没有接受治疗会发生什么，以及那些未接受治疗的患者假如接受了治疗又会发生什么。这需要基于对世界因果模型的审慎统计调整，并使用那些永远无法被完全证实、只能进行推理的假设。

### 弥合鸿沟：当因果思维提升预测能力

虽然预测与因果截然不同，但它们并非对立。事实上，最稳健的预测模型往往建立在因果原则的基础之上。

想象一下，你要建立一个模型来预测一位开始服用糖尿病药物[二甲双胍](@entry_id:154107)的患者在一年内心力衰竭的风险[@problem_id:4853254]。一种天真的方法可能会让你误入歧途。你可能会纳入已经服用该药物多年的患者，但这些“现存使用者”是能够耐受该药物的幸存者，这是一个有偏倚的样本。你可能会因为错误地定义起始时间而无意中造成“不朽时间偏倚”，给了患者一段现实中不存在的无风险期。要建立一个真正有用、能够推广到真实临床环境的预测模型，你必须像因果流行病学家一样思考。你必须在你的数据中模拟一个“目标试验”：定义一个明确的起点（第一次开具处方），建立一个“新用户”队列，并仔细追踪人们换药或停药时发生的情况。因果思维为构建一个值得信赖的预测模型提供了蓝图。

这种关系可能更为微妙。思考一下预测未来骨质疏松性骨折的挑战[@problem_id:4554443]。患者既往骨折史是未来骨折的一个极其强大的*预测因子*。从纯粹预测的角度来看，我们不需要问为什么。我们只需使用这条强有力的信息。但这个“为什么”非常有趣，并具有深远的意义。既往骨折之所以是一个强有力的预测因子，有两个原因：它是一个未被观察到的潜在脆弱性的标志（充当了混淆因子的代理变量），并且损伤本身可能削弱了身体（充当了直接原因）。如果我们的目标是估计一种新的骨骼强化药物的因果效应，我们就必须非常仔细地思考如何在模型中处理“既往骨折”这个变量，以避免纠缠的因果路径和像“[对撞偏倚](@entry_id:163186)”这样的偏倚。同一个变量，根据我们的问题是预测性的还是因果性的，扮演着完全不同的角色。

### 前沿：因果、预测与一个更公平的世界

今天，预测与因果之间的这种相互作用正处在应对社会一些最紧迫挑战的最前沿。

在现代遗传学中，我们看到了一个显著的分歧。一方面，我们有多基因风险评分（PRS），这是终极的预测工具。PRS汇总了数百万个遗传变异的微小效应，来预测个体患某种疾病的风险。它是相关性的一件杰作，不关心任何单个变异的“为什么”[@problem_id:4594733]。另一方面，我们有[孟德尔随机化](@entry_id:147183)（MR），这是一种卓越的因果推断技术。MR利用遗传变异作为一种天然的“随机试验”，来估计一个可改变的风险因素（如胆[固醇](@entry_id:173187)）对一种疾病的因果效应。对于MR来说，一个通过非研究路径影响疾病的基因变异——一种称为多效性的现象——是一个可能使研究无效的潜在灾难。然而对于PRS来说，同一个多效性变异可能是一个有价值的特征，能提高其预测能力！这两种方法，源于相同的遗传数据，却在寻找相反的特质，因为它们回答的是不同的问题。

也许最至关重要的前沿领域是在追求公平与平等的斗争中。几十年来，一种用于估计肾功能（eGFR）的常用算法包含了一个“种族校正”——一个对被识别为黑人的患者进行的数学调整。这种做法日益受到批评，因为种族是一种社会建构，而非生物学变量，使用它可能会固化健康差距。解决方案不仅仅是移除这个术语，因为那样会使模型对所有人都变得不那么准确。解决方案是用其真正的因果中介物来取代这个粗略的代理变量[@problem_id:4987598]。从历史上看，种族术语的理由很大程度上是基于人群平均肌肉量的差异，而肌肉量是肌酐水平的一个关键决定因素。通过直接测量真正的生物学原因（例如，肌肉量）并将其纳入一个新的预测模型中，我们可以创建一个不仅更准确，而且也更公平的算法。这需要我们的工具进行全面整合：一个因果理解来识别正确的测量变量，以及一个严谨的预测框架来确保最终的模型经过良好校准，并对所有患者亚组都表现公平。

### 结论：为正确的工作选择正确的工具

我们回到了开始的地方：问题。一个强大的败血症护理人工智能模型可以通过查看患者病历中的所有数据，以惊人的准确性来预测死亡率[@problem_id:4411284]。但如果它使用了*治疗后*的信息，它可能会学到虚假的关联。例如，它可能会注意到接受某种药物的患者往往会死亡，因此建议不要给予该药物。它未能理解，该药物是作为拯救最危重患者的最后手段而给予的。它的预测准确性很高，但其建议却是有害的。

第二个基于因果原则构建的人工智能，将被限制为仅使用*治疗决策前*可用的信息。它在历史数据集上的原始预测准确性可能会更低。但它的建议将基于对治疗真实效果的无偏估计。要知道哪个AI真正更好，我们不能只看一个标准的准确性指标。我们必须进行一种不同类型的测试——一种“[离策略评估](@entry_id:181976)”——来估计哪个AI的*策略*如果被遵循，会在现实世界中带来更好的结果。

这是最终的教训。抽象地讲，不存在所谓的“好模型”。只有适合工作的正确工具。预测为我们提供了未来可能样貌的地图。因果推断则给了我们方向盘和指南针。真正的智慧在于知道该使用哪一个，以及何时将它们结合使用，以便在我们试图理解和改善的这个复杂世界中导航。