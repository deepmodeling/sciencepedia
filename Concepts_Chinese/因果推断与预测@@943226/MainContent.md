## 引言
在数据科学领域，有两个问题占据主导地位：“将会发生什么？”和“如果我们采取不同措施会怎样？”。前者属于**预测**的范畴，即利用观察到的模式来预报未来。后者则属于**因果推断**的范畴，旨在理解我们行动的后果。虽然两者看似相似，但混淆这两种追求是现代科学、医学和人工智能领域最严重的错误之一，它会导致有缺陷的政策和有害的干预。本文旨在揭开这一关键区别的神秘面纱。首先，在“原理与机制”一章中，我们将探讨区分预测与因果的基础理念，利用[潜在结果](@entry_id:753644)等概念和辛普森悖论等悖论来揭示为何关联不等于因果。随后，“应用与跨学科联系”一章将展示这一理论上的分野如何在现实世界场景中体现，涵盖从临床试验、生态学研究到开发公平且可行动的人工智能等多个方面。通过理解这种差异，我们可以从仅仅观察世界，迈向有效地为更美好的未来而改变世界。

## 原理与机制

想象一下，你正站在河岸上。你脑海中可能会浮现两个截然不同的问题。首先，你可能会看着天上的云、风向以及上游的水位，然后问：“明天河水泛滥的概率是多少？”这是一个**预测**问题。其次，你可能会看着一座新建的大坝，然后问：“如果我们打开泄洪闸，下游的水位会改变多少？”这是一个**因果推断**问题。

这两个问题虽然看似相关，却分属于不同的思想领域。第一个问题是关于观察世界的现状并预测其自然演变。第二个问题是关于“做”某件事，即干预世界并理解其后果。这种区分并非无足轻重的哲学思辨，而是贯穿所有科学、工程和医学领域最深刻、最实际的分野之一。将两者混为一谈可能导致灾难性的错误，尤其是当我们构建人工智能系统来做出关键决策时。

### 两个世界：关联与干预

让我们把这个问题具体化。**预测**的任务是建立一个模型，在给定一组线索（或称特征）$X$ 的情况下，能够准确地猜测一个结果，我们称之为 $Y$。想象一个医疗风险计算器，它接收患者的年龄、血压和胆[固醇](@entry_id:173187)水平（$X$），然后输出他们十年内心脏病发作的风险（$Y$）。预测模型的数学目标是**[条件概率](@entry_id:151013)** $\Pr(Y=1 \mid X=x)$。它回答的是：对于具有这些特定特征的患者，观察到该结果的频率是多少？模型的成功与否，取决于它能否在数据中找到模式和关联，并且这些模式和关联在来自相同环境的新患者身上得到验证。模型不需要知道“为什么”高胆[固醇](@entry_id:173187)与心脏病发作相关，只需要知道它们之间存在关联。[@problem_id:4507645]

另一方面，**因果推断**处理的是“如果……会怎样”的问题。如果我们给这位患者服用他汀类药物会怎样？如果我们不给药呢？为了将此形式化，我们使用优美的**潜在结果**语言。对于任何个体，如果他们接受治疗（$A=1$），我们会观察到一个[潜在结果](@entry_id:753644) $Y(1)$；如果他们不接受治疗（$A=0$），则会有一个[潜在结果](@entry_id:753644) $Y(0)$。当然，对于任何给定的人，我们永远只能观察到其中一种情况。因果推断的目标是估计这两个世界之间的差异，例如**平均因果效应**（ACE），即 $\mathbb{E}[Y(1) - Y(0)]$，它告诉我们治疗对整个人群的平均影响。为了做出个性化决策，我们可能想知道**条件平均处理效应**（CATE），即 $\mathbb{E}[Y(1) - Y(0) \mid X=x]$，它告诉我们对于具有特定特征 $X=x$ 的患者，治疗的效果如何。

关键在于，因果效应是一个**干预性**的量，在因果图的语言中表示为 $\Pr(Y=1 \mid \mathrm{do}(A=a), X=x)$，它代表了在我们*干预*将治疗设定为 $a$ 之后 $Y$ 的分布。预测模型根本不是为回答这类问题而构建的。它的世界是被动观察，而非主动干预。一个在预测方面表现出色的模型，在用于决策时可能会产生危险的误导，而这正是麻烦——也是魔力——开始的地方。[@problem_id:4404403]

### [辛普森悖论](@entry_id:136589)：当整体呈现谎言

将关联误认为因果的最著名陷阱是**混淆**。想象一个非常普遍的场景，它甚至有自己的名字：“适应症混淆”。这种情况发生在病情最重的患者被优先给予新的或更激进的治疗时。对数据的粗略观察可能会表明该治疗是有害的，这仅仅是因为接受治疗的人本身就处于更高的风险之中。

让我们通过一个数值例子来全面揭示这个悖论。假设一家诊所启动了一项高血压预防项目（$A=1$ 表示参加， $A=0$ 表示未参加），旨在降低五年内发生中风的风险（$Y=1$）。患者在基线时被分为低风险组（$C=\text{L}$）和高风险组（$C=\text{H}$）。[@problem_id:4519156]

该项目的真实效果（只有自然知晓）如下：
-   对于低风险患者，该项目是有益的：中风风险从 $0.10$ 降至 $0.05$。
-   对于高风险患者，该项目也是有益的：风险从 $0.40$ 降至 $0.30$。

在*每一个分层中*，该项目都有效。其因果效应是明确无误的好。但现在，让我们看看在现实世界中数据可能是如何被收集的。临床医生根据他们的最佳判断，更有可能让高风险患者参加该项目。假设入组情况反映了这一点：
-   在接受治疗者中，$80\%$ 是高风险人群。
-   在未接受治疗者中，只有 $20\%$ 是高风险人群。

现在，让我们扮演一个数据科学家的角色，他看不到[因果结构](@entry_id:159914)，只是在寻找关联。我们计算治疗组与未治疗组中观察到的总体中风风险。根据[全概率定律](@entry_id:268479)：
-   治疗组的风险：$\Pr(Y=1 \mid A=1) = (0.05 \times 0.20) + (0.30 \times 0.80) = 0.01 + 0.24 = 0.25$。
-   未治疗组的风险：$\Pr(Y=1 \mid A=0) = (0.10 \times 0.80) + (0.40 \times 0.20) = 0.08 + 0.08 = 0.16$。

结果令人震惊。在观测数据中，治疗组的中风风险（$25\%$）显著*高于*未治疗组（$16\%$）！一个纯粹旨在最大化准确性的预测模型，会学习到这种正向关联。它会正确地得出结论：参加该项目是更高中风风险的*预测因子*。但若将此预测解读为该项目*导致*伤害的证据，则是一个灾难性的跳跃。数据在悄声说谎。总体关联与每个子组内的真实因果效应完全相反。这就是**[辛普森悖论](@entry_id:136589)**。它表明，即使一个完美准确的预测模型，对于因果问题也会给出错误的答案，这并非因为模型有缺陷，而是因为它被设计来回答的问题（预测）对于做出决策（因果）而言是错误的问题。[@problem_id:4437928]

### 信息的诡计：因果路径上的危险

混淆是一个众所周知的敌人。但数据中还潜藏着更微妙、更奇特的危险，即使是警惕的研究人员也可能失足。要看清它们，我们需要一个新工具：一张简单的因果关系地图。**有向无环图（DAG）**就是这样一种图示——其中的箭头代表直接的因果关系。

思考**对撞因子**这个经典陷阱。对撞因子是两个其他变量的共同*结果*。想象一个简化的世界，其中学术上的“成功”是由“天赋”和“努力”共同引起的。在 DAG 中，这表示为 `Talent -> Success - Hard Work`。现在，如果我们决定只研究成功人士，会发生什么？在这个被选定的群体中，我们可能会发现一个奇怪的负相关：非常有天赋的人似乎不那么努力，而非常努力的人可能不是最有天赋的。这种关联是虚假的；它是我们选择只关注对撞因子“成功”而产生的人为现象。对一个对撞因子进行条件限制，会打开其父变量之间一条非因果的关联路径。

这不仅仅是一个玩具例子。让我们回到医学领域。假设我们正在研究一款健身应用（$A$）对糖尿病风险（$Y$）的影响。一个人的基线健康焦虑（$U$，未被测量）可能会影响他们患糖尿病的几率，以及他们频繁自测血糖水平（$C$）的可能性。该应用本身也可能鼓励用户自测血糖。这就给了我们一张因果地图，其中应用采纳（$A$）和健康焦虑（$U$）都是血糖检测（$C$）的原因。即，$A \rightarrow C \leftarrow U$。变量 $C$ 是一个对撞因子。[@problem_id:4578267]

-   对于**预测**而言，了解患者的血糖检测频率（$C$）非常有用。它是他们潜在健康焦虑（$U$）的一个标志，并与他们的糖尿病风险相关。将 $C$ 包含在预测模型中可能会提高其准确性。
-   对于**因果推断**而言，对 $C$ 进行条件限制是一场灾难。它打开了应用采纳（$A$）和健康焦虑（$U$）之间的虚[假路径](@entry_id:168255)。这在应用和糖尿病风险之间制造了一种虚假的、非因果的关联，这种关联通过未被测量的焦虑变量传递。这种效应被称为**对撞分层偏倚**，它会污染我们对该应用真实因果效应的估计。

在这里，我们看到了另一个深刻的真理：对于预测而言最优的变量集，对于因果推断可能是有害的。在回答“如果……会怎样”的问题时，添加更多信息并非总是有益。在模型中包含哪些变量，完全取决于你所要问的问题。[@problem_id:4974051]

### 真相时刻：为何因果模型是可行动的

这一区别之所以重要，其最终原因在于我们不仅仅想理解世界，我们还想改变它。我们实施政策，施予治疗，并部署能够采取行动的人工智能系统。行动就是一种干预。如果我们对世界的模型纯粹是关联性的，那么在我们用它来行动的那一刻，它很可能就会失效。

让我们实时见证一个模型的失效。想象一个人工智能模型在观测性医院数据上进行训练，用以通过一个治疗后生物标志物（$M$）来预测死亡风险（$Y$）。它学习了关系 $P(Y=1 \mid M)$，这是一个复杂的混合体，包含了治疗的真实效果、患者严重程度造成的混淆，以及数据收集时所采用的治疗模式。[@problem_id:4410012]

假设该模型学习到，当生物标志物存在时（$M=1$），死亡风险约为 $27\%$。现在，医院实施了一项新政策：用一种特定药物治疗*所有人*。这是一种干预，$\mathrm{do}(T=1)$。关于谁能获得治疗的旧规则被抛弃了。当我们在新世界里重新计算真实风险时，我们发现对于有生物标志物（$M=1$）的患者，风险现在只有 $21\%$。

模型预测的 $27\%$ 现在是危险的错误。它**校准错误**了。它高估了风险，因为它所训练的观测数据中，构成生物标志物存在的病人和健康人的混合比例是不同的。它学到的[关联关系](@entry_id:158296)不是一个稳定、结构性的自然法则，而是一个特定环境下短暂的模式。当我们试图用这张地图来规划新航线时，它就变得毫无用处了。

这就是为什么，要让一个人工智能变得“可行动”——也就是说，让它能安全地指导决策——它就不能仅仅是一个预后模型。它必须是一个**因果模型**。它必须能够回答反事实的“如果……会怎样”的问题，以便选择能带来最佳结果的行动。这需要一套不同的工具、假设和验证策略。[@problem_id:4404403] [@problem_id:4974051]

### 不同问题，不同工具

预测与因果之间的深刻鸿沟延伸到了我们使用的工具本身。

[预测建模](@entry_id:166398)是一场[模式识别](@entry_id:140015)的游戏。我们可以使用像 ARIMA 这样的灵活算法进行时间序列分析，或者使用强大的[机器学习模型](@entry_id:262335)，并通过在预留的测试数据上测量其准确性（交叉验证）来验证它们。性能就是证明。[@problem_id:2438832]

因果推断是一场设计的游戏。黄金标准是随机对照试验（RCT），它通过设计来打破混淆的联系。当我们只有观测数据时，我们必须依赖巧妙的准实验设计——如回归断点或[工具变量](@entry_id:142324)——以及一系列关于世界[因果结构](@entry_id:159914)的、强有力的、无法检验的假设。验证不仅包括检查数据，还包括进行敏感性分析，以观察如果我们的假设错误，结论会如何改变。即使是像你需要多大样本量这样的实际考虑，也受制于这两种目标的不同原则。[@problem_id:4979734] [@problem_id:3148994]

理解“看”与“做”之间、一个存在的世界与一个可能存在的世界之间的区别，是构建能够推理、行动并真正帮助我们塑造更美好未来的工具的第一步，也是最关键的一步。

