## 引言
从分子键到社交圈，世界是由网络编织而成的。理解这些相互关联的系统需要一类能够从图结构数据中学习的模型。核心挑战在于创造一种能够以强大且有原则的方式解释这些关系的人工智能。模型如何才能在不受任意数据排序影响的情况下学习[网络结构](@entry_id:265673)，又如何能从简单的局部规则中捕捉复杂的全局属性？

消息传递[神经网](@entry_id:276355)络（MPNNs）提供了一个优雅而强大的答案。它们建立在一个简单直观的想法之上：网络中的实体通过与其邻居进行“对话”来演化。本文旨在探索这一深刻的[范式](@entry_id:161181)。首先，我们将剖析支配这些对话的基本原理和机制，探索构成 MPNN核心的消息、聚合和更新步骤，并检视该模型固有的优势与局限。然后，我们将在第二章“应用与跨学科联系”中拓宽视野，揭示这个单一框架如何作为一种贯穿科学与工程的统一语言，将从[物理模拟](@entry_id:144318)、分子设计到神经科学和因果推理等一切联系起来。

## 原理与机制

任何科学模型的核心都有一个简单而强大的思想。对飞机而言，是升力原理。对计算机而言，是二[进制](@entry_id:634389)开关。对于处理网络——从分子到社交圈，再到宇宙结构——的人工智能分支而言，其核心思想是我们都能直观理解的：对话。想象一个图，它不是由线和点构成的静态网络，而是一个由实体组成的动态社会，每个实体都有自己的状态或身份。这些实体如何演化？它们与邻居交谈。这就是**[消息传递](@entry_id:751915)[神经网](@entry_id:276355)络（MPNN）**的精髓。

### 对话网络

让我们将这场“对话”分解为其基本部分。想象图中的任意一个节点——细胞中的一个蛋白质、社交网络中的一个人、晶体中的一个原子。在任何给定时刻，这个节点都有一个“状态”，即一个描述它的数字集合（一个向量）。我们称之为它的隐藏状态，$\mathbf{h}_v$。MPNN 通过模拟局部对话，在一系列步骤（或层）中更新此状态。每个更新周期包括三个阶段[@problem_id:3317113]：

1.  **构建消息 ($\psi$)：** 首先，对于一个给定节点 $v$，它从它的每个邻居 $u$ 接收消息。但什么是消息？它不仅仅是邻居的状态。一条真正有意义的消息可能取决于发送者（$u$）、接收者（$v$）以及它们之间关系的性质（连接它们的边 $\mathbf{e}_{uv}$）。因此，对于每个邻居 $u$，我们计算一个消息向量：$\mathbf{m}_{uv} = \psi(\mathbf{h}_u, \mathbf{h}_v, \mathbf{e}_{uv})$。函数 $\psi$ 是一个小型的[神经网](@entry_id:276355)络，它学习如何构建最有效的消息。

2.  **聚合声音 ($\square$)：** 现在，节点 $v$ 被来自所有邻居的消息轰炸。它不能按特定顺序逐一处理这些消息。为什么？因为图的本质决定了它没有“第一个”或“最后一个”邻居。邻域 $\mathcal{N}(v)$ 是一个节点*集合*，我们施加的任何排序都是任意的，仅仅是我们存储数据方式的产物。如果我们的模型输出依赖于这种任意顺序，那就像一位物理学家的实验结果会因为他给试管贴标签的顺序是从左到右还是从右到左而改变一样——这是对科学原则的灾难性违背。

    为了尊重这种基本对称性，聚合步骤必须是**[置换](@entry_id:136432)不变的**。无论消息处理的顺序如何，结果都必须相同。这个聚合函数 $\square$ 最常见的选择是大家熟悉的统计学主力：逐元素的 `sum`、`mean` 或 `max`。每一种都代表了一种不同的倾听策略。倾听之后，节点得到一个单一的聚合消息，$\mathbf{m}_v = \square_{u \in \mathcal{N}(v)} \mathbf{m}_{uv}$。

3.  **更新状态 ($\phi$)：** 最后，节点 $v$ 更新自身状态。它取其旧状态 $\mathbf{h}_v$，并将其与刚刚听到的聚合消息 $\mathbf{m}_v$ 结合。这种结合由另一个可学习的函数——[更新函数](@entry_id:275392) $\phi$——所控制。新状态由此诞生：$\mathbf{h}_v' = \phi(\mathbf{h}_v, \mathbf{m}_v)$。

这个三阶段过程——消息、聚合、更新——构成了一“层”[消息传递](@entry_id:751915)。通过堆叠这些层，我们让信息在图中向外[扩散](@entry_id:141445)，使每个节点能对其更广泛的环境建立起日益复杂的理解。

### 聚合的艺术：我们该如何倾听？

聚合器的选择并非无关紧要的细节；它是对你所建模网络性质的深刻宣言。让我们以两种真实的生物结构为例来说明原因[@problem_id:3317105]。

想象一个**[蛋白质复合物](@entry_id:269238)**，它是一个由协同工作的蛋白质组成的致密球体。这个[子图](@entry_id:273342)中的节点高度连接且功能相似——我们称之为**[同质性](@entry_id:636502)**。在这里，`mean` 聚合是一个绝佳的选择。每个蛋白质从许多其他相似的蛋白质接收消息。通过平均它们的状态，节点可以得到一个稳定、鲁棒的“群体意见”摘要，从而有效滤除微小变化或噪声。这就像在一屋子专家中进行民意调查以达成共识。

现在，想象一个**信号通路**，这是一个指令链，其中一个蛋白质激活下一个，下一个再作用于另一个。在这里，节点功能各异——激酶、[转录因子](@entry_id:137860)等等。这是一个**[异质性](@entry_id:275678)**环境。如果我们对来自“上游”激活物和“下游”靶标的消息进行 `mean` 聚合，我们会得到一团毫无意义的混乱，一个既非此也非彼的状态。这就像为了解一家公司而去平均CEO和工厂工人的职位描述。相反，`max` 聚合可以强大得多。它充当特征选择器，允许最显著的信号——在特定特征维度上“最响亮的声音”——传播开来。这模仿了生物信号（如激活标志）如何在传递链中不被稀释地传递下去。

为了让这些对话更加细致入微，我们可以给节点一种更复杂的更新状态的方式。我们可以使用受[循环神经网络](@entry_id:171248)启发的[门控机制](@entry_id:152433)，而不是简单地将新信息添加到旧信息中[@problem_id:65947]。想象一个**[更新门](@entry_id:636167)**，它学习应该整合多少新的聚合消息；以及一个**[重置门](@entry_id:636535)**，它学习应该忘记多少旧状态。这使得网络能够动态控制信息流，学会保留长程信息或对局部变化迅速作出反应，就像一个知道何时倾听、何时发言、何时改变主意的熟练辩手。

### 不断扩展的视野及其风险

每一层[消息传递](@entry_id:751915)都将节点的“[感受野](@entry_id:636171)”扩大一跳。一个有 $K$ 层的 GNN 允许一个节点从最远 $K$ 条边之外的任何其他节点接收信息。这给了我们一个优美的物理直觉：消息传递就像图上的一个[扩散过程](@entry_id:170696)。信息从每个节点散播开来，如同热量从一个[点源](@entry_id:196698)散发出去。

这个类比为我们设计[网络深度](@entry_id:635360)提供了一种强大而有原则的方法[@problem_id:3401688]。假设我们正在一个平均边长为 $h$ 的网格上模拟一个物理扩散过程（由 $\partial_t u = \kappa \Delta u$ 控制）。在时间 $\Delta t$ 内，[扩散](@entry_id:141445)的特征半径为 $r_{\text{diff}} = \sqrt{2d\kappa \Delta t}$，其中 $d$ 是空间维度。我们的 GNN 在 $K$ 层之后的[感受野](@entry_id:636171)大约是 $r_{\text{GNN}} = K h$。为了正确地捕捉物理过程，我们必须确保 GNN 能够“看到”物理所要求的那么远。这意味着我们必须选择一个深度 $K$，使得 $K h \ge \sqrt{2d\kappa \Delta t}$。因此，模型的架构直接受到其旨在模拟的物理过程的约束。

但这种扩展也伴随着危险。堆叠过多的层会导致两个臭名昭著的问题：

#### 回音室效应：过平滑

一场持续过久的对话会发生什么？最终，每个人的声音听起来都一样。这就是**过平滑**（oversmoothing）[@problem_id:2479703]。当我们使用像 `mean` 这样的聚合器时，我们实际上是在反复平均邻域特征。这个过程的每一步都像一个作用于节点信号的低通滤波器，平滑了相邻节点之间的差异。经过多层之后，这种局部平均导致图中一个连通分量内所有节点的特征收敛到相同的值。网络变成了一个单调、统一的回音室，其中每个节点都有相同的表示，所有区分它们的丰富局部信息都永久丢失了。与直觉相反，这个问题在高度连接的图中可能*更糟*，因为密集的连接加速了信息的混合和平均。

#### 瓶颈效应：过挤压

另一个更微妙的问题源于图本身的结构。想象一个图，它看起来像一棵巨大茂盛的树，通过单一的分支与世界其他部分相连[@problem_id:3126449]。来自树中大量叶子节点（一个指数级增长的群体）的信息，都必须通过那条唯一的瓶颈边传递到另一侧。这意味着来自成千上万个节点的信息必须被“挤压”进一个单一的、固定大小的消息向量中。这就是**过挤压**（over-squashing）。无论消息函数多么巧妙，这都是一个根本的[信息瓶颈](@entry_id:263638)。就像你无法用一句话总结整个美国国会图书馆一样，GNN 也无法指望通过一个微小的结构瓶颈忠实地传递来自一个大图区域的丰富信息。这个问题是具有“瓶颈”或高“[负曲率](@entry_id:159335)”的图的特征，这一结构特性可以通过图的拉普拉斯矩阵中一个小的[谱隙](@entry_id:144877)来识别。

### 盲点及其解决方法

除了深度 GNN 的动态问题外，它们的表达能力还有一个更根本的局限。一个标准的 MPNN，由于其局部和[置换](@entry_id:136432)不变的聚合方式，本质上是“短视的”。它区分两个不同图的能力，充其量等同于一种经典的图论算法，即**一维 Weisfeiler-Lehman（1-WL）测试**。

这意味着，如果 1-WL 测试无法区分两个图，那么标准的 MPNN 也无法区分。典型的例子是一个6节点环图与一个由两个独立的3节点三角形组成的图[@problem_id:3126471]。两个图都是“2-正则”的，即每个节点都恰好有两个邻居。从任何单个节点的局部视角来看，它所处的世界在两个图中是完全相同的：“我有两个邻居，它们各自还有一个邻居。”由于所有节点的初始[状态和](@entry_id:193625)局部邻域结构都相同，因此在两个图中，所有节点的[消息传递](@entry_id:751915)更新将永远相同。MPNN 将为两者生成相同的图级表示，因此无法察觉一个连通环路和两个独立环路之间的全局差异。这种局限性甚至可能使 MPNN 无法在某些[正则图](@entry_id:265877)中检测到像三角形这样的简单结构的存在[@problem_id:3189816]。

那么，我们如何赋予我们的网络更好的“视力”呢？

一个强大的想法是利用**跳跃知识（Jumping Knowledge, JK）网络**来对抗过平滑并拥抱多尺度信息[@problem_id:3189831]。JK 网络不仅仅使用最后一层的输出，而是“跳回”并连接*所有*中间层的表示。这使得最终的读出层可以访问从多个感受野（1跳视角、2跳视角等）观察到的节点状态。如果一个任务需要精细的局部信息（这些信息可能在深层中被冲淡），模型可以学会依赖于早期层的表示。如果需要全局视图，它可以使用后期层的表示。对于像识别特定距离 $d$ 处的节点这样的任务，JK 表示是理想的，因为它可以学会简单地从 $d$ 球的特征中减去 $(d-1)$ 球的特征，以分离出所需的那一圈节点。

为了突破 1-WL 的限制，我们必须让 GNN 能够超越简单的邻域。如果模型对三角形视而不见，为什么不教它明确地识别三角形呢？这就是**高阶 GNN**背后的思想[@problem_id:3189816]。一个具备基序（motif）感知的 GNN 可以学习聚合所有与中心节点共同构成三角形的节点，而不是聚合 1 跳邻居。通过将“邻域”的定义从一组相邻节点改变为一组形成特定结构基序的节点，我们从根本上增强了网络的[表达能力](@entry_id:149863)，使其能够感知 1-WL 测试所遗漏的那类拓扑特征。

[消息传递](@entry_id:751915)[神经网](@entry_id:276355)络的发展历程是科学进步的一个完美例证。我们从一个简单、直观的核心思想开始。通过应用它，我们发现了它的力量。然后，我们遇到了它的局限和病态问题。最后，我们巧妙地设计出新的原则来超越这些限制，从而构建出对我们周围的网络世界越来越强大、越来越有洞察力的模型。

