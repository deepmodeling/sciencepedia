## 引言
对人工智能的追求通常侧重于创建能提供正确答案的系统。然而，一个真正智能的系统不仅要准确，还必须了解其自身知识的局限性。像简易计算器一样以绝对确定性呈现预测的人工智能，在从[药物发现](@article_id:324955)到医疗诊断等高风险领域可能会产生危险的误导。一个没有[置信度](@article_id:361655)度量的答案是不完整的，可能导致我们丢弃有前景的解决方案或相信错误的结论。这种差距——传统深度学习中缺乏对不确定性的表达——对构建真正稳健可靠的系统构成了重大障碍。

本文介绍[贝叶斯深度学习](@article_id:638257)，它是一个应对这一挑战的强大框架。通过拥抱概率论，这种方法教会模型不再将预测表达为单点值，而是信念的分布，从而内在地量化其[置信度](@article_id:361655)。在接下来的章节中，我们将踏上理解这一[范式](@article_id:329204)的旅程。首先，在 **原理与机制** 部分，我们将剖析核心概念，探讨[偶然不确定性](@article_id:314423)（固有的）和认知不确定性（基于模型的）之间的数学区别，并揭示常见深度学习技术背后令人惊讶的贝叶斯根源。随后，在 **应用与跨学科联系** 部分，我们将见证这种[量化不确定性](@article_id:335761)的能力如何彻底改变科学发现，如何促成更安全、更合乎道德的人工智能的构建，并为理解世界提供一个全新的视角。

## 原理与机制

在我们构建智能机器的征程中，我们学到了一个至关重要的教训：一个真正智能的系统不仅给出答案，还会告诉你这个答案在多大程度上值得信赖。一个像袖珍计算器那样以不容置疑的确定性提供的简单数字，可能会产生误导，甚至是危险的。想象一个用于筛选新药的人工智能模型。如果它预测某个分子无效，我们可能会将其丢弃。但如果模型极不确定，基本上是在猜测呢？我们可能刚刚扔掉了一种治愈方法。

正是在这里，贝叶斯视角改变了我们对机器学习的看法。我们不再将模型看作产生单一“正确”输出的实体，而是认为模型拥有一系列*信念的分布*。对于任何给定的输入，它不仅提供一个答案，而是提供一系列可能的答案，每个答案都带有一定的概率。预测是可能性的全景，而其“不确定性”则是这片全景的广度。在设计治疗性病毒等安全关键型应用中，如果一个模型预测的相互作用得分很低且安全（$0.05$），但其相关的不确定性却高达$0.92$，那么这个模型是不可信的。这种高度不确定性是一个警示信号，大声宣告模型正处于未知领域，其预测并不可靠。唯一合理的策略是听从这个警告，在继续操作前进行物理实验以获得真实答案 [@problem_id:2018096]。这种[量化不确定性](@article_id:335761)并据此采取行动的能力不仅仅是一个功能，它正是负责任和稳健的人工智能的精髓所在。

### 不确定性的两面性

当我们说一个模型“不确定”时，我们实际上在谈论两种根本不同类型的“不知”。要构建真正智能的系统，我们必须学会区分它们。它们被称为**[偶然不确定性](@article_id:314423)（aleatoric）**和**[认知不确定性](@article_id:310285)（epistemic）**。

**[偶然不确定性](@article_id:314423)**源自拉丁词*alea*，意为“骰子”。它是掷骰子时的不确定性——我们所观察的系统固有的随机性。可以把它看作是世界中不可减少的噪音。即使拥有一个完美的硬币模型，你也无法确定下一次抛掷是正面还是反面。在测量细胞荧光的生物实验中，基因表达存在固有的随机性，测量设备本身也存在随机噪声，如[光子](@article_id:305617)散粒噪声。无论你收集多少数据，这种随机性在任何单一的未来测量中都依然存在 [@problem_id:2749107]。这就是[偶然不确定性](@article_id:314423)。它是世界的属性，而不是我们模型的属性。

另一方面，**[认知不确定性](@article_id:310285)**源自希腊词*episteme*，意为“知识”。这是由于我们自身知识的缺乏而产生的不确定性。这是模型在说：“我没有见过足够多这样的数据来做出确定的判断。”想象一下，你正在训练一个模型，根据蛋白质序列预测其功能。如果你的训练数据中没有包含带有长重复模式（同聚物序列）的序列示例，那么当你要求模型对这样的序列进行预测时，它会非常不确定。这就是认知不确定性。它的奇妙之处在于它是可以减少的。通过向模型展示更多数据，尤其是在它不确定的区域，我们可以减少它的无知，使其更有信心 [@problem_id:2749107] [@problem_id:3135744]。

### “知”与“不知”的数学

这种概念划分的美妙之处在于，它不仅仅是一个哲学上的区别，它有一个精确的数学公式。预测中的总不确定性可以被优雅地分解。如果我们将总预测方差 $\operatorname{Var}(y)$ 看作我们对结果 $y$ 的总不确定性，那么全方差定律给我们一个惊人地简单的方程：

$$
\operatorname{Var}(y \mid x, \mathcal{D}) = \underbrace{\mathbb{E}_{\theta \sim p(\theta \mid \mathcal{D})}\!\left[\operatorname{Var}(y \mid x, \theta)\right]}_{\text{偶然不确定性}} + \underbrace{\operatorname{Var}_{\theta \sim p(\theta \mid \mathcal{D})}\!\left(\mathbb{E}[y \mid x, \theta]\right)}_{\text{认知不确定性}}
$$

我们不要被这些符号吓倒。这个方程讲述了一个简单的故事 [@problem_id:2749107] [@problem_id:3180557]。右边的第一项是**偶然**部分。它是模型预期看到的噪声或固有方差的平均值。第二项是**认知**部分。它衡量的是，当我们考虑模型本身的不同可能版本（与数据 $\mathcal{D}$ 一致的不同参数设置 $\theta$）时，模型的*平均预测*变化了多少。它是模型对真实答案的信念的方差。

想象一下，我们有一个只见过很少数据（$n=50$ 个样本）的模型。它对一个新数据点的认知不确定性可能很高，比如 $0.9$，因为许多不同的函数都可以拟合这些稀疏的数据。过程的[固有噪声](@article_id:324909)（[偶然不确定性](@article_id:314423)）假设是 $0.4$。总预测方差将是二者之和：$0.9 + 0.4 = 1.3$。现在，假设我们收集了大量数据（$n=5000$）。模型现在受到了更多的约束。它的[认知不确定性](@article_id:310285)急剧下降，也许降到了 $0.1$。但世界的[固有噪声](@article_id:324909)没有改变；[偶然不确定性](@article_id:314423)仍然是 $0.4$。新的总方差是 $0.1 + 0.4 = 0.5$。我们已经驯服了我们的无知，但我们无法驯服宇宙的随机性 [@problem_id:3180557]。

这种分解不仅仅是一项学术练习。在[主动学习](@article_id:318217)中，我们希望选择能让模型学到最多的新实验。我们应该在高[偶然不确定性](@article_id:314423)区域采样还是高[认知不确定性](@article_id:310285)区域采样？答案很明确：我们应该探索高*认知*不确定性的区域，因为那是模型知道自己无知的地方，而新数据可以减少这种无知 [@problem_id:2749107]。

### 模型如何学会说“我不知道”

很长一段时间以来，深度学习的工具似乎只是一系列聪明但互不相干的“技巧”的集合。我们使用[权重衰减](@article_id:640230)、dropout 和提前终止等技术，因为它们有效，能防止我们的模型对训练数据过拟合。[贝叶斯框架](@article_id:348725)揭示了这些方法背后惊人而隐藏的统一性：从某种意义上说，它们都是对处理不确定性的原则性贝叶斯方法的近似 [@problem_id:2749038]。

当我们进行[贝叶斯推理](@article_id:344945)时，我们从对模型参数的**先验**信念 $p(\theta)$ 开始。这个先验概括了我们在看到任何数据之前的假设。例如，一个简单的信念可能是模型的权重应该很小，以偏好更简单的函数。当我们在数据 $\mathcal{D}$ 上训练模型时，我们使用贝叶斯规则来更新我们的信念，从而得到一个**后验**分布 $p(\theta \mid \mathcal{D})$。这个后验代表了我们更新后的知识。

事实证明，常见的[正则化技术](@article_id:325104)在数学上等同于假设了某种先验：

-   **L2 [正则化](@article_id:300216)（[权重衰减](@article_id:640230)）：** 加上一个与权重[平方和](@article_id:321453) $\lambda \sum \theta_i^2$ 成正比的惩罚项，等同于对权重施加一个独立的、零均值的**高斯先验**。[正则化](@article_id:300216)强度 $\lambda$ 与该高斯分布的方差成反比。这是一种形式化的说法：“我相信权重可能很小，并以零为中心。”

-   **L1 [正则化](@article_id:300216)（稀疏性）：** 加上一个与权重[绝对值](@article_id:308102)之和 $\lambda \sum |\theta_i|$ 成正比的惩罚项，等同于对权重施加一个**拉普拉斯先验**。这种先验在零点处有一个尖锐的峰值，这会鼓励许多权重变为精确的零，从而产生[稀疏模型](@article_id:353316)。

-   **[Dropout](@article_id:640908)：** 在训练期间随机将[神经元](@article_id:324093)激活设置为零这种看似奇怪的做法，具有深刻的[贝叶斯解释](@article_id:329349)。可以证明，它是在[贝叶斯神经网络](@article_id:300883)中进行推理的一种近似。每个 dropout 掩码对应于对一个不同的“[子网](@article_id:316689)络”进行采样。在测试时，对许多不同 dropout 掩码的预测进行平均（一种称为 **[MC Dropout](@article_id:639220)** 的技术）是[贝叶斯模型平均](@article_id:348194)的一种近似 [@problem_id:2749038] [@problem_id:2886031]。

-   **提前终止：** 即使是当验证误差开始增加时就停止训练这种简单的[启发式方法](@article_id:642196)，也可以被看作是一种[隐式正则化](@article_id:366750)，对应于一个高斯先验，其有效强度由停止时间控制 [@problem_id:2749038]。

这种统一是深刻的。它告诉我们，多年来，[深度学习](@article_id:302462)社区在不一定意识到的情况下，一直在发现强大的、近似的[贝叶斯推理](@article_id:344945)方法。

### 利用不确定性进行模型诊断的艺术

借助完全的[贝叶斯神经网络](@article_id:300883)（BNN），我们可以用标准网络无法实现的方式来诊断模型行为。通过检查预测的均值和方差，我们可以清楚地了解我们的模型是[欠拟合](@article_id:639200)还是[过拟合](@article_id:299541) [@problem_id:3135744]。

-   **过拟合：** 一个过拟合的 BNN 的行为就像一个学生，他记住了考试的答案，但没有学会概念。在训练数据上，它会极其准确且高度自信（低误差，低预测方差）。但是当面对新的、分布内的数据时，它的误差会跃升。而当面对分布外（OOD）的数据时，它会通过产生具有非常高方差的预测来表明它的困惑。这种高的 OOD 方差不是一个缺陷，而是一个特性！它表明模型正确地告诉我们它已超出其能力范围。

-   **[欠拟合](@article_id:639200)：** 一个[欠拟合](@article_id:639200)的 BNN 就像一个根本没学习的学生。它在任何地方都表现不佳——无论是在训练、验证还是 OOD 数据上。至关重要的是，它常常“自信地犯错”。因为它学到了一个简单（但错误）的规则，所以它在任何地方都以低预测方差应用这个规则。它不知道自己不知道什么。这里的补救方法是增加模型的容量（例如，使网络更大）或放宽正则化，使其能够学习更复杂的模式。

### 当现实比单一答案更复杂时

科学中最重要的教训之一是，我们的模型的好坏取决于它们的假设。一个 BNN 可以告诉我们它的认知不确定性，但它无法克服自身假设结构中的根本缺陷。

考虑[材料科学](@article_id:312640)中的一个场景，其中特定的化学配方可以产生两种不同的[晶体结构](@article_id:300816)（**[多晶型现象](@article_id:319879)**），每种结构都有不同的[带隙](@article_id:331619)。这个配方的真实结果分布是**双峰的**：有两个不同的、正确的答案。这是一种结构化的[偶然不确定性](@article_id:314423) [@problem_id:2479724] [@problem_id:3197060]。

如果我们用一个标准的、假设输出噪声为简单单峰高斯分布的 BNN 在这些数据上进行训练，会发生什么？它会彻底失败。模型学会了预测两个[带隙](@article_id:331619)的平均值——这个值在现实中可能永远不会出现！为了解释数据的广泛分布，它会夸大其预测方差。但这个方差只是一个混乱的混合体，将模型的错误设定与真实的不确定性混为一谈。它不是对双峰现实的有用表示。

解决方案不仅仅是增加模型的[认知不确定性](@article_id:310285)。问题在于模型假设的似然函数——它对世界噪声的模型。为了捕捉多峰的现实，我们需要一个多峰的[似然函数](@article_id:302368)，比如**混合密度网络（MDN）**，它可以明确地预测不同结果的混合。这是一个谦卑的提醒：即使是我们最复杂的用于处理“已知的未知”（认知不确定性）的模型，如果它们对“未知的未知”（[偶然不确定性](@article_id:314423)的结构）有错误的认识，也可能被打败。

### 作为[信息增益](@article_id:325719)的学习：更深层次的原理

我们可以通过信息论的视角，将我们的理解提升到一个更深的层次。“从数据中学习”意味着什么？从这个角度来看，学习是减少不确定性的过程 [@problem_id:3137998]。

让我们使用**熵**来量化我们对模型参数 $\theta$ 的不确定性，记为 $H(\theta)$。在看到任何数据之前，我们的不确定性由先验的熵 $H(\theta)$ 给出。在我们观察到数据 $D$ 之后，我们的不确定性由后验的熵 $H(\theta \mid D)$ 给出。由于数据提供了信息，我们的后验不确定性将小于我们的先验不确定性：$H(\theta \mid D)  H(\theta)$。

不确定性的减少量是参数和数据之间的**互信息**：
$$
I(\theta; D) = H(\theta) - H(\theta \mid D)
$$
这个量 $I(\theta; D)$，正是数据 $D$ 提供的关于参数 $\theta$ 的[信息量](@article_id:333051)（以比特或奈特为单位）。这将整个贝叶斯学习事业与 Claude Shannon 信息论的基本货币联系起来。

这个视角甚至为泛化提供了一个有原则的看法。一个“记住”训练数据的模型，是一个从中提取了大量信息的模型，导致 $I(\theta; D)$ 非常大。为了鼓励模型只学习可泛化的模式并忽略训练集的噪声细节，我们可能希望*限制*互信息。这就是**[信息瓶颈](@article_id:327345)**原则的核心思想，这是一个优美的理论，它将学习框架化为在压缩输入和保留关于输出的信息之间的权衡，为为什么更简单的模型通常泛化得更好提供了深刻的理论基础。

从实用的诊断到深刻的理论统一，贝叶斯视角为我们提供了一种更丰富、更诚实、最终也更强大的方式来思考和构建智能系统。它教会我们的模型谦卑地说出“我不知道”，这是通往真正智慧之路上关键的一步。

