## 引言
人工智能在改变临床医学方面展现出巨大前景，它提供的工具能以前所未有的速度和规模诊断疾病、预测患者需求。然而，在这份前景之下潜藏着一个关键危险：这些强大的算法在从我们这个不完美世界的数据中进行训练时，可能会继承甚至放大历史上的不平等。对机器客观性的信念是一个神话；临床模型可能系统性地对特定患者群体失效，将治愈的工具变为伤害的载体。本文通过全面概述临床人工智能中的偏见来应对这一关键挑战。首先，在“原则与机制”一章中，我们将剖析偏见的起源，从有缺陷的数据到有缺陷的人机交互，并探讨在定义公平性时所涉及的艰难选择。随后，“应用与跨学科联系”一章将展示这些理论概念如何在现实世界的临床实践中体现，揭示该问题与[系统工程](@entry_id:180583)、伦理学和法学等领域的深层联系。要构建真正公平的系统，我们必须首先理解机器中的幽灵。

## 原则与机制

要理解偏见如何渗透到临床算法的灵魂深处，我们必须首先领会一个基本事实：算法学习的不是现实，而是现实的*记录*。这份记录——我们称之为电子健康记录（EHR）的庞大数据宝库——并非对患者完美、清晰的反映。它是由一个复杂、混乱且充满人性的过程投下的阴影。它是由匆忙的临床医生书写、受医院经济状况塑造、并通过历史不平等的滤镜过滤的故事。我们在模型中发现的偏见，本质上是这个过程的幽灵，永远萦绕在机器之中。我们的任务是成为幽灵猎手，去理解这些幽灵是如何产生的，以及我们如何能将它们带到阳光下。

### 扭曲的镜子：偏见的源头

想象一下，你试图评估一个人的健康状况。你会测量什么？他们的生命力？他们免于痛苦的程度？这些是我们真正关心的东西。但算法无法测量“生命力”。它只能测量已被记录的东西。通常，为了寻求一个可测量的量，我们会选择一个**代理（proxy）**——一个真实事物的替代品。

这是一个极其重要且危险的步骤。考虑一个现实世界中的算法，它旨在识别“高需求”患者以提供额外的预防性护理。创建者需要一个“健康需求”的代理，并选择了“未来一年的总医疗保健成本”。乍一看，这似乎很合理；病情较重的人通常会使用更多的医疗资源。但现在，想一想两个患有完全相同基础疾病的人。一个富裕，拥有优质保险和便利的交通。另一个生活贫困，面临巨大的就医障碍。第一个人接受了检查，看了专科医生，产生了高额费用。第二个人，尽管病情同样严重，却无法经常去医院，产生的费用很低。

当算法基于这些数据进行训练时，它学到了一个扭曲的教训：它学会将特权患者的特征与“高需求”联系起来，而将服务不足患者的特征与“低需求”联系起来。结果是灾难性的失败。该模型系统性地为更健康、更富裕的患者标记出需要更多护理，却忽视了最需要护理的、病情更重、更贫穷的患者 [@problem_id:4519501]。代理背叛了我们。镜子是扭曲的，它反映的不是患者的健康，而是他们在支离破碎的体系中穿行的能力。这就是**代理偏见**，一个根本性的错误，即我们能测量的指标并非我们所关心结果的良好替代品。

问题甚至比代理更深。即使我们旨在记录一个直接的临床事实，比如脓毒症的诊断，标签本身也是一个人为产物。患者真实、潜在的临床状态，我们称之为 $Z$，与记录的标签 $Y$ 是有区别的。标签是一个工作流程的最终产品：$Y = f(Z, X, D, I, G)$，它不仅取决于真实状态 $Z$ 和临床预测因子 $X$，还取决于记录过程 $D$、机构激励环境 $I$ 和患者的群体身份 $G$ [@problem_id:4421580]。

一个与非英语患者存在沟通障碍的临床医生，可能会不够精确地记录症状。医院的计费部门，在报销激励的驱动下，可能偏好某些诊断代码而非其他。这些都不是恶意行为；它们是压力下系统的自然产物。但其后果是**标签偏见**：一种系统性失真，记录的标签 $Y$ 以一种与患者背景相关的方式偏离了真实的临床状态 $Z$。数据不再是简单的生物学记录；它是一份社会和经济文件。

而当整个群体完全被排除在对话之外时会发生什么？想象一个旨在通过部分分析临床医生自由文本笔记中丰富细微之处来检测脓毒症的人工智能。如果出于预算原因，该模型仅在英文笔记上进行训练，那么对于英语水平有限（LEP）的患者来说，它实际上就对他们的叙述视而不见 [@problem_id:5014160]。对于这个群体，一个至关重要的信息渠道被一个[零向量](@entry_id:156189)所取代——一种数字化的沉默。

在一个直接、可量化的**代表性偏见**例子中，这样一个模型的真阳性率（或灵敏度）对英语流利的患者为 $0.80$。也就是说，它正确识别了 $80\%$ 的脓毒症病例。然而，对于LEP患者，灵敏度骤降至 $0.55$。高达 $45\%$ 的LEP脓毒症患者被漏诊，这种差异直接源于他们被排除在训练数据之外。镜子不仅是扭曲的；对某些人来说，它完全是一片空白。

最后，偏见可以被编织进护理的节奏之中。在动态的医院环境中，数据不是静态的快照，而是测量的时间序列。但谁被测量，以及多久测量一次？观察过程本身可能存在偏见。一个患者，或许因为有更好的保险或更坚决的家人，可能每隔几小时就接受一次实验室检测。另一个社会地位较低的患者，可能一天只被检测一次 [@problem_id:5199789]。一个从这些数据中学习的模型，不仅学习患者的生理状况，还学习他们的“可测量性”。像“距离上次测量的时间”这样的特征成为强大的预测因子，它们与患者的病情严重程度和社会经济地位都纠缠在一起。这种**测量偏见**，即数据收集模式在不同群体间系统性地存在差异，是[算法偏见](@entry_id:637996)中最微妙但最普遍的形式之一。

### 环路中的人：一种有缺陷的共生关系

一个算法一旦部署，就不是一个孤立行动的神谕。它成为一个**社会技术系统**的一部分，一个人类与机器的伙伴关系。在这种伙伴关系中，偏见会以危险的方式相互作用和放大。

考虑一个用于脓毒症的决策支持系统，由于我们讨论过的数据偏见，它为来自特定社会群体 $G_2$ 的患者系统性地生成了较低的风险评分 [@problem_id:4849720]。这是**[算法偏见](@entry_id:637996)**。现在，临床医生登场了。假设这位临床医生，无论是有意还是无意，持有一种偏见，在为 $G_2$ 群体的患者启动治疗前要求更高的确定性。他们应用了更高的决策阈值。结果是错误的叠加。算法首先提供了一个较低的分数，使其更不可能越过任何门槛。然后临床医生将门槛提得更高。一个来自 $G_1$ 群体的患病患者有 $85\%$ 的机会得到及时治疗的系统，变成了一个来自 $G_2$ 群体、病情相同的患者只有 $60\%$ 机会的系统。这种不平等是人机交互的涌现属性，远比任何单个组件的偏见更糟糕。

这引出了一个有趣的悖论。环路中的人（human-in-the-loop）通常被提议为最终的安全网，但人类心智有其自身的偏见，尤其是在与自动化系统交互时。我们容易受到**自动化偏见**的影响，这是一种过度依赖自动化系统的倾向。这可能导致作为错误（遵循不正确的警报）和不作为错误（当系统沉默时未能采取行动）。

想象一个上市后监测团队试图监控一个脓毒症AI的性能 [@problem_id:4434700]。他们只有在订购了“金标准”确证性测试时才能测量性能。现在，考虑两种类型的临床医生。**过度依赖**的临床医生倾向于只在AI发出警报（$A=1$）时才订购测试。当AI保持沉默（$A=0$）时，他们感到放心，不订购测试。毁灭性的后果是，这位临床医生几乎永远不会发现模型的假阴性——那些AI错过（$A=0$）的真正患病患者（$Y=1$）。因为这些错误从未被验证，它们也从未被计算在内。监测团队看到一个具有极高但完全虚幻的灵敏度的模型。他们创造了一个**认知盲点**：模型失败空间中一个他们系统性地看不见的区域。

相反，**不信任**的临床医生，不信任AI，可能在AI发出警报时也不屑于订购确证性测试，认为它是错的。这就为[假阳性](@entry_id:635878)创造了一个盲点。在这两种情况下，人机交互都污染了我们评估AI安全性所需的数据，形成了一个错位信心的反馈循环。

### 定义“公平”：一个不可能的选择？

鉴于我们的模型存在偏见，我们希望让它们变得“公平”。但公平意味着什么？这个问题不仅仅是技术性的；它是伦理性的，并迫使我们面对令人不安的权衡。科学界已经形式化了几种不同的公平性定义，而一个模型很少能同时满足所有这些定义。

让我们用一个基于生物标志物的风险评分的具体场景来审视三个最常见的标准 [@problem_id:4999480]。

1.  **[机会均等](@entry_id:637428)（或均等真阳性率）：** 该标准提问：对于所有真正患有该疾病的患者，模型是否在所有群体中以相同的比率正确识别他们？在数学上，我们希望[真阳性率](@entry_id:637442)（TPR）相等：$\mathbb{P}(\hat{Y}=1 \mid Y=1, G=A) = \mathbb{P}(\hat{Y}=1 \mid Y=1, G=B)$。这个定义植根于避免伤害的原则；它指出，测试的好处（正确诊断）应该对所有人都是可及的，无论其群体身份如何。那个错过了 $45\%$ LEP患者脓毒症病例，但只错过了 $20\%$ 讲英语患者病例的模型，就违背了这一原则 [@problem_id:5014160]。

2.  **预测均等（或均等阳性预测值）：** 该标准提问：当模型发出阳性警报时，它对每个群体是否意味着相同的事情？预测是否同样可靠？在数学上，我们希望阳性预测值（PPV）相等：$\mathbb{P}(Y=1 \mid \hat{Y}=1, G=A) = \mathbb{P}(Y=1 \mid \hat{Y}=1, G=B)$。这关乎临床医生对警报的信任度。如果对A群体的警报有 $50\%$ 的可能性是正确的，但对B群体的警报只有 $44\%$，临床医生可能会开始轻视对B群体患者的警报 [@problem_id:4999480]。

3.  **[均等化赔率](@entry_id:637744)：** 这是一个更严格的标准，它将[机会均等](@entry_id:637428)与第二个条件相结合：假阳性率（FPR）在各群体间也必须相等。不仅患病患者应以相同的比率被识别，健康患者也应以相同的比率被错误标记。这可以保护群体免受过度检测和不必要干预的负担。

这里存在一个深刻而令人不安的数学真理。当一种疾病在不同群体中的基础患病率不同时，一个非平凡的分类器通常不可能同时满足所有这些公平性标准。必须做出决定。我们优先考虑哪种类型的公平？是测试对所有患病者同等有效（[机会均等](@entry_id:637428)）更重要，还是警报对所有人同等可信（预测均等）更重要？没有唯一的“正确”答案。这种选择是一种价值判断，取决于临床背景、不同类型错误的后果以及我们的社会价值观。

### 前进之路：从缓解到透明

偏见的挑战是巨大的，但并非不可克服。前进的道路包括向内看，从内部修复机器，以及向外看，建立问责和透明的体系。

机器学习的创新提供了执行某种算法手术的方法。一种引人入胜的方法涉及使用**概念激活向量（Concept Activation Vectors, CAVs）** [@problem_id:5182073]。从本质上讲，研究人员可以在神经网络“思维”的高维空间内，识别出对应于特定概念——甚至是像“肤色”这样的敏感概念——的“方向”。一旦这个方向 $v_{sensitive}$ 已知，我们就可以用一个附加的惩罚项来重新训练模型。这个惩罚项会在模型的预测沿着那个敏感方向发生变化时惩罚模型。我们实际上是在教导模型：“学会识别疾病，但我禁止你使用任何与肤色相关的信息来做到这一点。”通过对模型进行正则化，使其对不良概念的表征不敏感，我们可以从源头上减轻偏见。

然而，没有哪种缓解措施是完美的。这使我们达到了最终的伦理原则：**透明**。一个开发人工智能工具的实验室，根植于**正义**和**不伤害**的原则，负有深远的伦理责任，去披露其所知的关于工具局限性的信息 [@problem_id:4366370]。如果一个用于检测[癌症转移](@entry_id:154031)的人工智能对一个群体的灵敏度为 $0.94$，而对另一个群体仅为 $0.78$，这不应是需要保守的商业秘密。这是一个关键的患者安全问题。隐瞒这些信息会导致对服务不足群体的可预见、可避免的伤害。披露这些信息则能使临床医生能够实施缓解策略，例如对来自漏诊风险较高群体的病例进行二次人工审核。

这最终形成了一个责任蓝图：致力于严格、透明的文档记录 [@problem_id:4442186]。对于每一个临床人工智能模型，我们都必须要求一份“机器中幽灵的用户手册”。这份手册必须细致地详述：

- **人口统计构成：** 训练数据中包括谁？谁被遗漏了？这使我们能够判断该模型是否适用于我们自己的患者。
- **疾病患病率：** 疾病在数据中有多普遍？这对于在新的环境中解释模型的预测值至关重要。
- **标签协议：** “金标准”是什么？它是如何定义和测量的？这告诉我们模型训练目标的有效性。
- **裁决程序：** 当人类专家对标签意见不一时，冲突是如何解决的？这关乎金标准的可靠性。
- **已知偏见：** 最重要的是，诚实地说明模型已知的局限性、其潜在影响以及为解决这些问题所采取的步骤。

这不仅仅是官僚程序。它是我们从一个不透明、不负责任的黑箱世界，走向一个值得信赖、负责任、并最终公平的临床工具世界的根本机制。它是我们学会驾驭幽灵，而不是被它们所困扰的方式。

