## 引言
在追求更强大人工智能的过程中，我们常常创建出庞大且计算成本高昂的[神经网络](@article_id:305336)。这些过参数化的模型虽然准确率很高，但在部署到手机或[嵌入](@article_id:311541)式系统等资源受限的设备上时，会带来巨大挑战。这就引出了一个关键问题：这些庞大网络中的所有连接真的都是必需的吗？[网络剪枝](@article_id:640263)为此提供了一个令人信服的答案，它提供了一种系统性方法来识别并移除不重要的连接，从而在不显著损失性能的情况下，得到更小、更快、更高效的模型。本文将深入探讨[网络剪枝](@article_id:640263)的科学与艺术。在第一章“原理与机制”中，我们将探索其核心概念，从生物学中[突触修剪](@article_id:323337)的启示，到定义“重要性”并指导剪枝过程的数学形式化方法，其中也包括著名的彩票假设。随后，在“应用与跨学科联系”中，我们将拓宽视野，审视剪枝如何应用于像[Transformer](@article_id:334261)这样的现代架构，以及它如何与优化、工程学和探究学习本身的科学探索中的经典问题相联系。

## 原理与机制

想象一下新生儿的大脑。它是一个充满潜能的奇迹，但也充满了杂乱无章的连接。在发育的早期阶段，大脑会产生过量的突触——[神经元](@article_id:324093)之间的微小连接点。这就像一个电话公司把每家每户都相互连接起来。然后，一个非凡的过程开始了：根据经验，基于哪些连接被使用、哪些没有，大脑开始系统性地剪断这些线路。这并非衰退的迹象，而是一个精炼的过程，是将一个混乱的网络雕琢成一个高效、特化的电路。这个被称为**[突触修剪](@article_id:323337)**（synaptic pruning）的生物学过程，为我们提供了深刻的洞见。单个连接可以被定位并移除这一事实，告诉了我们关于大脑结构的根本性信息：[神经元](@article_id:324093)是离散的、独立的细胞，它们在特定的、可分离的点上进行通信。如果神经系统是一个连续的原生质网络，如此精确的编辑将是不可能的 [@problem_id:2353228]。

这种“过度生长，然后修剪”的自然效率蓝图，正是人工智能领域[网络剪枝](@article_id:640263)的灵感来源。我们从一个庞大、过参数化、连接密集的神经网络开始，试图找到并移除那些“不重要”的连接，从而留下一个更小、更快但能力相当的[子网](@article_id:316689)络。然而，这个简单的想法打开了一个充满迷人问题的潘多拉魔盒。一个连接“不重要”究竟意味着什么？我们如何找到这些可有可无的部分？为什么我们的网络一开始就充满了这些部分？以及，一旦我们创建了一个稀疏网络，它真的会更快吗？让我们从第一性原理出发，踏上回答这些问题的旅程。

### 遗忘的艺术：双目标的故事

从本质上讲，剪枝是一种平衡行为。我们面临两个相互竞争的愿望：我们想要一个**准确率**高的模型，这意味着它在我们的数据上的错误或损失要低；同时我们想要一个**高效**的模型，这意味着它的参数要少，计算量要小 [@problem_id:3154134]。这是一个经典的[多目标优化](@article_id:641712)问题。想象一个图表，其中x轴代表模型大小（参数数量），y轴代表错误率。每个可能的剪枝后网络都是这个图上的一个点。

我们感兴趣的是最佳的权衡取舍，即一组被称为**帕累托前沿**（Pareto front）的模型。对于这个前沿上的任何模型，你都无法在不增加其错误率的情况下使其变得更小，也无法在不使其变得更大的情况下降低其错误率。剪枝的目标就是在这个[帕累托前沿](@article_id:638419)上或其附近找到一个满足我们实际需求的模型。

我们如何在这个领域中导航呢？主要有两种策略。第一种是工程师的直接方法，即**$\varepsilon$-约[束方法](@article_id:640602)**（$\varepsilon$-constraint method）：“给我一个参数不超过（比如说）1000万的最准确的模型。”在这里，我们对模型大小（$\varepsilon$）设定一个严格的预算，并找到符合该预算的最佳模型。这种方法直观且实用。

第二种方法更偏向数学，即**加权和方法**（weighted-sum method）。它不设硬性预算，而是将我们的两个目标合并成一个单一函数进行最小化：

$$
\text{Total Cost} = \text{Loss}(w) + \lambda \cdot \text{Size}(w)
$$

在这里，$w$ 代表网络权重，$\text{Loss}(w)$ 是我们通常的准确性度量，而 $\text{Size}(w)$ 是对复杂度的惩罚。参数 $\lambda$ 是一个我们可以调节的旋钮：一个小的 $\lambda$ 会告诉优化器将准确性置于首位，而一个大的 $\lambda$ 则在呐喊“让模型更小，即使会牺牲一点准确性！”通过改变 $\lambda$，我们可以在[帕累托前沿](@article_id:638419)上描绘出不同的点。这种形式化为我们解决该问题提供了一个强大的数学工具。

### 重要性的数学原理

让我们采用加权和方法，并更具体地讨论。我们如何衡量一个网络的“大小”？最直接的方法是计算非零权重的数量。在数学中，这个数量用“$L_0$范数”（$\|w\|_0$）表示。我们的优化问题就变成了：

$$
\min_{w} \frac{1}{2}\|\Phi w - y\|_2^2 + \lambda \|w\|_0
$$

在这里，第一项是标准的最小二乘损失，衡量带权重$w$的模型拟合数据的程度；第二项是我们对存在非零权重的惩罚 [@problem_id:2405415]。这个方程完美地捕捉了我们的目标：找到在仍然能很好拟合数据的情况下，尽可能稀疏的模型。

不幸的是，大自然很少会如此轻易地揭示其秘密。这个看似简单的问题，要精确求解却极其困难。$\|w\|_0$项使得优化图景变成了一个由不相连的点构成的雷区。试图找到[全局最小值](@article_id:345300)是一个**NP难**（NP-hard）问题，相当于要搜索所有可能的保留或丢弃权重的组合——对于任何有实际意义大小的网络来说，这在计算上都是不可能完成的任务。

那么，我们该怎么办？我们作弊，但用一种非常聪明的方式。核心困难在于$\|w\|_0$范数的不连续性。优化中的标准技巧是用一个表现良好的近似函数来替代一个困难的函数。在这种情况下，主角是**$L_1$范数**（$\|w\|_1 = \sum_i |w_i|$），它就是权重的[绝对值](@article_id:308102)之和。我们的问题转化为了著名的LASSO问题，这是一个凸问题，并且在计算上是可行的。令人惊讶的是，对于许多问题，最小化$L_1$范数能得到与最小化$L_0$范数完全相同的[稀疏解](@article_id:366617) [@problem_id:2405415]！这是一个名为[压缩感知](@article_id:376711)（compressed sensing）的领域的基础，也是数学“超乎常理的有效性”的美妙例证之一。

虽然$L_1$正则化是在训练期间鼓励稀疏性的强大工具，但大部分剪枝研究关注一个更直接的问题：如果我们有一个训练好的网络，我们如何决定哪些权重最不重要？这里一个有力的概念是**显著性**（saliency）。一个权重的显著性回答了这个问题：“如果我删除这个权重，损失函数会增加多少？”利用[泰勒展开](@article_id:305482)可以证明，对于一个权重$w_j$，移除它所导致的损失增加量$\Delta L_j$近似为：

$$
\Delta L_j \approx \frac{1}{2} w_j^2 H_{jj}
$$

其中，$H_{jj}$是海森矩阵（Hessian matrix）中相应的对角[线元](@article_id:324062)素，它衡量了[损失函数](@article_id:638865)的曲率 [@problem_id:2405415]。这个方程极具洞察力。它告诉我们，一个权重的重要性取决于两件事：它的大小（$w_j^2$）和损失对该权重变化的敏感度（$H_{jj}$）。一个权重可能很大，但如果它位于损失图景的一个非常平坦的区域（$H_{jj}$很小），移除它造成的损害很小。相反，一个位于非常陡峭谷底的小权重（$H_{jj}$很大），可能至关重要。

计算[海森矩阵](@article_id:299588)的成本很高，因此在实践中，我们退而求助于更简单的[启发式方法](@article_id:642196)。最常见的是**幅度剪枝**（magnitude pruning）：我们假设所有的$H_{jj}$项大致相等，因此显著性仅与$w_j^2$成正比。我们只需剪掉[绝对值](@article_id:308102)最小的权重。另一个有趣的[启发式方法](@article_id:642196)是**移动剪枝**（movement pruning），我们剪掉在训练过程中变化最小的权重，其逻辑是静态的权重对学习过程贡献不大 [@problem_id:3152818]。寻找完美的、计算成本低的“重要性”度量方法，至今仍是一个活跃而激动人心的研究领域。

### [神经元](@article_id:324093)的秘密生活：为何网络如此可剪枝？

我们已经讨论了如何剪枝，但一个更深层的问题随之而来：剪枝为什么能行得通？为什么我们可以丢弃一个网络高达90%的权重，却仍能保持其性能？答案在于两个关键属性：冗余性和[神经元](@article_id:324093)本身的性质。

首先是**冗余性**。我们训练的那些庞大、过[参数化](@article_id:336283)的网络充满了冗余。你可以把一个网络看作是由许多“功能组”组成的，每个功能组负责检测某个特定特征。由于随机初始化和训练过程，网络可能会学到多种几乎相同的方式来检测同一个特征。它内置了备用方案。我们可以用一个简单的概率练习来模拟这一点 [@problem_id:3166593]。想象一个功能组有 $m$ 个冗余单元。如果我们以概率 $q$ 随机剪枝每个单元，那么只有当*所有* $m$ 个单元都被剪掉时，整个功能组才会失效，而这个事件发生的概率要小得多，为 $q^m$。冗余性提供的这种指数级保护，是大型网络对剪枝具有非凡韧性的一个主要原因。

其次，也是更深刻的一点，像**[修正线性单元](@article_id:641014)（ReLU）**这样的现代激活函数的本质，使得网络天生就适合剪枝。[ReLU函数](@article_id:336712)定义为 $f(z) = \max(0, z)$。这意味着如果一个[神经元](@article_id:324093)的输入 $z$ 是负数，它的输出就是零——该[神经元](@article_id:324093)处于“非激活”状态。在[反向传播](@article_id:302452)过程中，损失的梯度只能通过激活的[神经元](@article_id:324093)流动。如果一个[神经元](@article_id:324093)处于非激活状态，梯度路径就被切断了，连接到该[神经元](@article_id:324093)的所有权重都不会收到更新。

让我们回顾一下我们的显著性公式。一个更严谨的推导表明，一个权重的显著性与其所在[神经元](@article_id:324093)被激活的概率成正比 [@problem_id:3197666]。

$$
S_j \propto P(\text{neuron fires})
$$

如果一个[神经元](@article_id:324093)是“死的”——意味着它对训练数据中的所有输入都处于非激活状态——那么它被激活的概率就是零。因此，其所有输入权重的显著性也为零。这些权重被证明是完全不重要的！它们对网络的输出没有任何贡献，也接收不到任何学习信号。它们是完美的剪枝候选者。[ReLU网络](@article_id:641314)中激活的普遍稀疏性直接导致了大量低显著性权重的出现，这解释了现代网络为何能承受惊人程度的剪枝。

### 彩票假设：大海捞针

这就引出了现代深度学习中最优雅、最具影响力的思想之一：**彩票假设（LTH）**。该假设提出，一个大型、密集的随机初始化网络中，包含了一个小型的[子网](@article_id:316689)络——即“中奖彩票”——当这个子网络被单独训练时，其性能可以媲美完整的[密集网络](@article_id:638454)。[密集网络](@article_id:638454)与其说是在从头学习，不如说它是一个方便的集成体，用于发现和训练这个预先存在的幸运[子网](@article_id:316689)络。

在这种观点下，剪枝是*发现*中奖彩票的机制。完整的训练过程就像一个测试，它加强了构成彩票的连接，而让其他连接衰退。最后进行的幅度剪枝只是揭示了训练过程所选择出的子网络。

这听起来可能像魔术，但我们可以将其构建为一个简单的概率问题 [@problem_id:3166653]。假设我们的网络中有 $m$ 个潜在的“中奖彩票”，每个彩票都需要一组特定的 $r$ 个连接存在。如果我们以概率 $s$（对应于最终的网络密度）随机保留连接，那么保留一个特定彩票的概率是 $s^r$。那么，找到 $m$ 个不相交彩票中*至少一个*的概率是：

$$
\mathbb{P}(\text{found a ticket}) = 1 - (1 - s^r)^m
$$

这个简单的公式揭示了LTH的逻辑。在一个巨大的、过[参数化](@article_id:336283)的网络中，潜在彩票的数量 $m$ 是巨大的。即使存活概率 $s$ 很低（一个非常稀疏的网络），找到至少一张彩票的机会也可能相当高。该假设表明，我们不是创造了稀疏网络，而仅仅是发现了它。

LTH方案的一个关键部分是剪枝后该做什么：你需要将中奖彩票的权重**回溯**（rewind）到它们最初的初始化值，然后只重新训练那个子网络。为什么呢？因为那些使“失败彩票”可被剪枝的小幅度权重，不一定是好的训练起点。中奖彩票的关键特征是它的*结构*，以及其特定的*初始值*。这些初始值虽然很小，但却处在一个“甜蜜点”上，使得它们易于成功训练。一个简单的训练中权重增长模型表明，要使一个剪枝掩码保持稳定，权重的初始幅度必须足够小，以至于它们不会立即增长超过剪枝阈值 [@problem_id:3188075]。回溯确保我们从这个有利的初始状态开始。

### 从理论到现实：追求真正的速度

我们找到了中奖彩票，一个优美稀疏的子网络。我们完成了吗？不完全是。剪枝的最终目标不仅仅是在理论上得到一个参数更少的模型，而是要得到一个在真实硬件上运行得更快、更高效的模型。而在这里，我们遇到了一个关键的障碍。

想象一个权重矩阵。**非[结构化剪枝](@article_id:641749)**（Unstructured pruning）根据权重大小移除单个权重，这会产生一个看起来像瑞士奶酪的稀疏模式。虽然非零权重的数量减少了，但它们的位置是不规则的。像GPU和TPU这样的标准硬件是为密集的、规则的计算而设计的。它们就像庞大的军乐队，每个乐手都必须同时迈步。处理不规则的稀疏模式需要额外的索引、条件逻辑和分散的内存访问，这完全抵消了因计算量减少而带来的好处 [@problem_id:3188072]。

这就引出了**[结构化剪枝](@article_id:641749)**（structured pruning）这一关键概念。我们不再移除单个权重，而是移除整个、规则的权重组——例如，卷积层中的整个通道，或Transformer中的整个[注意力头](@article_id:641479)。最终的权重矩阵可能会有整行或整块的零，这是一种标准硬件可以轻易利用的结构。

我们可以用一个简单而强大的硬件模型来将其形式化 [@problem_id:3152881]。假设执行一次[稀疏矩阵](@article_id:298646)乘法的时间为：

$$
T_{\text{sparse}} = \beta \cdot s + \gamma \cdot B
$$

在这里，$s$ 是非零权重的数量（算术成本），$B$ 是硬件必须处理的结构块的数量（开销成本）。$\beta$ 和 $\gamma$ 分别是代表每次操作成本和每个块成本的常数。

**实际效率**（realized efficiency）$\rho$，它将实际[加速比](@article_id:641174)与仅由[稀疏性](@article_id:297245)带来的理论[加速比](@article_id:641174)进行比较，可以表示为：

$$
\rho = \frac{\text{Actual Speedup}}{\text{Theoretical Speedup}} = \frac{1}{1 + \frac{\gamma B}{\beta s}}
$$

这一个方程就说明了一切。
- 对于**非[结构化剪枝](@article_id:641749)**，每个非零权重都是其自己的“块”，所以 $B \approx s$。开销项 $\frac{\gamma s}{\beta s} = \frac{\gamma}{\beta}$ 很大且是常数，导致效率非常低。
- 对于**[结构化剪枝](@article_id:641749)**，我们可能移除一个矩阵的一半行。块的数量 $B$（行数）远小于非零权重的数量 $s$。开销项 $\frac{\gamma B}{\beta s}$ 变得非常小，实际效率 $\rho$ 接近1。

这是剪枝最终的、实践性的教训。要将美丽的稀疏性理论转化为现实世界的收益，我们必须尊重硬件所要求的结构。剪枝的艺术不仅仅是遗忘的艺术，更是以一种结构化的、有序的方式进行遗忘的艺术。从生物学的洞见到数学理论，最终到硬件感知的[算法](@article_id:331821)，这段旅程揭示了科学发现核心中原理与实践的美妙统一。

