## 引言
在现代医学中，海量的数据——从基因组序列到实时生命体征——掌握着更早期诊断和更有效治疗的关键。算法决策有望释放这一潜力，提供能够发现超越人类感知模式的工具。然而，将这些强大的“黑箱”整合到生死攸关的决策中，引发了关于安全性、公平性和问责制的深刻问题。本文旨在通过对这一领域进行全面概述来填补这一关键知识空白。首先，我们将剖析核心的“原则与机制”，探讨这些算法如何工作、如何被监管，以及它们可能固化的伦理偏见。随后，我们将转向其“应用与跨学科联系”，展示它们如何改变诊断和治疗，同时强调其负责任部署所必需的法律和社会框架。

## 原则与机制

请想象一下，你是一名医生。你的日常工作充满了数据的漩涡：化验结果、生命体征、病史、影像扫描。在这信息雪崩中，埋藏着微妙的模式和微弱的信号，它们可能意味着生与死的区别。现在，如果你有一位才华横溢的助手呢？一个拥有完美记忆力，能在几秒钟内筛选数百万个案例以发现你可能错过的模式的助手。这就是算法决策在医疗保健领域的承诺。但就像任何强大的新工具一样，在我们能将生命托付给它之前，我们必须了解其内部运作——它的原理和陷阱。

### 医生的新助手：从信息到行动

医疗保健算法的核心是一种模式发现器。它从海量的过往医疗数据中学习，以便对当前患者做出预测。我们将这些系统称为**临床决策支持（CDS）**，这是一个刻意谦逊的名称。其目标不是创造一个“机器人医生”，而是建立一个“提供针对特定患者的评估或建议以辅助临床决策的信息系统”[@problem_id:4861499]。

可以把它想象成针对人体的[天气预报](@entry_id:270166)。[气象学](@entry_id:264031)家的模型可能会分析大气数据来预测雷暴的概率为 $90\%$。模型本身并不会*制造*雷暴，但它提供了一个概率，帮助你决定是否带伞。同样，一个CDS可能会分析糖尿病患者的连续血糖数据和用药史，计算出未来24小时内发生严重低血糖的风险评分，比如 $r=0.85$。这并不能取代医生的判断，但它提供了一个关键的证据，帮助他们决定如何调整患者的胰岛素剂量 [@problem_id:4861499]。

然而，并非所有助手的行为方式都相同。最深刻的区别在于它们如何与人类专家互动。

首先是**辅助性**系统。这就像一个乐于助人的同事，他会探过你的肩膀说：“你可能需要仔细看看这位患者的钾水平，原因如下。”它提供建议、概率和理由，但临床医生完全可以自由考虑或忽略这些建议。控制的中心仍然牢牢掌握在人手中。

然后是**指令性**系统。这是一个更为果断的合作伙伴。它可能会直接干预工作流程，说：“低血糖风险极高。我正在自动暂停胰岛素输注。要重新启动，你必须提供书面理由。”这通常被称为“硬性中止”[@problem_id:4861499]。其利弊权衡显而易见。一个指令性系统可以作为一个强大的安全网，防止常见错误。但它也引入了我们所说的**算法权威**。一个忙碌的医生可能会犹豫是否要推翻看似客观的机器，即使他们自己的直觉告诉他们情况并非如此。

这种区别不仅仅是学术上的；它具有深刻的法律和伦理后果，其影响超出了医院病房。考虑一个由健康保险公司用来审查索赔的算法。一个仅仅将复杂索赔标记出来供人类专家审查的系统是辅助性的（模式1）。但一个在计算出“缺乏医疗必要性”的高概率后，不经任何人工审查就自动发出拒绝通知的系统则是指令性的（模式2）。这种自动拒绝是一个具有“法律效力”的决定，直接影响一个人获得护理的权利，并触发了像欧洲GDPR等法规下的一系列法律保障措施[@problem_id:4512240]。一个有用的建议和一个自动化的判断之间的界限是至关重要的，跨越这条界限会改变一切。

### 这东西安全吗？监管作为医疗设备的软件

当一个工具能够影响生死攸关的决策时，我们要求它必须是安全和有效的。但你如何监管一段代码呢？答案越来越倾向于将其视同于一个实体医疗设备。

这就引出了**作为医疗设备的软件（SaMD）**的概念。其正式定义是，旨在用于医疗目的，并且其功能不依赖于硬件医疗设备的软件[@problem_id:5014163]。一个读取头部[CT扫描](@entry_id:747639)以对潜在中风受害者进行分诊的人工智能就是一个完美的例子。它不是[CT扫描](@entry_id:747639)仪本身，而是一个解释图像的独立程序。

像美国食品药品监督管理局（FDA）这样的监管机构已经开发了一个基于风险的框架。最重要的不是算法的复杂性，而是它一旦失灵可能造成的潜在伤害。

*   **I类（低风险）**设备可能包括一个帮助你记录用药时间表的应用程序。通常，一般控制就足够了。
*   **III类（高风险）**设备是那些维持生命的设备，比如控制人工胰腺的软件。它们需要最严格的上市前批准形式。
*   **II类（中等风险）**是当今许多最有趣的人工智能所处的类别。考虑一下用于头部[CT扫描](@entry_id:747639)的人工智能分诊工具。疑似脑出血是一种危及生命的状况，那为什么这不是一个高风险设备呢？答案在于一个简单但强大的安全原则：**人在回路中**（human-in-the-loop）[@problem_id:5014163]。人工智能不做出最终诊断。它唯一的行动是重新排列放射科医生的工作清单，将最可疑的扫描移到最前面。如果人工智能错了（[假阳性](@entry_id:635878)），它只会造成轻微的效率低下。如果人工智能漏掉一个病例（假阴性），放射科医生仍然会按正常顺序阅读该扫描。人类专家作为一个关键的后盾，减轻了风险，使得该设备可以被归类为中等风险，需要特殊控制，如性能测试和人因验证。

### 机器中的幽灵：揭示偏见与不公

我们拥有一个经过批准、受到监管且有人在回路中的工具。还能出什么问题呢？医疗AI最大的危险不是它会变得具有恶意智能，而是它会以最好的意图和令人不寒而栗的效率，延续甚至放大我们自己的社会偏见。

要理解这一点，我们必须首先扩展我们的伦理词汇。生物伦理学的基本原则——自主、行善、不伤害和公正——仍然是我们的指南，但它们在数字时代被赋予了新的含义。一个关键的新概念是**信息伤害**（informational harm）[@problem_id:4837991]。

*   **自主**（Autonomy）变成了**信息自决**（informational self-determination）：你有权控制谁使用你的数据以及用于何种目的。
*   **不伤害**（Nonmaleficence）（“do no harm”）扩展到包括避免信息伤害，如算法歧视或导致污名化的隐私泄露。
*   **行善**（Beneficence）（“do good”）要求证明这些系统提供的实际利益超过其信息风险。
*   **公正**（Justice）从分配稀缺资源转向要求数据、算法及其结果的公平性。

这种不公正并非源于单一来源。它是一系列潜在失败的连锁反应，是在算法生命周期的每个阶段都可能悄悄潜入的一系列“偏见”[@problem_id:4824163]。

*   **选择偏见（Selection Bias）：** 训练数据并非世界的完美镜像。如果一家医院的数据主要来自拥有良好保险且容易获得护理的患者，那么基于此数据训练的算法自然会为该人群进行优化，可能会对那些处境较差的人群失效。
*   **测量偏见（Measurement Bias）：** 我们测量世界的工具可能存在缺陷，有时对某些人的缺陷比其他人更严重。一个著名的现实世界例子是[脉搏血氧仪](@entry_id:202030)，它在肤色较深的人身上可能不太准确。一个基于这种有缺陷数据训练的人工智能会将这种偏见当作事实来学习。
*   **混杂偏见（Confounding Bias）：** 算法将相关性误认为因果关系。它可能了解到来自某个邮政编码的患者健康状况更差。但邮政编码并不导致疾病；它是贫困、[环境污染](@entry_id:197929)和缺乏健康食品等未测量因素的代理变量。在不解决真正原因的情况下对代理变量采取行动，是走向歧视的典型路径。
*   **[算法偏见](@entry_id:637996)（Algorithmic Bias）：** 有时，偏见来自开发者的选择。如果一个算法被要求最大化整体准确率，它可能会通过对多数群体极其准确而对少数群体表现不佳来实现这一目标。整体分数看起来不错，但这是以牺牲公平为代价的。
*   **自动化偏见（Automation Bias）：** 最后的偏见在于我们自身。我们有一种认知倾向，即过度信任计算机生成的信息。医生在面对来自人工智能看似自信的建议时，可能会被微妙地推动，从而忽略自己来之不易的临床直觉。

当这些偏见根深蒂固时，它们就会演变成现实世界的不公正。其中最关键的一个是**差异性影响**（disparate impact），这是一个法律概念，即一种“表面中立”的做法——比如对每个人使用相同的算法——不成比例地伤害了一个受法律保护的群体。这可能产生法律责任，即使没有歧视的*意图*[@problem_id:4494811]。想象一下，一项审计显示，某个人工智能诊断工具对一个群体的假阴性率为 $0.06$，而对另一个群体则为 $0.12$。这意味着第二个群体中真正患病的患者被人工智能漏诊的频率是第一个群体的*两倍*。这不是一个统计上的奇特现象；这是一个具有深远法律和伦理后果的实质性伤害[@problem_id:4494853]。

更深层次地看，这种统计上的差异可能导致一种深刻的人类伤害，即**认知不公**（epistemic injustice）[@problem_id:4888862]。这是针对某人作为认知者（knower）身份的不公正。
*   **证言不公（Testimonial Injustice）：** 一位来自代表性不足群体的患者说：“我了解我的身体，我感觉非常不舒服。”但算法由于其训练数据未能恰当代表她的群体，给出了一个低风险评分。临床医生在“客观”机器的影响下，可能会无意识地贬低患者自身证言的可信度。
*   **诠释不公（Hermeneutical Injustice）：** 算法本身缺乏理解患者病情的概念，因为她的群体体验和表达症状的方式在训练数据中没有得到很好的体现。这个本应在患者和医生之间建立共同理解的工具，对她来说却是坏的。

### 打开黑箱：问责与[程序正义](@entry_id:180524)

情况可能看起来黯淡，但并非没有希望。通往可信赖人工智能的道路不是追求一个神话般的完美算法，而是建立稳健、透明和公正的*系统*。

一个常见的呼声是“[可解释性](@entry_id:637759)人工智能”——即能够窥视“黑箱”内部并理解其内在逻辑。但这真的是我们所需要的吗？当你使用手机时，你不需要理解其晶体管的量子力学来信任它。你需要知道它的设计目的、典型的[故障率](@entry_id:264373)，以及如果它坏了你有追索权。这里同样适用。需求不是**可解释性**（interpretability）（它内部如何工作），而是**有意义的透明度**（meaningful transparency）（它做什么、它的性能以及它的作用）[@problem_id:4442216]。这意味着要披露其目的、其对不同人群的准确性、其常见的失败模式，以及最终负责人是人类这一事实。

对这些复杂问题最有效的解药是**[程序正义](@entry_id:180524)**（procedural justice）。公平不仅关乎最终结果，也关乎达成结果的过程的公平性。一个在医疗保健领域真正公正的人工智能系统建立在四个支柱之上[@problem_id:4417396]：

1.  **透明度（Transparency）：** 主动发布一份“模型卡”，详细说明算法的目的、训练数据、在不同人口群体中的表现及其局限性。
2.  **参与（Participation）：** 创建监督委员会，不仅包括数据科学家和高管，还包括一线临床医生、伦理学家，以及——最重要的是——患者和社区代表。
3.  **可争议性（Contestability）：** 建立清晰、独立且易于访问的渠道，让患者能够对他们认为错误的决定提出申诉，并有权推翻该决定。
4.  **问责制（Accountability）：** 设计一个清晰的责任链，以便当出现问题时，我们知道谁应负责。

这引出了我们最后的、统一的原则：**分布式问责**（distributed accountability）[@problem_id:4861499]。在这个新世界里，责任是一项团队运动。构建算法的供应商对其安全性、有效性和技术完整性负责。医院或医疗系统负责管理其使用、监控其性能并确保其公平实施。而床边的临床医生则保留着对面前单个患者的护理的最终、不可推卸的责任。算法是一个强大的工具，但它终究只是一个工具——一个掌握在有爱心、有责任感的人类手中的工具。

