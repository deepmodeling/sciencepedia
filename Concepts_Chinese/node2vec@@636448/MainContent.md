## 引言
复杂网络是无数系统的支柱，从社会交往到细胞内错综复杂的蛋白质网络。然而，它们离散的、基于图的结构对于传统的机器学习算法来说本质上是难以处理的。这就产生了一个根本性的鸿沟：我们如何将网络连接中蕴含的丰富关系信息，转化为计算机能够理解和学习的格式？答案在于为每个节点创建捕捉其在网络中角色和上下文的低维[数值表示](@entry_id:138287)，即“嵌入”。

本文深入探讨了 `node2vec`，一个为此目的而设计的、具有开创性且高度灵活的算法。通过巧妙地借鉴自然语言处理中的概念，`node2vec` 提供了一个强大的框架来学习高质量的节点嵌入。本文将引导您了解该算法的设计和应用。在第一章“原理与机制”中，我们将剖析其核心组成部分，从探索网络的巧妙的有偏[随机游走](@entry_id:142620)，到生成最终表示的学习模型。随后，在“应用与跨学科联系”中，我们将看到这些嵌入在实际中的应用，探索它们如何在生物学等领域解锁新发现，支持从识别[蛋白质功能](@entry_id:172023)到为新疾病重新定位药物等任务。

## 原理与机制

要真正理解 `node2vec` 的强大之处，我们必须踏上一段旅程，从一个借鉴自人类语言世界、简单而深刻的想法开始，最终到达一个能够解码[复杂网络](@entry_id:261695)隐藏逻辑的复杂算法。我们的探索将不是对公式的枯燥背诵，而是为了建立直觉，去发现教会机器理解关系的方法中所蕴含的美感与统一性。

### 宏大的类比：从词语到网络

在 20 世纪 50 年代，语言学家 J.R. Firth 有一句名言：“观其伴，知其义 (You shall know a word by the company it keeps)。”这短短一句话引发了一场为计算机表示语言方式的革命。我们不再试图通过字典条目来定义一个词，而是通过其*上下文*——即通常出现在它周围的词语——来定义它。例如，“国王”一词经常出现在“王后”、“皇家”和“王位”附近。“程序员”则与“代码”、“计算机”和“软件”共现。这个思想是现代自然语言处理及 `word2vec` 等模型的核心。

现在，让我们进行一次想象的飞跃。如果我们能将同样的原则应用于网络中的节点呢？想象一个细胞内巨大的蛋白质相互作用网络。单个蛋白质“意味着”什么？它的功能、它的角色，并非一个孤立的属性，而是由与它相互作用的其他蛋白质所定义的。因此，我们可以为网络科学重述 Firth 的公理：**观其邻，知其点 (You shall know a node by the neighbors it keeps)。**

这便是 `node2vec` 的核心哲学。其目标是为网络中的每个节点学习一个[数值表示](@entry_id:138287)——一个向量，或称**嵌入**。这些嵌入不仅仅是任意的标签；它们的设计使得拥有相似网络邻域的节点拥有相似的嵌入向量。但我们如何定义“邻域”？又如何捕捉这种“相似性”的概念？第一步是学习如何“阅读”网络，将其静态的连接图谱转化为动态的“句子”。这正是[随机游走](@entry_id:142620)的作用所在。

### 游走的艺术：探索网络景观

想象一下，你是一名游客，被空降到一个由网络[图表示](@entry_id:273102)的新城市的中心。为了解城市的布局，你开始行走。你从一个十字路口（一个节点）移动到下一个，沿着街道（边）前行。你的一系列步伐，如 `节点 A -> 节点 D -> 节点 F -> ...`，构成了一个描述城市部分结构的“句子”。这正是**[随机游走](@entry_id:142620)**在图上所做的事情。通过从不同节点开始生成大量此类游走，我们创建了一个描述[网络拓扑](@entry_id:141407)的大型文本——一个语料库。

这个过程会自然地适应我们正在研究的特定网络类型。例如，在一个基因调控网络中，一个[转录因子](@entry_id:137860)控制一个基因，这种连接是有向的。[随机游走](@entry_id:142620)会尊重这一点，只沿着调控的方向移动。在一个无向的蛋白质-蛋白质相互作用网络中，移动是对称的。如果相互作用的强度不同，由**边权重**表示，游走可以被偏置，以更频繁地遍历更强的连接，将它们视为更宽、更重要的通道 [@problem_id:3331352]。

然而，一次简单的、漫无目的的游走是不够的。如果我们想捕捉节点的细微角色，我们需要在探索中更具策略性。这就引出了 `node2vec` 的核心创新：**有偏[随机游走](@entry_id:142620)**。

再想想探索一座城市。你可以采取两种主要策略：
1.  **[广度优先搜索 (BFS)](@entry_id:272706):** 你可以细致地探索与当前路口直接相连的每条街道，然后再向更远处移动。这种策略能让你对局部邻域有一个完整的了解。在生物网络中，这类似于识别同一功能模块或复合物中紧密联系的蛋白质群落。这种局部社区相似性的概念被称为**[同质性](@entry_id:636502) (homophily)**。
2.  **[深度优先搜索](@entry_id:270983) (DFS):** 或者，你可以选择一个方向一直走下去，尽可能远地穿过城市，创建一条长长的探索路径。这种策略更少关注直接的周边环境，而更多地在于发现城市的整体结构以及不同社区是如何连接的。在网络中，这有助于识别具有相似结构角色的节点——例如，两个都充当不同模块之间桥梁的蛋白质。这被称为**结构对等性 (structural equivalence)** [@problem_id:3331433] [@problem_id:3331399]。

没有哪种策略是普遍“更好”的；它们捕捉不同类型的信息。`node2vec` 的高明之处在于它不强迫我们做出选择。相反，它提供了两个“控制旋钮”，即参数 $p$ 和 $q$，以便在这两种探索策略之间流畅地切换。

其机制非常简单。这是一种**二阶游走**，意味着它的下一步不仅取决于它现在的位置，还取决于它刚从哪里来。假设一次游走刚刚从节点 $t$ 经过一条边到达了节点 $v$。现在，站在 $v$ 处，它必须决定接下来访问哪个邻居 $x$。这个选择是基于前一个节点 $t$ 和潜在的下一个节点 $x$ 之间的[最短路径距离](@entry_id:754797) $d(t,x)$ 来进行偏置的 [@problem_id:3331348]。

只有三种可能性：
-   **$d(t,x) = 0$:** 这意味着 $x=t$。游走正在考虑返回它刚刚离开的节点。**返回参数** $p$ 控制着这一点。高值的 $p$ 会使返回的可能性降低，鼓励游走继续前进。
-   **$d(t,x) = 1$:** 节点 $x$ 也是 $t$ 的直接邻居。游走停留在 $t$ 的局部邻域内。这被认为是“中性”移动。
-   **$d(t,x) = 2$:** 节点 $x$ 是 $v$ 的邻居，但*不是* $t$ 的邻居。游走正在向外移动，远离其先前的位置。**进出参数** $q$ 控制着这一点。

从 $v$ 移动到 $x$ 的未归一化概率与边权重 $w_{vx}$ 乘以一个偏置 $\alpha_{pq}(t,x)$ 成正比：
$$
\alpha_{pq}(t, x) =
\begin{cases}
    \frac{1}{p}  \text{ if } d(t,x) = 0 \\
    1  \text{ if } d(t,x) = 1 \\
    \frac{1}{q}  \text{ if } d(t,x) = 2
\end{cases}
$$

让我们在一个微型网络上看看它的实际效果：一个由三个蛋白质 $a$、$b$ 和 $c$ 组成的三角形，它们彼此相连 [@problem_id:3331426]。假设我们的游走刚刚从 $a$ 移动到 $b$。我们现在在 $b$ 处。我们下一步的选择是返回 $a$ 还是前进到 $c$。让我们将旋钮设置为 ($p,q$)=(2, 0.5)。
-   对于移回 $a$：$d(a,a)=0$，所以偏置是 $\frac{1}{p} = \frac{1}{2}$。
-   对于移向 $c$：$c$ 是 $a$ 的邻居，所以 $d(a,c)=1$。偏置就是 $1$。
归一化后，返回 $a$ 的概率是 $\frac{1/2}{1/2 + 1} = \frac{1}{3}$，而移动到 $c$ 的概率是 $\frac{1}{1/2 + 1} = \frac{2}{3}$。游走继续前进的可能性是回溯的两倍。

通过调整 $p$ 和 $q$，我们可以优雅地控制我们探索的性质 [@problem_id:3331357]：
-   **低 $p$，高 $q$ (类 BFS):** 高值的 $q$ (例如，$q > 1$) 会使偏置 $\frac{1}{q}$ 变小，不鼓励游走向外移动 ($d(t,x)=2$)。因此，游走被限制在局部邻域内，执行类似 BFS 的探索，捕捉**[同质性](@entry_id:636502)**。
-   **高 $p$，低 $q$ (类 DFS):** 低值的 $q$ (例如，$q  1$) 会使偏置 $\frac{1}{q}$ 变大，鼓励游走探索远处的节点 ($d(t,x)=2$)。高 $p$ 则不鼓励立即回溯。这导致了类似 DFS 的探索，非常适合捕捉**结构对等性**。

### 学习网络语法：Skip-Gram 与[负采样](@entry_id:634675)

我们现在有了我们的“句子”——由我们巧妙的有偏游走生成的序列。下一步是将它们输入一个学习机器，由它来推断每个节点的“含义”。`node2vec` 直接借鉴了 `word2vec` 的学习架构，使用了**带[负采样](@entry_id:634675)的 Skip-gram 模型 (SGNS)**。

这个想法很简单。我们在每个游走序列上滑动一个特定大小（比如 $k$）的窗口。对于序列中的每个节点，我们将其视为“中心”节点，并将其窗口内的其他节点视为其“上下文”。然后，Skip-gram 模型被训练来完成一个简单的任务：给定一个中心节点，预测它的上下文节点。

这如何产生好的嵌入呢？该模型将两个节点出现在同一上下文中的概率定义为与它们嵌入向量的**[点积](@entry_id:149019)**相关。如果两个向量在高维空间中指向相似的方向，它们的[点积](@entry_id:149019)就高；如果它们正交，[点积](@entry_id:149019)就为零。因此，训练过程就变成了一个调整[节点向量](@entry_id:176218)的游戏，使得频繁共现的节点的[点积](@entry_id:149019)最大化。

但这有一个陷阱。如果我们只向模型展示共现节点的“正”例，它可能会学到一个平庸的解决方案：让所有向量都相同！为了防止这种情况，我们使用**[负采样](@entry_id:634675)**。对于我们游走中的每一个“真实”的中心-上下文对 ($u,v$)，我们还生成几个“假”的对 ($u, v_i$)，其中 $v_i$ 是从整个网络中随机选择的节点。模型的优化目标于是变得双重：
1.  增加真实对 ($u,v$) 的相似度（[点积](@entry_id:149019)）。
2.  减少虚假的[负采样](@entry_id:634675)对 ($u, v_i$) 的相似度（[点积](@entry_id:149019)）。

在数学上，这被构建为一个逻辑回归问题。目标是最大化我们训练数据中所有真实对 $\mathcal{D}$ 的以下[对数似然函数](@entry_id:168593) [@problem_id:3331347]：
$$
\sum_{(u,v)\in \mathcal{D}} \left( \log \sigma(\mathbf{z}_u^\top \mathbf{z}'_v) + \sum_{i=1}^{K} \mathbb{E}_{v_i \sim P_n} \left[\log \sigma(-\mathbf{z}_u^\top \mathbf{z}'_{v_i})\right] \right)
$$
在这里，$\mathbf{z}_u$ 是中心节点 $u$ 的嵌入，$\mathbf{z}'_v$ 是上下文节点 $v$ 的嵌入，$\sigma(x)$ 是将值压缩在 $0$ 和 $1$ 之间的 sigmoid 函数，$K$ 是[负采样](@entry_id:634675)的数量，$P_n$ 是从中抽取负样本的噪声[分布](@entry_id:182848)。这个优雅的公式只是将我们的游戏形式化：让真实对的 sigmoid 输出接近 $1$，让虚假对的输出接近 $0$。

**窗口大小** $k$ 也扮演着至关重要的角色。它定义了“局部上下文”的含义。一个小的 $k$ 意味着学习将集中于游走中的直接邻居，捕捉细粒度的局部结构。一个更大的 $k$ 则允许模型看到沿游走路径相距较远的节点之间的联系，从而将更多的中尺度和全局信息融入嵌入中 [@problem_id:3331363]。这个参数与 $p$ 和 $q$ 协同作用，共同定义了所学表示的最终特性。

### 更深层的联系：机器学到了什么

我们很自然会想，这个过程背后是否存在更深层次的结构。事实证明，确实存在。SGNS 的目标函数可以被证明是隐式地在执行一种**矩阵分解**。对于像 `DeepWalk` 中那样的简单[随机游走](@entry_id:142620)，该算法实际上是在分解一个与图的转移概率相关的矩阵。

对于 `node2vec` 来说，情况更为复杂，也更为优美。因为游走是二阶的，这个过程不能用一个简单的点对点转移矩阵来描述。相反，它等价于在一个更大的“提升图”上进行一阶游走，其中状态不是节点，而是*有向边* ($t,v$)。由 $p$ 和 $q$ 引入的复杂偏置意味着 `node2vec` 正在分解一个比简单方法所能表示的更丰富的[网络结构](@entry_id:265673)表示 [@problem_id:3331395]。

正是这种灵活性使 `node2vec` 从其他嵌入技术（如 **Laplacian Eigenmaps**）中脱颖而出。像 Laplacian Eigenmaps 这样的[谱方法](@entry_id:141737)旨在最小化直接连接节点的嵌入之间的距离。其目标函数 $\sum w_{uv} \|\mathbf{z}_u - \mathbf{z}_v\|^2$ 是[同质性](@entry_id:636502)的直接数学表达。它们在发现[社区结构](@entry_id:153673)方面表现出色。然而，它们并非为捕捉结构对等性而设计。`node2vec` 凭借其可调的有偏游走，可以两者兼顾。通过设置 $q > 1$，它可以模拟[谱方法](@entry_id:141737)的[社区发现](@entry_id:143791)行为。但通过设置 $q  1$，它可以进入发现功能角色和结构相似性的领域——这是更简单的局部平滑目标无法实现的一项壮举 [@problem_id:3331399]。

从本质上讲，`node2vec` 提供了一个统一而灵活的框架。它从一个源自语言的直观类比开始，将其转化为一个优雅的有偏游走机制，并采用强大的学习模型来生成丰富、有意义的网络节点表示。它证明了一个理念：通过仔细定义我们如何探索一个复杂系统，我们就能教会一台机器去理解其隐藏的语法。

