## 引言
在快速发展的[计算机视觉](@entry_id:138301)领域，很少有概念能像视觉 Transformer (ViT) 一样带来如此大的变革。数十年来，该领域一直由[卷积神经网络](@entry_id:178973) (CNN) 主导，CNN 通过模仿人类视觉皮层对局部特征的关注而表现出色。然而，这种对局部性的依赖也带来了固有的局限性，尤其是在需要对图像进行全局理解的任务中。ViT 范式提供了一种激进的解决方案，它摒弃了这些长期以来的假设，不再将图像视为像素网格，而是将其视为一系[列图像](@entry_id:150789)块，类似于句子中的单词。本文将深入探讨实现这一点的架构。第一章**“原理与机制”**将解构 ViT，探索词元化、位置嵌入以及革命性的[自注意力机制](@entry_id:638063)如何使其从零开始学习空间关系。随后的**“应用与跨学科联系”**一章将展示 ViT 非凡的多功能性，从其对医学成像的影响，到其模拟物理定律的惊人能力，揭示了这一通用问题解决工具的强大威力。

## 原理与机制

要真正理解视觉 Transformer，我们必须从理念的转变开始，而不是从代码或公式入手。几十年来，计算机视觉一直被一种架构所主导，它以一种简化的方式模仿我们自己的视觉皮层：[卷积神经网络](@entry_id:178973) (CNN)。想象一位一丝不苟的艺术史学家正在审视一幅巨大的壁画。他们不会一次性看完整幅景象。相反，他们用一个小放大镜扫描，识别微小的图案——这里的一笔，那里的特定色彩混合。然后，他们在大脑中将这些局部的观察结果逐层拼接起来，以建立对更大形状、人物乃至整个艺术作品叙事的理解。

这就是 CNN 的精神。它的“放大镜”是一个**卷积核**，一个小型的权重网格，它在图像上滑动，检测边缘、角落和纹理等局部特征。通过堆叠许多这样的层，CNN 逐渐建立起越来越大的“感受野”，从简单的模式发展到复杂的对象。这个过程将两个强大的假设，或称**[归纳偏置](@entry_id:137419)**，硬编码到网络的设计中。第一个是**局部性**：即相邻的像素比远处的像素更相关。第二个是**[平移等变性](@entry_id:636340)**：即一个对象即使移动到图像的不同部分，它仍然是同一个对象。一只猫无论在左上角还是右下角，它都是一只猫。这些偏置非常有效且数据高效，因为它们在理解视觉世界方面给了网络一个强大的起点。 `[@problem_id:4496228]`

而视觉 Transformer (ViT) 提出了一个惊人大胆的问题：如果我们把这些全都抛弃会怎样？

### 一种新哲学：像拼图一样看世界

ViT 不再采用那种小心翼翼、循序渐进的检查方式，而是像对待一幅打乱的拼图一样处理图像。第一步，称为**词元化** (tokenization)，是将图像切割成一个不重叠的图像块网格。例如，一个标准的 $224 \times 224$ 像素图像可能会被分割成一个 $14 \times 14$ 的 $16 \times 16$ 像素图像块网格。然后，这些小图像块中的每一个都被展平成一个长向量，并线性投影到一个特定的维度，从而创建一系列“词元” (token)。`[@problem_id:4834552]`

这个简单的操作意义深远。原本刚性的二维像素网格消失了。取而代之的是一个一维的词元序列——可以看作一个“图像块包”——就像一个句子是一个单词序列一样。像素与其直接邻居之间的固有空间关系被打破了。

然而，这种初始的切割引入了一个微妙但关键的局限性。每个图像块投影成词元的过程通常涉及对其像素进行某种形式的平均。如果一个具有诊断关键性的特征——比如病理切片中一小簇异常细胞——远小于其所在的图像块，它的信号可能会被周围平淡无奇的背景所稀释。存在一个最小对象尺寸 $s_{\text{min}}$，低于该尺寸，对象的信号就会在图像块的噪声中消失。这个[可检测性](@entry_id:265305)阈值取决于图像块大小 $p$、对象的强度差异 $\Delta I$ 和噪声水平 $\sigma$。仔细分析揭示了这样一个关系：$s_{\text{min}} = \sqrt{\frac{\gamma \sigma p}{\Delta I}}$，这表明随着图像块大小 $p$ 的增加，最小可检测对象的尺寸也会增加。具有讽刺意味的是，ViT“全局”视图的第一步，是一种纯粹的局部平均，而这种平均可能恰恰掩盖了我们希望找到的细节。`[@problem_id:3199228]`

### 失忆的网络与位置的馈赠

将图像转化为词元序列后，我们现在将其输入 Transformer 的核心引擎。但这里我们面临一个灾难性的问题。源于自然语言处理领域的标准 Transformer 架构是**置换等变的**。如果你给它一个句子，“The cat sat on the mat,” 它会进行处理。如果你给它一个打乱顺序的句子，“mat the on sat cat The,” 其内部机制会相应地产生一个原始输出的打乱版本，完全意识不到句子的意思已经被破坏了。

应用于我们的图像块，这意味着网络是一个失忆者。它看到了所有的拼图碎片，但完全不知道它们应该放在哪里。它无法区分一幅正确渲染的人脸和一幅被打乱的毕加索风格画作。`[@problem_id:4834552]`

为了治愈这种失忆症，我们必须明确地赋予网络视觉——或者说，位置感。在词元进入主处理模块之前，我们为每个词元添加一个独特的“空间签名”。这就是**位置嵌入**。对于序列中的每个位置（第一个、第二个、第三个等），我们都有一个可学习的向量，它表示“我来自左上角的词元”，或者“我来自最中心的词元”。通过将这个位置向量加到图像块的内容向量上，我们创造了一个新的词元，它既知道自己*是什么*，也知道自己*在哪里*。

这个想法的力量可以通过一个简单而优雅的实验来体现。想象一个微小的 $2 \times 2$ 图像网格，我们得到四个图像块。我们创建一个数据集，其中每张图像都包含相同的一组图像块：两个 'A' 型和两个 'B' 型。从一张图像到另一张图像，唯一改变的是它们的空间排列。例如，一类图像可能将 'A' 放在主对角线上，而另一类则将它们放在[反对角线](@entry_id:155920)上。一个没有位置信息的模型将完全无法察觉这种差异。但是一个带有位置嵌入的 ViT 可以学会完美地解决这个任务。它可以学习一个“查询”，有效地询问：“左上角和右下[角位置](@entry_id:174053)的内容是什么？” 它通过位置嵌入的引导，检查这些特定位置，如果在那儿找到了 'A'，它就能正确地对图像进行分类。ViT 就是通过这种简单的机制，在将世界粉碎成碎片后，又重建了其空间属性。`[@problem_id:3199205]`

### 全员通信：[自注意力机制](@entry_id:638063)

现在我们的词元被赋予了内容和位置，它们准备好进行交互了。这就是**[自注意力](@entry_id:635960)**的魔力所在。在 CNN 中，信息传播缓慢，就像池塘里的涟漪，从一个局部邻域传播到下一个。而在 ViT 中，每个词元都可以直接与网络每一层中的其他所有词元进行通信。

把它想象成一次委员会会议。房间里的每个“词元-人”都想更新自己对当前议题的理解。为此，在每一轮讨论（一个注意力层）中，每个人都从自己当前的知识状态中生成三个向量：

1.  **查询 ($Q$)**：一个代表他们正在寻找什么的问题。“我是一个显示耳朵的图像块；我正在寻找可能构成一张脸的其他图像块。”
2.  **键 ($K$)**：一个关于他们包含什么的广告。“我是一个显示眼睛的图像块；我是脸部拼图的一个关键部分。”
3.  **值 ($V$)**：他们持有的实际信息或内容。“这是代表我眼睛图像块的详细特征向量。”

然后，每个词元的查询都与所有其他词元的键进行比较（通常通过点积）。这会产生一个相关性或**注意力分数**。你的查询和另一个词元的键之间的高分意味着“你拥有我正在寻找的信息！”这些分数随后被归一化（使用 softmax 函数）为总和为一的权重。最后，每个词元通过计算所有其他词元值的加权和来更新自己的表示。它会更关注它认为最相关的词元。

这种“全员对全员”的通信协议从第一层开始就赋予了 ViT 一个**全局[感受野](@entry_id:636171)**。原则上，它可以模拟图像中任意两个部分之间的关系，无论它们相距多远。考虑一个合成任务，其中一个“对象”被定义为两个相同但很小的标记，它们被放置在一张大图像中相距很远的位置。一个 CNN，由于其局部窗口，会看到两个不相连的东西，并可能将它们计为两个独立的对象。它很难跨越这种[长程依赖](@entry_id:181727)。然而，ViT 在这方面表现出色。一个标记的词元可以学习一个查询，在图像的任何地方寻找其双胞胎的键。通过注意力，它找到它的伙伴，模型就能正确地将它们识别为单个对象。`[@problem_id:3199150]`

同样的原理也使 ViT 对某些类型的遮挡具有显著的鲁棒性。想象一个物体，其身份取决于其两端特征的同时出现，但其整个中心部分都被遮挡了。CNN 的信息通路将被切断。然而，ViT 可以简单地“关注”越过被遮挡的区域，直接连接两个可见的、信息丰富的部分，并做出正确的推断。`[@problem-id:3199235]`

### 全局视图的代价与回报

这种强大的全局通信并非没有代价。[自注意力](@entry_id:635960)的计算核心是创建一个 $N \times N$ 的注意力分数矩阵，其中 $N$ 是图像块（或词元）的数量。这意味着计算成本与词元数量呈二次方增长，这一复杂度我们可以表示为 $\mathcal{O}(N^2)$。`[@problem_id:3199246]`

其后果是严峻的。假设我们有一张 $1024 \times 1024$ 的图像。如果我们使用 $16 \times 16$ 的图像块，我们会得到 $(1024/16)^2 = 64^2 = 4096$ 个词元。如果我们决定需要更多细节而改用 $8 \times 8$ 的图像块，词元数量将翻四倍，达到 $(1024/8)^2 = 128^2 = 16384$ 个。[注意力机制](@entry_id:636429)的计算成本不仅仅是翻四倍；它会猛增 $4^2=16$ 倍。这种二次方缩放是 ViT 的阿喀琉斯之踵，使其在处理高分辨率图像时计算成本非常高昂。`[@problem_id:4353718]`

付出这个代价的回报是，我们得到了一个比 CNN 具有弱得多的[归纳偏置](@entry_id:137419)的模型。由于不假设局部性，ViT 更加灵活，理论上可以学习更多种类的模式。然而，这种灵活性是一把双刃剑。CNN 从其架构中“免费”获得了局部性；而 ViT 必须通过观察数据从头开始学习空间的规则。这就是为什么在较小的数据集上，ViT 的表现通常不如 CNN。它们需要被展示海量的例子（在拥有数亿张图像的数据集上进行预训练）来学习视觉世界的基本属性，而这些属性是 CNN 早已知道的。`[@problem_id:4496228]`

### 堆叠层与保持稳定

单层的[自注意力](@entry_id:635960)很强大，但真正的深度学习来自于堆叠这些层。一个“委员会会议”的输出成为下一个会议的输入。一个使其成为可能的关键创新是**[残差连接](@entry_id:637548)**。在每个注意力块之后，原始输入被加回到该块的输出上：$x_{\text{new}} = x_{\text{old}} + \text{Attention}(x_{\text{old}})$。这创建了一条“信息高速公路”，允许原始信号和训练时的梯度在网络中畅通无阻地流动。没有它，训练非常深的 Transformer 几乎是不可能的。

即使有[残差连接](@entry_id:637548)，稳定性也是一个主要问题。向量的幅度可能会在逐层传递中爆炸性增长。一个微妙但至关重要的架构选择是[归一化层](@entry_id:636850)（**LayerNorm**）的放置位置。早期的 Transformer 将 LayerNorm 放在残差加法*之后*（post-LN）。仔细分析表明，这可能导致信号幅度呈指数级、不受控制的增长。后来的一项创新是将归一化移到块的*开始*，在计算注意力之前（pre-LN）。这个看似微小的改变驯服了爆炸性增长，将其从指数级级数转变为更易于管理的线性级数。这是[深度学习](@entry_id:142022)工程中一个美丽的例子，它展示了如何使这些架构在实践中奏效。`[@problem_id:3199138]`

最后，有了这个由堆叠的、带残差的、归一化的注意力层组成的复杂机器，我们如何才能指望理解模型在“想”什么呢？一种优雅的技术被称为**注意力展开**（attention rollout）。通过将每一层的注意力权重视为一个[变换矩阵](@entry_id:151616)，我们可以“展开”信息的流动。为了正确地考虑[残差连接](@entry_id:637548)，我们在每一层将注意力矩阵与一个单位矩阵进行数学上的组合。然后，通过将这些有效的[变换矩阵](@entry_id:151616)在所有层上相乘，我们可以计算出一个单一的、端到端的注意力图。这张图揭示了每个初始输入图像块对任何其他词元的最终输出有多大影响，为我们提供了一个强大的镜头来窥视 ViT 的推理过程。`[@problem_id:4330006]`

从其与传统观念的激进哲学决裂，到注意力的复杂机制以及使其稳定的工程优雅，视觉 Transformer 代表了一段通往新视觉方式的旅程。它将我们的世界解构成一个拼图，然后通过一场全员通信的交响乐，学会将其重新组合在一起。

