## 引言
在构建智能系统的探索中，我们已经从教机器掌握特定技能转向为它们提供基础教育。这一[范式](@article_id:329204)转变由[预训练](@article_id:638349)驱动，在[预训练](@article_id:638349)过程中，模型首先从海量未标注数据中学习通用表示。该过程的核心是**[预训练目标](@article_id:638546)**——即模型被训练去解决的特定任务或“游戏”。这个目标的选择并非一个简单的技术细节，而是在塑造模型“世界观”、决定其最终成败方面最关键的决策。

本文探讨了这些目标如何塑造模型知识这一根本问题。文章将探索一个看似简单的“填空”游戏为何能教会模型人类语言的语法，乃至编码在 DNA 中的生命语言。

在接下来的章节中，您将对现代人工智能的这一基石获得深刻而直观的理解。“原理与机制”一章将解构不同目标的工作方式、它们教授的内容以及设计不当的课程所带来的危险。随后的“应用与跨学科联系”一章将揭示这一概念的普适力量，展示它如何将进化生物学、计算机视觉和工程学等截然不同的领域联系起来，而所有这些都统一在为模型提供良好开端的核心原则之下。

## 原理与机制

想象一下，你受命从零开始创造一个才华横溢、多才多艺的头脑。你不可能教会它所需的所有事实或技能。世界太广阔，未来太难预测。一个更好的策略是给它提供基础教育——一套旨在培养通用问题解决能力、深度逻辑感以及对世界运作方式的直觉的课程。当这个头脑日后面对一个特定而新颖的问题时，它不会从零开始。它将利用其丰富的基础，以惊人的速度和优雅的方式学习新技能。

这[实质](@article_id:309825)上就是[预训练](@article_id:638349)大型神经模型背后的理念。**[预训练目标](@article_id:638546)**是我们为这些人工智能设计的课程。它是一个代理任务，一个模型被迫大规模进行的游戏，并非因为我们关心游戏本身，而是因为我们相信，掌握这个游戏将锻造出强大、通用的数据内部表示。这个游戏——这个目标——的选择并非一个简单的技术细节；它是在塑造模型“世界观”、其能力及局限性方面最重要的单一决策。

### 课程为王：模型学到的就是你要求它做的

模型是一个非常刻板的学生。它会精确地学习你奖励它的行为，并且会不择手段地获取奖励。[预训练](@article_id:638349)的美妙与危险就在于这种刻板。不同的目标，或课程，会灌输根本不同类型的理解。

让我们来看几种流行的教育理念。

首先是**掩码语言模型 (MLM)** 目标，它是像 BERT 这样的模型背后的驱动力。这是一个“填空”游戏。我们取一个句子，隐藏几个词，然后要求模型根据周围的上下文来预测它们。一个在这种游戏上训练了万亿个句子的头脑能学到什么？它会对语言的局部结构——语法、句法和常见的词语搭配——产生非凡的直觉。它会学到“the dog ___ the ball”很可能被“chased”或“caught”这样的动词填补。它是一个局部上下文的大师。

第二种方法是**[对比学习](@article_id:639980)**。想象一下，给一个学生看无数对图像。对于每一对，你只告诉他们“这两张是同一事物的不同视角”或“这两张是不同事物的”。这是一个宇宙尺度的“找不同”游戏。模型的目标是学习一个编码器，将同一物体的不同视角（例如，一只猫的正面和侧面）映射到高维空间中的相近点，同时将不同物体（一只猫和一只狗）的表示推得尽可能远。以这种方式训练出来的思维会成为识别物体*本质*的专家。它学会了对无关变换（如光照、角度或颜色的变化）保持**[不变性](@article_id:300612)**，而只关注核心的、决定性的特征。

第三种理念是**[自编码器](@article_id:325228)**，它玩的是一种“凭记忆重构”的游戏。我们给模型一个输入，迫使它通过一个计算瓶颈（一个压缩表示），然后要求它尽可能完美地重构原始输入。为了成功，模型必须学会有智慧地使用其有限的内存。它必须决定输入的哪些方面最值得保留。对于图像或信号，“最重要”通常意味着“最高方差”。模型实际上学会了执行非线性版本的**[主成分分析 (PCA)](@article_id:352250)**，保[留数](@article_id:348682)据中“最响亮”的成分，而丢弃“最安静”的成分 [@problem_id:3162652]。

这里深刻的教训是，即使是课程中的细微变化也可能导致截然不同的技能。以最初 BERT 的**下一句预测 (NSP)** 目标为例。模型会看到两个句子 A 和 B，并必须预测 B 是文本中实际的下一句，还是从别处随机抽取的句子。其目标是教模型关于语篇和句子之间关系的知识。但研究人员发现了一个奇怪的缺陷：模型在这个任务上表现得非常好，但并非通过学习深层[连贯性](@article_id:332655)。相反，它注意到随机的“负”句子几乎总是来自不同的文档，因此主题也不同。模型找到了一个捷径：它学会了成为一个主题分类器！

随后的一个目标，**句子顺序预测 (SOP)**，通过更巧妙的课程设计修复了这个问题 [@problem_id:3102444]。在 SOP 中，负样本是通过简单地交换两个连续句子的顺序来创建的。现在，“正确顺序”和“交换顺序”的句子对都来自同一文档和同一主题。主题捷径消失了。为了成功，模型被迫学习逻辑流程和连贯性的那些微妙而真实的线索。精心设计的课程能精确地塑造学生的思维。

### 设计不当课程的危害

如果目标是王，那么一个选择不当的目标可能成为暴君，引导模型走向优雅但无用的解决方案。这主要通过两种方式发生：错位和退化。

**错位：当代理任务选错时**

最引人入胜的失败发生在，一个看似完全合理的目标在根本上与真实目标不一致时。想象一下我们的[自编码器](@article_id:325228)，它被勤奋地训练以保留最高方差的方向。现在，假设我们想用它学到的表示来解决一个分类问题，而其中关键的、用于区分的特征方差非常非常小——就像在一个充满叫喊声的房间里的一丝低语 [@problem_id:3162652]。[自编码器](@article_id:325228)在追求最小化重构误差的过程中，学会了成为捕捉叫喊声的专家。它实际上对那丝低语充耳不闻。它产生的表示，虽然是数据方差的高保真摘要，但对于下游任务却毫无用处。一个在原始数据上训练的简单分类器，可以学会倾听那丝低语，其性能将远超这个复杂的[预训练](@article_id:638349)模型。

我们可以从信息论的角度来思考这个问题 [@problem_id:3195202]。一个输入 $X$ 既包含信号 $S$（任务所需）也包含无关信息 $N$（我们不需要的）。[对比学习](@article_id:639980)目标明确地尝试实现对 $N$ 的[不变性](@article_id:300612)，从而有效地丢弃了关于它的信息。这是一个强有力的策略，当且仅当你定义为无关信息的东西对于所有未来任务都确实无关时。但如果你的下游任务恰好依赖于那个所谓的无关信息（即 $I(Y;N \mid S) > 0$），那么你的[预训练](@article_id:638349)就通过丢弃关键信息而永久性地损害了表示。贝叶斯误差，即可能达到的最佳错误率，会因为你雕琢掉了部分信号而增加。

**退化解：在考试中作弊**

模型，就像一些学生一样，可能会偷懒。如果目标中存在一个漏洞，允许它们在不付出学习的艰辛努力的情况下获得非常低的损失，它们就会找到这个漏洞。在[对比学习](@article_id:639980)中，这被称为**表示坍塌**。模型学会将每一个输入都映射到完全相同的点或一个非常小的空间区域。现在，同一图像的任意两个视角都被映射到同一点（完美对齐！），损失因此骤降至接近零。模型在考试中获得了满分。但这个表示完全无用——就像一本字典里每个词都有相同的定义。

我们如何发现这场灾难？[学习曲线](@article_id:640568)揭示了真相 [@problem_id:3115515]。在健康的训练过程中，[预训练](@article_id:638349)损失下降，而下游验证任务的性能上升。它们同步变化。但如果你看到[预训练](@article_id:638349)损失突然骤降至接近零，而下游准确率停滞不前甚至下降，就应该亮起红灯。模型很可能找到了一个“作弊码”。解决方案通常是让课程更难：使用更强的[数据增强](@article_id:329733)或增加负样本的数量，使其更难找到一个微不足道的解。

### 回报：量化良好教育的价值

当课程设计得当时，其益处是深远且可衡量的。一个好的[预训练目标](@article_id:638546)不仅产生一个模型；它产生一个学习效率更高的模型。

最 tangible 的好处是**[样本效率](@article_id:641792)**。想象两个学生学习微积分。一个有很强的代数背景，另一个没有。第一个学生会更快地掌握新概念。同样，一个经过良好[预训练](@article_id:638349)的模型需要少得多的标记样本来掌握一个新的下游任务。我们可以通过绘制[学习曲线](@article_id:640568)来优美地展示这一点 [@problem_id:3138175]。对于一个从零开始训练的模型，验证损失随着训练样本数 $n$ 的增加而减少。一个[预训练](@article_id:638349)模型展现出类似的曲线，但它向左平移了。它可以用一小部分数据达到相同的损失，就好像它实际上是在 $s \cdot n$ 个样本上训练的，其中 $s > 1$ 是一个“[预训练](@article_id:638349)[质量指数](@article_id:369825)”，量化了其先前教育的价值。

这不仅仅是一个定性的画面。我们可以从[预训练](@article_id:638349)的进展直接推断下游的潜力。对于一个语言模型，它在[预训练](@article_id:638349)期间对一个留存文本的[困惑度](@article_id:333750)（一种衡量其不确定性的指标）与它在对新任务进行微调时能达到的最佳性能（渐近误差）有很强的相关性 [@problem_id:3115529]。这使我们能够就何时停止极其昂贵的[预训练](@article_id:638349)过程做出有原则的决定。只要[困惑度](@article_id:333750)的提升能转化为下游潜力的有意义的改善，我们就会继续。

最后，我们可以为特定的优点量身定制课程。如果我们知道我们的模型将被部署在一个充满噪声、混乱数据的世界中——比如带有拼写错误的用户生成文本——我们可以通过将这类噪声纳入[预训练目标](@article_id:638546)本身来使其更具鲁棒性。通过在字符级损坏上进行训练，我们可以创建一个模型，它学会看穿表面的拼写错误，抓住潜在的含义，而这种技能是它从完全干净的文本中无法获得的 [@problem_id:3102531]。

因此，[预训练目标](@article_id:638546)不仅仅是一个[损失函数](@article_id:638865)。它声明了我们关于什么是重要的、什么是无关的、什么构成理解以及我们希望在模型中灌输何种美德的信念。这是一个充满深刻而优美问题的领域，而我们找到的答案反映在我们正在构建的人工智能那非凡的能力——以及那些说明问题的缺陷中。

