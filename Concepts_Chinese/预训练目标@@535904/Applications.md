## 应用与跨学科联系

### 良好开端的艺术：作为[功能变异](@article_id:350010)的学习

在进化生物学中，有一个优美的概念叫做“[功能变异](@article_id:350010)”（exaptation）。它描述的是一种在某组压力下进化出的性状，后来被用于一个全新的目的。古代恐龙的羽毛很可能是为了[体温调节](@article_id:307751)或展示而进化的，但它却成了其后代实现飞行的必要先决条件——一种[功能变异](@article_id:350010)。这个结构早已存在，充满了潜力，等待着新的挑战。

机器学习中[预训练](@article_id:638349)的理念与这一自然原则惊人地相似。我们首先给予模型一种通用的、基础的教育。我们不教它特定的、狭隘的技能。相反，我们让它[沉浸](@article_id:320671)在海量的未标注数据世界中——互联网的文本、已知基因组的文库、数百万张照片——然后我们要求它解决一个简单的、自成一体的谜题。它在解决这个谜题时获得的知识成为一种强大的[功能变异](@article_id:350010)。这种对世界结构的通用“理解”随后可以被迅速地适应或“微调”，以非凡的效率和准确性解决一个新的、特定的问题 [@problem_id:2373328]。这并非为每个任务都从头构建一个新工具；而是拿一个奇妙复杂的、预先存在的工具，为新功能做一些小而巧妙的修改。

### 学习世界的语言：从文字到基因组

也许[预训练](@article_id:638349)最自然的领域是在序列领域，其中顺序和上下文就是一切。最熟悉的序列当然是人类语言。一个模型可以通过玩一个简单的游戏在海量文本上进行训练：我们给它看一个涂掉几个词的句子，然后让它猜出缺失的词。要精通这个游戏，模型不能仅仅记住单词；它必须学习语法、上下文和语义。它必须学到，“queen”这个词在“the king and...”的上下文中是一个合理的替换词，因为它已经含蓄地学习了概念之间的关系。

同样地，这个原则可以驱动更复杂的任务，比如机器翻译。在从法语翻译成英语之前，拥有一张两种语言都指向的统一概念地图会很有帮助。我们可以通过一个名为*[对比学习](@article_id:639980)*的[预训练目标](@article_id:638546)来实现这一点。我们取大量平行的句子（例如，一个英语句子及其法语翻译），然后教模型为意思相同的句子[对生成](@article_id:314537)相似的[向量表示](@article_id:345740)，同时将所有不匹配的句子对的表示推得远远的。这个过程不教模型如何逐词翻译，而是教了一些更深刻的东西：它迫使模型构建一个共享的“意义空间”，一个“le roi”和“the king”落在同一邻域的地图。有了这张地图，学习具体的翻译规则就变得简单得多 [@problem-id:3173686]。

但如果我们告诉你，完全相同的思想可以用来解读所有语言中最古老的那一种——生命语言呢？基因组是一本由四个字母（$A, C, G, T$）写成的书，而蛋白质则是一个由二十种氨基酸写成的复杂句子。通过将相同的[掩码语言建模](@article_id:641899)技术应用于庞大的蛋白质和 DNA [序列数据](@article_id:640675)库，我们可以训练出能学习生物学“语法”的模型 [@problem_id:2429075]。这些模型凭借自身，发现了被数十亿年进化雕刻出的深层统计模式。它们为蛋白质中的每个氨基酸生成的上下文[嵌入](@article_id:311541)不仅仅是任意的向量；它们是丰富的描述符，含蓄地编码了关于[蛋白质三维结构](@article_id:372078)和生物学功能的信息，而所有这些都从未见过任何一个关于这两者的标注样本 [@problem_id:2749082]。

这带来了革命性的后果。例如，找到一个“[启动子](@article_id:316909)”——一个启动基因表达的特殊 DNA 序列——就像在一个句子中找到动词。一个生物学家可能只有几百个特定生物的[启动子](@article_id:316909)样本。对于一个从零开始训练的模型来说，这远远不够。但对于一个在整个人类基因组上[预训练](@article_id:638349)过的、已经理解这种语言的模型来说，这只是一个简单的微调任务。[预训练](@article_id:638349)的知识充当了极其强大的归纳偏见，极大地减少了达到高准确性所需的标注数据量 [@problem_id:2429075]。

当然，设计这些生物学[预训练](@article_id:638349)任务需要非常小心。我们很容易会无意中创造出一个过于简单的谜题。想象一下，我们想通过让模型预测一个被掩盖的氨基酸的结构（例如，$\alpha$-螺旋或$\beta$-折叠）来教它关于蛋白质折叠的知识。如果我们向模型提供了该[残基](@article_id:348682)在蛋白质链中直接相邻[残基](@article_id:348682)的精确结构信息，它就可以通过复制它们来“作弊”，因为相邻[残基](@article_id:348682)通常具有相同的结构。这样，它学到的是一个微不足道的局部规则，而不是支配折叠的复杂长程力。一个精心设计的目标会仔细地隐藏信息，迫使模型学习系统中更深层次的、非局部的物理规则来解决这个谜题 [@problem_id:2395460]。

这种[范式](@article_id:329204)的力量使我们能够跨越看似不可逾越的鸿沟。假设我们有一个在小药物分子的化学图谱上[预训练](@article_id:638349)的模型，但我们想预测像蛋白质这样巨大的[生物聚合物](@article_id:368448)的性质。其规模、组成和物理原理都完全不同。然而，通过一系列有原则的步骤——使模型适应新的原子词汇，利用在未标注蛋白质数据上的[自监督学习](@article_id:352490)来学习新领域的统计特性，甚至增加新的架构模块来理解[三维几何](@article_id:355311)——我们可以成功地迁移知识。在小分子上学到的基本化学原理为理解大得多的分子提供了基础 [@problem_id:2395410]。这一旅程最终使我们能够进行*计算机*生物设计。借助一个强大的[预训练](@article_id:638349)模型，它提供了一个平滑、有意义的蛋白质宇宙“地图”，我们可以使用像[贝叶斯优化](@article_id:323401)这样的复杂[搜索算法](@article_id:381964)来导航这张地图，智能地探索和利用它来发现具有所需功能的新蛋白质，从而使[生物工程](@article_id:334588)的过程效率提高几个[数量级](@article_id:332848) [@problem_id:2749082]。

### 看见世界：教机器拥有常识

从一维序列转向二维图像，挑战依然相同：我们如何教模型其世界固有的结构？一张图像不是一堆随机的像素；它是一个由物体、纹理和空间关系构成的连贯场景。我们可以通过一个极其简单的游戏来教这种“视觉常识”：拼图游戏。

想象一下，拿一张图片，把它切成一个网格状的图块，打乱它们，然后让模型把它们按正确的顺序拼回去。要解决这个谜题，模型不能只看相邻图块的颜色。它必须学会“狗耳朵”长什么样，并且知道它通常出现在“狗眼睛”的上方。它必须学会草通常在场景的底部，而天空在顶部。通过在数百万张图片上解决这个自监督的拼图游戏，模型发展出一种对视觉世界结构的内部表示。这种[预训练](@article_id:638349)的结构性理解可以为它在其他更复杂的视觉任务上提供巨大的优势，比如将白天的场景转换成夜晚的场景，因为它已经知道什么是“场景” [@problem_id:3127633]。

### 科学与工程的通用工具箱

[预训练](@article_id:638349)的理念并不仅限于语言和图像的“非结构化”数据。它是一个在科学和工程学科中加速发现的通用工具。

考虑一个经典的工程问题：预测涡轮叶片内部的热传递，该叶片可能具有复杂的、带肋的内部通道以增强冷却。对此进行仿真是计算昂贵的，而物理实验则更是如此。我们可以构建一个快速的“代理模型”来近似物理过程。如果我们只有复杂带肋通道的少数数据点，一个从零开始训练的模型将是不准确的。然而，一个简单的、光滑平板的物理过程要容易建模和生成数据得多。我们可以先在这个简单的系统上[预训练](@article_id:638349)我们的[代理模型](@article_id:305860)。模型学习了[对流](@article_id:302247)的基本标度律——热传递如何依赖于流速和流体性质。这种植根于物理学的知识，作为一个极好的起点。当我们随后用来自复杂带肋通道的少数数据点对这个模型进行微调时，它学习得更快，并产生更准确的预测。[预训练](@article_id:638349)赋予了它一种物理“直觉” [@problem_id:2502983]。

这种提高“[样本效率](@article_id:641792)”的概念可以变得更加具体。在[强化学习](@article_id:301586)中，智能体通过试错来学习。如果智能体必须从零开始学习看和行动，可能需要数百万次尝试。但如果我们先在一个大型图像数据集上[预训练](@article_id:638349)其[视觉系统](@article_id:311698)，它就带着一双能工作的“眼睛”进入新环境。它已经可以区分物体和纹理。这意味着它需要少得多的试验次数来学会掌握任务。我们可以用一个简单的数学抽象来模拟这个过程：[预训练](@article_id:638349)将我们的模型移动到可能解决方案的广阔空间中一个好得多的起点，并且它还可以重塑“学习景观”，使通往最优解的路径更平滑、更直接 [@problem_id:3195203]。

### 深入探究：良好猜测的数学原理

为什么这种“领先一步”如此有效？从本质上讲，从有限数据中学习是一种平衡行为。我们应该在多大程度上信任我们拥有的少数数据点，又在多大程度上依赖我们对世界的[先验信念](@article_id:328272)？[预训练](@article_id:638349)提供了一个强大的、由数据驱动的先验信念。

从贝叶斯角度来看，微调一个[预训练](@article_id:638349)模型就像带着一个非常有力、有充分根据的假设开始一项调查。我们不是平等地考虑所有可能的解决方案，而是告诉模型，真正的解决方案很可能“接近”在[预训练](@article_id:638349)期间发现的那个。这种正则化防止模型被小数据集中的噪声所左右，而去追求一个虽然完美拟合少数样本但最终是错误的解决方案 [@problem_id:2429075] [@problem_id:2749082]。

我们甚至可以用优美的数学语言清晰地写下这一点。想象一下，我们想为一个特定任务学习一个模型，该任务由一组理想参数 $\boldsymbol{\theta}$ 来表征。我们有少量数据（强度为 $n$）、一个通用的[预训练](@article_id:638349)先验（$\mathbf{w}_{\mathrm{pre}}$，信任因子为 $\mu$），或许还有一个来自相关任务的更具体的先验（$\mathbf{w}_{\mathrm{par}}$，信任因子为 $\lambda$）。我们模型参数的最佳估计值 $\mathbf{w}^\star$ 结果是一个简单的加权平均：

$$
\mathbf{w}^\star = \frac{n\boldsymbol{\theta} + \lambda\mathbf{w}_{\mathrm{par}} + \mu\mathbf{w}_{\mathrm{pre}}}{n + \lambda + \mu}
$$

这个优雅的公式揭示了一切。我们的最终信念 $\mathbf{w}^\star$ 是新数据告诉我们的信息、我们的一般经验所建议的以及我们领域特定知识所蕴含的内容的混合体。当新数据稀缺时（$n$ 很小），由[预训练](@article_id:638349)提供的先验（带有 $\lambda$ 和 $\mu$ 的项）在结果中占主导地位，提供了一个稳定而合理的猜测。随着我们收集更多数据（$n$ 很大），这些先验的影响会减弱，我们让自己更多地被直接证据所引导。这是对如何智能学习的完美数学描述 [@problem_id:3195251]。

### 非监督数据的超常有效性

世界充满了数据，但大部分是未标注的。很长一段时间里，这被认为用处有限。[预训练目标](@article_id:638546)的魔力在于，它们提供了一把钥匙，解锁隐藏在这个未标注宇宙中的巨大价值。通过发明巧妙而简单的游戏——预测缺失的词、重组拼图、学习对比相似与不相似的事物——我们给模型一个理由去探索和内化数据的结构。

这个过程赋予它们一种关于其训练领域的“常识”或“直觉”。这种学到的知识是一个通用的基础，一个强大的[功能变异](@article_id:350010)，可以被应用于无数的专门问题。它揭示了学习原则中深刻的统一性，连接了语言的语法、生命的逻辑、视觉的物理学和推理的数学，并从根本上改变了科学和工程领域中可能实现的一切。