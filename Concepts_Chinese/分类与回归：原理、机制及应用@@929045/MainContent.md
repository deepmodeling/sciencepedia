## 引言
在广阔的机器学习领域，能否从数据中提取有意义的洞见，取决于我们是否提出了正确的问题。所[有监督学习](@entry_id:161081)问题都从一个关键的区别分叉而出：我们是试图预测一个类别还是一个数量？这个选择将该领[域划分](@entry_id:748628)为两个最基本的范畴：[分类与回归](@entry_id:637626)。误解这一差[异或](@entry_id:172120)误用相应的方法可能导致模型缺陷和结论错误，因此，这一区别是任何数据科学家首先必须掌握的最重要的概念。

本文对分类和回归进行了从基础理论到实际应用的全面探讨。我们将阐明定义这些任务并支配模型学习方式的核心原理。在接下来的章节中，我们将首先深入探讨“原理与机制”，正式定义分类和回归，探索指导它们的[损失函数](@entry_id:136784)以及执行这些任务的核心算法——如[决策树](@entry_id:265930)和 k-近邻。然后，在“应用与跨学科联系”部分，我们将遍览不同的科学领域，看这些方法如何被用来解决现实世界的问题，从设计新药到诊断疾病，揭示这个简单而强大的选择所带来的深远影响。

## 原理与机制

在机器学习乃至众多科学领域的核心，在于提出正确问题的艺术。宇宙中充满了数据，从奇异晶体的电子特性到医院病人的生命体征。学习算法本质上是我们设计的一种机制，用以筛选这些数据并学习我们所提问题的答案。这个问题的性质从根本上将监督学习的世界划分为两大领域：**分类**（classification）与**回归**（regression）。

### 两类问题的传说：类别还是数量？

想象你是一位材料科学家，拥有一个包含大量新合成化合物的庞大数据库。你手头有每种化合物的一系列特征——[化学式](@entry_id:136318)、[晶格](@entry_id:148274)几何形状等等。现在，你可以提出两种截然不同的问题。

首先，你可能会问：“根据其特征，这种新材料是金属、半导体还是绝缘体？”在这里，你要求机器将材料放入几个预定义的类别或**类**（classes）中。这就是**分类**的本质。输出是一个标签，一个类别。答案可能是“半导体”，而不能是“有点像半导体”。它是在一组离散可能性中的一个选择 [@problem_id:1312321]。

或者，你也可以问：“这种新材料的精确电学[带隙](@entry_id:138445)是多少，单位是[电子伏特](@entry_id:144194)？”现在，你要求的是一个数字。答案可能是 $2.7$ eV，或 $2.718$ eV，或某个连续范围内的任何值。你要求的是一个数量，而不是一个类别。这就是**回归**的世界。

这个单一的区别——预测**类别**与预测**数量**——是监督学习中最重要的组织原则。为其中一个任务构建的模型通常不适用于另一个，这并非因为某些微小的技术细节，而是因为其目标的本质就不同。一个预测术后并发症是“胆漏”、“出血”还是“无”的外科AI是在进行分类。而一个估算同一次手术中失血量（以毫升为单位的连续值）的AI则是在进行回归 [@problem_id:5110421]。

### 科学的记分卡：何为“好”的预测？

机器如何学会“出色地”完成其任务？我们必须给它一张记分卡——一种衡量其误差的方法。在机器学习中，这张记分卡被称为**[损失函数](@entry_id:136784)**（loss function）。整个学习过程就是试图将这个函数上的得分降到尽可能低。其精妙之处在于，[损失函数](@entry_id:136784)的选择并非任意；它精确地体现了我们所认为的“好”的预测是什么。

对于回归，一个自然的选择是**[平方误差损失](@entry_id:178358)**（squared error loss），$L(y, \hat{y}) = (y - \hat{y})^2$，其中 $y$ 是真实值，$\hat{y}$ 是模型的预测值。这个简单的公式有一个非常直观的特性：它对大错误的惩罚远超小错误。偏差10个单位比偏差1个单位要糟糕100倍。一个训练来最小化此损失的模型，对于任何给定的输入 $X$，会倾向于预测该输入下 $Y$ 的*平均值*——我们称之为**条件期望**（conditional expectation），$E[Y \mid X]$ [@problem_id:5188895]。这通常正是我们想要的。一个预测失血量并最小化平方误差的模型，实际上总是在尝试猜测它在类似情况下见过的平均失血量 [@problem_id:5110421]。

但如果平均值具有误导性呢？考虑预测病人的住院天数 [@problem_id:4579939]。大多数病人可能住院3-5天，但少数病人可能会有严重并发症并住院100天。这些极端离群值会把平均值拉得很高。基于平均值的预测对大多数病人来说可能持续偏高。在这种情况下，我们可能更关心**[中位数](@entry_id:264877)**——即50%的病人住院时间更长，50%的病人住院时间更短的值。要让我们的模型预测[中位数](@entry_id:264877)，我们只需更换记分卡。我们可以使用**[绝对误差](@entry_id:139354)**（absolute error），$L(y, \hat{y}) = |y - \hat{y}|$，来代替平方误差。更进一步，我们可以使用**[分位数](@entry_id:178417)损失**（pinball loss）函数，让模型以我们期望的任何分位数（例如，第75百分位数）为目标。这揭示了一个深刻的统一性：[损失函数](@entry_id:136784)不仅仅是一种惩罚；它是一个工具，用以精确定义我们希望模型从数据中学习何种统计特性。

对于分类，最简单的记分卡是**[0-1损失](@entry_id:173640)**（0-1 loss）：答对得0分，答错得1分。试图最小化此损失的模型将只会学习预测给定输入下最常见的类别——即**多数类**（majority class）[@problem_id:5188895]。这虽然可行，但通常并不令人满意。医生不仅想知道肿瘤是“恶性”还是“良性”；他们想知道其为恶性的*概率*。我们需要一个[损失函数](@entry_id:136784)，它不仅奖励模型答对，还奖励其有信心地答对（并惩罚其有信心地答错）。这就是**[交叉熵损失](@entry_id:141524)**（cross-entropy loss）的作用。它直接源于最大似然原理，本质上衡量的是模型对真实答案的“惊讶”程度。一个为正确类别分配 $0.9$ 概率的模型会得到一个非常低（好）的分数，而一个分配 $0.1$ 概率的模型会得到一个非常高（差）的分数 [@problem_id:5110421]。这促使模型产生校准过的概率，这对于做出真实世界的决策至关重要。

### 简单的机器，强大的思想

定义了目标之后，我们就可以开始构建机器本身了。令人惊讶的是，一些最强大的思想植根于非常简单、直观的结构中。

#### [决策树](@entry_id:265930)：伟大的划分器

想象一下与你的数据玩“20个问题”的游戏。这就是一棵**[决策树](@entry_id:265930)**（decision tree）。在每一步，树都会针对单个特征提出一个简单的“是/否”问题——例如，“患者年龄是否大于65岁？”或“胆[固醇](@entry_id:173187)水平是否低于200？”这个过程递归地分割数据，将每个数据点引导至一条路径，直到它落入一个最终的桶中，即**[叶节点](@entry_id:266134)**（leaf node）[@problem_id:4910434]。

使这个简单结构如此通用的是，同一个框架可以用于分类和回归。唯一改变的是在[叶节点](@entry_id:266134)中做出的预测以及提问的策略。
-   在**[回归树](@entry_id:636157)**中，对新数据点的预测就是所有落入同一[叶节点](@entry_id:266134)的训练数据点结果的*平均值*。选择问题的标准是使每个后续组内的结果尽可能相似——也就是最小化方差 [@problem_id:5192617] [@problem_id:4910434]。
-   在**[分类树](@entry_id:635612)**中，预测是该[叶节点](@entry_id:266134)中训练点的*最常见类别*（多数投票）。选择问题的标准是使[叶节点](@entry_id:266134)尽可能“纯粹”，即由单一类别主导。这不是通过方差来衡量，而是通过**不纯度**（impurity）指标，如**[基尼指数](@entry_id:637695)**（Gini index）或**熵**（entropy）[@problem_id:5192617]。

#### K-近邻：物以类聚

另一个极其简单的算法是**k-近邻（k-NN）**。其原理我们在日常生活中也会使用：“物以类聚，人以群分”。为了对一个新的数据点进行预测，算法只需在[训练集](@entry_id:636396)中寻找 $k$ 个最相似的数据点（即“最近的邻居”），然后让它们投票 [@problem_id:5205358]。

同样，这个原理也适用于两种任务。要预测患者未来的肌酐水平（回归），你可以找到数据库中 $k=5$ 个最相似的患者，并计算他们肌酐水平的平均值。要预测患者是否会发生不良事件（分类），你可以找到相同的5名患者，看看哪种结果在他们中更常见。最终的预测是多数投票的结果。其核心思想是局部近似：我们假设在一个足够小的邻域内，答案不会有太大变化。一个卓越的理论结果，即普适一致性，表明在非常普遍的条件下，随着我们的数据集无限增大并且我们巧妙地选择邻居数量 $k$，k-NN的预测将收敛到所有可能预测中最好的那个 [@problem_id:5205358]。

### 森林的智慧：从单棵树到随机森林

单棵[决策树](@entry_id:265930)透明且易于理解，但它有一个关键弱点：不稳定。训练数据中一个微小、几乎无关紧要的变化，就可能导致在第一个分裂点提出不同的问题，从而产生一个完全不同的树结构。这使得单棵树成为**高方差、低偏差**的学习器：它们足够灵活以很好地拟合数据（低偏差），但它们的预测可能因样本的不同而剧烈波动（高方差）[@problem_id:5192617]。

我们如何驯服这种方差？我们可以利用“群众的智慧”。我们不依赖于一个专家（单棵树），而是建立一个由多个专家组成的委员会，并对他们的意见取平均。这就是**自助汇聚法**（bootstrap aggregation），或称**bagging**。我们通过从原始数据中*有放回地*抽样（这就是“自助”的部分）来创建数百个不同的训练数据集。每个新数据集都略有不同，但仍能代表整体。然后，我们在每个数据集上训练一棵深的、不稳定的树。为了做出最终预测，我们只需将所有树的预测取平均（对于回归）或进行多数投票（对于分类）[@problem_id:4559726]。这个平均过程极大地平滑了单棵树的剧烈波动，从而在不损害偏差的情况下降低了方差。

**[随机森林](@entry_id:146665)**（Random Forests）引入了另一个巧妙的转折。为了使我们委员会中的树彼此之间更加不同，我们增加了另一层随机性。在构建每棵树时，在每个分裂点，我们只允许树考虑可用特征的一个小的随机子集。这可以防止所有的树都锁定在一两个“超级预测性”的特征上。它迫使它们探索不同、更多样化的策略来解决问题。这降低了树之间的相关性，正如[集成方法](@entry_id:635588)的数学原理所示，[对相关](@entry_id:203353)性较低的专家进行平均会产生一个更好的最终预测 [@problem_id:5192617]。

### 当世界碰撞：真实世界预测的细微差别

虽然分类和回归之间的区别是根本性的，但现实世界常常给我们带来更复杂的场景，这些场景模糊了界限，并考验着我们的理解。

例如，如果我们把一个回归问题强行塞进一个分类的盒子里，会发生什么？假设我们不预测一个人的确切血压（一个连续的数字），而是创建三个类别：“低”、“正常”和“高”。这个过程，称为**[分箱](@entry_id:264748)**（binning）或离散化，似乎简化了问题。但天下没有免费的午餐。通过丢弃精确的数值信息，我们失去了一些东西。我们可以将其形式化：对数据进行[分箱](@entry_id:264748)后我们能做出的最佳预测，其[均方误差](@entry_id:175403)总是会高于原始回归问题的最佳预测。这种误差的增加是我们丢弃信息所付出的代价 [@problem_id:3170614]。在某些情况下，这种权衡可能是可以接受的，但关键是要明白我们做出了选择并付出了代价。

当我们必须同时解决多个问题时，复杂性会增加。一个现代的临床AI可能被要求使用单一的、统一的模型，根据相同的数据预测患者的死亡率（分类）、30天内再入院风险（分类）和住院天数（回归） [@problem_id:5214967]。这就是**[多任务学习](@entry_id:634517)**（multi-task learning）。一个新挑战立即出现：你如何平衡不同的记分卡？住院天数的平方误差（以天数的平方为单位）可能是一个数千的数字，而死亡率的交叉熵（一个基于概率的指标）则是一个介于0和比如说2之间的数字。如果我们只是将这些损失加在一起，回归任务的“声音”将比分类任务大数千倍。模型的学习过程将完全被试图减少住院天数误差所主导，实际上忽略了生死攸关的死亡率预测。这迫使我们开发更复杂的策略，比如仔细加权损失、标准化目标，甚至让模型根据其对每个预测的置信度自行学习如何平衡任务的优先级 [@problem_id:5214967]。

从一个在类别和数量之间的简单选择，我们看到了一个丰富而优美的结构出现。通过[损失函数](@entry_id:136784)定义目标，通过将简单、直观的模型组合成强大的集成，我们可以构建能够驾驭世界复杂性的机器，无论是发现新材料、支持临床决策，还是在增强现实显示中分割复杂的解剖结构 [@problem_id:5110421]。原理虽少，其应用却无穷。

