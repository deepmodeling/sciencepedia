## 引言
拥有数十亿参数的巨型神经网络是如何仅使用简单的梯度下降就如此高效地学习的？这个问题是现代科学中最深的谜团之一。训练的景观是难以想象的复杂，但我们却总能找到解决方案。为了解开这个谜团，我们求助于一种强大的理想化方法：无限宽[神经网络](@article_id:305336)。通过研究这个极端极限，我们可以剥离那些令人困惑的细节，揭示一个优美而简单的底层结构。

本文将通过两章来探索这个强大的理论视角。在“原理与机制”一章中，我们将揭示无限宽视角如何通过确保信号在深度架构中稳定传播，为网络初始化提供一个有原则的基础。然后，我们将见证“大简化”的发生，即整个训练过程被[线性化](@article_id:331373)，并由一个称为[神经正切核](@article_id:638783)（NTK）的恒定矩阵完美描述。在“应用与跨学科联系”一章中，我们将把理论与实践联系起来，展示 NTK 框架如何帮助我们设计更好的网络，阐明[良性过拟合](@article_id:640653)等现象，并与计算物理学、[量子计算](@article_id:303150)和经济学等不同领域建立令人惊讶的联系。通过这段旅程，我们将看到，无穷的幻想为现实世界提供了异常清晰的视角。

## 原理与机制

[深度学习](@article_id:302462)究竟是如何运作的？我们被告知要想象一个具有难以想象的复杂性的景观，一个拥有数百万甚至数十亿参数的函数，我们必须在其中找到一个代表着好解决方案的、微小的山谷。这个任务似乎就像在全世界的海滩上寻找一粒特定的沙子一样毫无希望。然而，不知何故，[梯度下降](@article_id:306363)这个简单的[算法](@article_id:331821)——仅仅是不断地向山下迈出一小步——总能找到它的路。这是一个深刻的谜题。

为了解开它，我们将借鉴物理学中的一个经典技巧：当面对一个极其复杂的系统时，研究它的一个理想化的、极端的版本。如果我们的[神经网络](@article_id:305336)不仅仅是宽，而是*无限*宽呢？这听起来可能像是一个数学幻想，但就像物理学家所用的无摩擦平面或点质量行星一样，这种理想化剥离了令人困惑的细节，揭示了一个惊人简单而优美的核心。

### 生存的艺术：[混沌边缘](@article_id:337019)的信号传播

在网络能够学习之前，它必须首先能以一种稳定的方式存在。想象一个信号——你的输入数据——进入第一层。这个信号被处理并传递到下一层，再下一层，以此类推。它的强度会发生什么变化？如果每一层都系统性地削弱信号，那么经过几层之后，它将消失得无影无踪。网络的深层将是“死的”，接收不到任何信息。相反，如果每一层都放大信号，它将爆炸成一堆无用的大数字。网络将变得饱和而混乱。

两者都不可取。一个健康的网络必须生存在“[混沌边缘](@article_id:337019)”，这是一个[临界状态](@article_id:321104)，信号的幅度在向前传播时平均得以保持。我们可以用一个简单的量来捕捉这一点，即**平均场灵敏度** $\chi$。它衡量一个信号的微小扰动的平方范数在通过一层时被乘的平均因子。如果 $\chi  1$，信号消失。如果 $\chi > 1$，它会爆炸。最理想的情况是 $\chi = 1$。

让我们看看这个简单的原则会引导我们走向何方。对于一个权重从方差为 $\sigma_w^2$ 的分布中抽取的网络，其灵敏度结果为 $\chi = \sigma_w^2 \mathbb{E}[(\phi'(z))^2]$，其中 $\phi$ 是激活函数，[期望值](@article_id:313620)是在[神经元](@article_id:324093)看到的典型预激活值 $z$ 上计算的 [@problem_id:3094645]。设定 $\chi=1$ 直接为我们提供了如何初始化网络的方案！

- 对于经典的**[双曲正切函数](@article_id:638603)**（$\tanh$）激活函数，其在原点的[导数](@article_id:318324)为1。$\chi=1$ 的条件立即得出 $\sigma_w^2 = 1$。这就是著名的 Glorot 或 Xavier 初始化，它是凭经验发现的，但在这里我们从稳定性的基本原则推导出来。

- 对于现代[深度学习](@article_id:302462)的主力军——**[修正线性单元](@article_id:641014)**（ReLU），情况稍微复杂一些。对于正输入，其[导数](@article_id:318324)为1，对于负输入，其[导数](@article_id:318324)为0。由于初始化时的输入对称分布在零点周围，[导数](@article_id:318324)的平方有一半时间是1，另一半时间是0。[期望值](@article_id:313620) $\mathbb{E}[(\phi'(z))^2]$ 就是 $\frac{1}{2}$。于是，$\chi=1$ 的条件要求 $\sigma_w^2 = 2$。这就是同样著名的 He 初始化，它对于使非常深的 ReLU 网络可训练至关重要 [@problem_id:3094645]。

这是我们从无限宽视角得到的第一个主要见解：那些看似随意的成功初始化方案，实际上是信息必须在其穿越网络深度的旅程中存活下来的物理需求的直接结果。

### 大简化：函数空间中的训练

好了，我们有了一个正确初始化的网络。现在我们来训练它。在这里，在无限宽极限下，真正的魔术发生了。在数十亿参数空间中那个噩梦般的、非凸的优化问题，转变成了一个惊人简单的东西。

让我们想象一下[损失函数](@article_id:638865)。作为参数 $\theta$ 的函数，$L(\theta)$ 是一个由山丘、山谷和[鞍点](@article_id:303016)构成的可怕景观。但如果我们换个角度思考呢？真正重要的是网络的*预测*，即它产生的输出 $f(x)$。当损失函数被看作是关于训练数据上预测向量 $f$ 的函数时，$L(f) = \frac{1}{2}\|f-y\|^2$，它只是一个简单的、完美的、凸形的碗 [@problem_id:3159053]。只有一个最小值：即预测 $f$ 与真实标签 $y$ 完全匹配的地方。

无限宽极限的奇迹在于，参数 $\theta$ 的混乱旅程共同作用，为函数 $f$ 创造了一条简单、笔直的下降路径，直达这个碗的底部。动态过程变得线性化。网络函数不再在参数的荒野中徘徊，而是可预测地朝着正确答案前进。

预测向量 $f(t)$ 随时间 $t$ 的演化由一个异常简洁的方程控制：
$$
\frac{d f(t)}{dt} = -K (f(t) - y)
$$
其中 $y$ 是真实标签的向量，而 $K$ 是一个在整个训练过程中保持*恒定*的矩阵 [@problem_id:3186533]。这个矩阵就是著名的**[神经正切核](@article_id:638783)（NTK）**。

这个核是什么？它是一个编码了整个训练动态的对象。条目 $K(x, x')$ 告诉我们，当我们试图改善网络在另一点 $x'$ 的输出时，它在点 $x$ 的输出会改变多少。更正式地说，它是输出相对于所有网络参数的梯度的[点积](@article_id:309438)，在随机初始化下取平均值 [@problem_id:3159095]。
$$
K(x, x') = \mathbb{E}_{\theta_0} \left[ \nabla_{\theta} f(x; \theta_0) \cdot \nabla_{\theta} f(x'; \theta_0) \right]
$$
对于一个简单的两层 ReLU 网络，这个[期望](@article_id:311378)可以被精确计算，从而得到一个核函数，其对于任意两个输入 $x$ 和 $x'$ 的值取决于它们的范数和它们之间的夹角 [@problem_id:3167854]。误差向量的分量呈指数衰减，衰减率由该核矩阵 $K$ 的[特征值](@article_id:315305)给出 [@problem_id:3186533]。训练变得等同于一种称为核回归的经典方法。

所以，谜题解开了！梯度下降之所以有效，是因为对于非常宽的网络，它解决的问题根本不是非凸的。它在另一个空间——[函数空间](@article_id:303911)——中是一个凸问题，而 NTK 就是引导下降的地图。如果这个核是行为良好的（具体来说，是正定的），那么收敛到零[训练误差](@article_id:639944)就得到了保证 [@problem_id:3159053] [@problem_id:3194218]。

### 简约的代价：惰性与特征学习的缺失

但是，这种优美的简约性要付出什么代价呢？核 $K$ 在初始化那一刻就被确定，然后在之后的所有时间里都保持不变。这意味着网络在某种意义上是“惰性”的。它在最开始就确定了一组特征——编码在核的结构中——并且从不改变它们。训练所做的全部工作就是学习这些固定特征的最佳线性组合来拟合数据。这里没有“特征学习”，即网络自行发现数据中逐渐更抽象、更强大的表示的过程 [@problem_id:3157550]。

把它想象成一位雕塑家。一个真实的、有限宽度的网络就像一个可以塑造黏土的雕塑家，他可以改变黏土的形式和质地来创作杰作。这就是特征学习。相比之下，无限宽网络则是在开始时被给予一套精心制作的、固定的凿子（NTK）。它能创作出美丽的雕塑，但只能通过线性组合这些特定凿子所能做出的切割来实现。它不能在中途发明一种新型的凿子。

即使加深网络也无法改变这个基本事实。一个更深的无限宽网络对应一个不同的、更复杂的初始核——一套更奇特的凿子——但那套凿子仍然是从一开始就固定的 [@problem_-id:3157550]。我们甚至可以分析像[残差网络](@article_id:641635)（[ResNet](@article_id:638916)s）这样的复杂架构，并发现它们在这个极限下同样由一个固定的核控制，我们可以精确推导出其形式 [@problem_id:3159113]。

### 弥合差距：有限网络的真实世界

那么，这一切都只是一个数学上的奇闻异事，与我们在实践中使用的有限网络无关吗？完全不是。无限宽理论提供了一个强大的基准——对现实的“零阶近似”。真实世界的网络是“惰性”的类核行为和“丰富”的特征学习行为的迷人混合体。

想象一下运行一个[计算机模拟](@article_id:306827)，比较一个真实的、有限宽度的网络和其理想化的 NTK 对应物，两者从完全相同的初始化开始 [@problem_id:3159054]。
- 对于一个窄网络，你会看到真实网络的预测迅速偏离 NTK 的预测。真实网络正在学习特征；其内部的核在演化。
- 对于一个非常宽的网络，你会发现这两条轨迹保持得非常接近。网络越宽，它就变得越“惰性”，固定的 NTK 近似就越有效。

这个视角也从新的角度阐明了经典的[梯度消失](@article_id:642027)/爆炸问题。我们看到，在无限极限下，梯度是完全稳定的。但在有限宽度 $n$ 和深度 $L$ 的情况下会发生什么？该理论可以被扩展以找到一个修正项。[梯度范数](@article_id:641821)在整个网络中变化的因子不再是精确的 1，而是更接近 $\exp(-L/2n)$ [@problem_id:3194529]。这个优雅的公式表明，稳定性是深度和宽度之间的一场拉锯战。对于其宽度而言过深的网络（$L \gg n$）将遭受[梯度消失](@article_id:642027)的困扰，正如我们在实践中看到的那样。但这可以通过加宽网络来抵消。

因此，无限宽网络理论描述的不仅仅是一个幻想。它为理解提供了一个阶梯的第一级。它解释了*为什么*深度网络根本就是可训练的，它为初始化策略提供了一个有原则的基础，并且它给了我们一个基准——[神经正切核](@article_id:638783)——我们可以用它来衡量并开始理解[深度学习](@article_id:302462)核心的那个真正非凡的现象：自动化地发现我们世界的[特征和](@article_id:368537)表示。

