## 引言
在机器学习领域，一些概念之所以强大，不仅在于它们的功能，更在于它们为我们提供的思维方式。[基于能量的模型](@article_id:640714) (EBM) 就是这样一个框架，它直接借鉴了统计物理学，为我们提供了一个优雅直观的视角来理解[概率分布](@article_id:306824)。其核心思想很简单：世界上的每一种状态都有一个与之相关的能量，而自然界偏爱低能量状态。然而，将这一优美的原则转化为可训练的模型时，会遇到一个重大的计算挑战——难解的配分函数——这在历史上限制了其应用。本文旨在通过解决这一问题来揭开 EBM 的神秘面纱，展示现代技术如何将其转变为一种通用而强大的工具。

本文的探讨分为两个主要部分。在“原理与机制”一章中，我们将剖析 EBM 背后的基本理论。您将了解到吉布斯-玻尔兹曼 (Gibbs-Boltzmann) 分布、使我们能够训练这些模型的[对比学习](@article_id:639980)中优雅的“推拉”动态，以及用于在[能量景观](@article_id:308140)中导航的 MCMC 采[样方法](@article_id:382060)。随后，“应用与跨学科联系”一章将展示 EBM 卓越的通用性，阐述其在创造性生成、[结构化预测](@article_id:639271)以及作为构建更稳健、公平和可信赖 AI 系统的基础等方面的应用。

## 原理与机制

任何科学模型的核心都有一个基本原则，这个思想如此简洁优美，一句话便可解释，但其力量又如此强大，其推论可以描述广泛的现象。对于[基于能量的模型](@article_id:640714) (EBM) 而言，这个原则是：**为世界上每一种可能的[状态分配](@article_id:351787)一个称为能量的标量值，能量较低的配置[比能量](@article_id:334705)较高的配置更可能出现。**

就是这样。这个思想直接借鉴自[统计物理学](@article_id:303380)，在统计物理学中，自然界在不懈追求稳定性的过程中，倾向于偏爱低能量状态。因此，EBM 就是一种试图为数据捕捉这一原则的模型。想象一个广阔的景观，景观中任何一点的海拔高度代表能量。我们在现实世界中观察到的数据——猫的图片、语言的句子、蛋白质结构——就像坐落在这片景观中深邃舒适山谷里的村庄。而那些不合理的事物——比如三头猫、胡言乱语的句子——则位于高耸贫瘠的山峰之上。

深度神经网络成为这片景观的雕塑家。给定一个输入 $x$，带有参数 $\theta$ 的网络计算其能量 $E_{\theta}(x)$。那么，观察到 $x$ 的概率由 **吉布斯-玻尔兹曼 (Gibbs-Boltzmann) 分布** 给出：

$$
p_{\theta}(x) = \frac{\exp(-E_{\theta}(x) / \tau)}{Z(\theta, \tau)}
$$

能量 $E_{\theta}(x)$ 是我们故事中的主角。参数 $\tau$ 是 **温度**，一个我们熟悉的概念，它控制着我们景观的“对比度”。低温 ($\tau \to 0$) 会使山谷变得极深，山峰变得极高；模型会变得非常“尖锐”，对其认为合理与不合理的事物非常有信心。高温 ($\tau \to \infty$) 会使整个景观变得平坦，让所有事物的可能性几乎相等。为简单起见，我们通常设置 $\tau=1$。

但这个故事中有一个反派，一个阻碍这个优雅思想实现的巨大障碍。它就是分母中的项：$Z(\theta, \tau)$，即**配分函数**。为了将我们未归一化的分数 $\exp(-E_{\theta}(x))$ 转化为总和为一的真实概率，我们必须用这些分数在*宇宙中所有可能的 $x$ 配置*上的总和去除：

$$
Z(\theta, \tau) = \int \exp(-E_{\theta}(x) / \tau) dx \quad \text{(for continuous data)}
$$

想象一下，你想计算在房间里找到某个特定的人的概率。要使用这个公式，你首先需要数遍地球上的每一个人。这就是配分函数的挑战所在。对于任何像图像这样有趣的[高维数据](@article_id:299322)，所有可能的 $x$ 的空间都大得惊人，使得直接计算 $Z(\theta)$ 变得完全不可能。这一事实是 EBM 的核心计算问题，而克服它正是其天才之处。

### 学习之舞：推与拉

如果我们无法计算配分函数，我们又如何能训练模型呢？雕塑家如何知道在哪里雕刻山谷，在哪里抬高山峰？答案不在于计算景观的总量，而在于观察它*应该如何变化*。

当我们使用最大似然法训练模型时，我们希望调整参数 $\theta$ 以增加我们实际观察到的数据的概率 $p_{\theta}(x)$。[对数似然](@article_id:337478)的梯度——即我们应该调整 $\theta$ 的方向——最终呈现出一种非常优美和直观的形式：

$$
\nabla_{\theta} \log p_{\theta}(x) = -\nabla_{\theta} E_{\theta}(x) - \nabla_{\theta} \log Z(\theta)
$$

这可能看起来很复杂，但通过一点小技巧（即[对数导数](@article_id:348468)技巧），第二项可以被重写，从而得到这个最终的杰作：

$$
\nabla_{\theta} \log p_{\theta}(x) = \underbrace{-\nabla_{\theta} E_{\theta}(x)}_{\text{Positive Phase}} - \underbrace{\mathbb{E}_{x' \sim p_{\theta}(x')} [-\nabla_{\theta} E_{\theta}(x')]}_{\text{Negative Phase}}
$$

这个方程描述了两种相反力量之间的一场精彩的“舞蹈”。

**正相 (Positive Phase)：** 这一项来自我们数据集中的一个真实数据点 $x$。它告诉雕塑家：“我在现实世界中看到了这个点 $x$。在这里挖掘！让它的能量降低。” 这是一种在真实数据所在位置*向下*推动[能量景观](@article_id:308140)的力量。如果我们只有这一项，我们只会将整个景观沉入无底深渊。

**负相 (Negative Phase)：** 这一项提供了关键的[反作用](@article_id:382533)力。它说：“等等！在挖掘之前，先看看你*目前想象中*的世界。” 我们从模型当前的分布 $p_{\theta}(x')$ 中生成“幻想”或“负”样本 $x'$。对于这些自生成的点，梯度告诉我们做相反的事情：“这个点 $x'$ 是我的模型目前认为合理的。把它的能量*推高*！” 这种力量提高了模型认为可能的点的能量，从而防止了景观的坍塌。

学习过程是一个微妙的平衡。我们降低真实数据的能量（**正相**），同时提高模型“幻想”数据的能量（**负相**）。其净效应是能量从真实数据区域被抽走，并堆积到其他所有地方。随着时间的推移，我们景观的低能量山谷会与数据实际存在的区域完美对齐。奇妙之处在于，为了计算这个梯度，我们不需要难解的配分函数 $Z(\theta)$ 本身，但我们*确实*需要一种方法来生成那些幻想样本。

### 漫游的艺术：从景观中采样

那么，我们如何获得这些幻想样本呢？我们需要从 $p_{\theta}(x)$ 中采样，而这是一个我们不知道其[归一化常数](@article_id:323851)的分布！这似乎是一个循环问题。解决方案是以一种巧妙的方式在[能量景观](@article_id:308140)中“漫游”，以保证我们大部分时间都停留在低能量的山谷中，正如分布所指示的那样。这就是**[马尔可夫链](@article_id:311246)蒙特卡洛 (MCMC)** 方法的工作。

最直观的 MCMC 方法之一是**[朗之万动力学](@article_id:302745) (Langevin Dynamics)**。想象一个蒙着眼睛的徒步者在[能量景观](@article_id:308140)上行走。为了找到山谷，他们可以做两件事：
1.  用脚感受他们所站之处的地面的坡度。这个坡度就是“分数”函数，即 $-\nabla_{x} E_{\theta}(x)$。他们朝着下坡方向迈出一小步。
2.  偶尔，进行一次小的随机跳跃。这可以防止他们陷入一个小的、无趣的局部凹陷中，并帮助他们探索更广阔的景观。

我们采样器的更新规则正是如此：
$$
x_{k+1} = x_k - \eta \nabla_x E_{\theta}(x_k) + \sqrt{2 \eta \tau} \xi_k
$$
在这里，$\eta$ 是步长，$\xi_k$ 是从高斯分布中抽取的随机跳跃。经过多步之后，点集 $\{x_k\}$ 构成了我们模型分布 $p_{\theta}(x)$ 的一个[代表性样本](@article_id:380396)。这些就是我们在学习之舞中需要的负样本。

然而，这个徒步者可能会遇到麻烦。如果景观的形状不正确，徒步者可能会“跑偏”到无穷远处，导致采样器崩溃。或者，如果景观有深邃而孤立的山谷，徒步者可能会被困在其中一个山谷而永远无法探索其他山谷，从而对世界产生有偏见的看法。这被称为**链停滞 (chain stagnation)**。EBM 的实际成功取决于设计能够避免这些陷阱的能量函数和采样器。

### 良好景观的设计蓝图

MCMC 采样的挑战告诉我们，并非任何能量函数都可以。计算 $E_{\theta}(x)$ 的[神经网络架构](@article_id:641816)不是任意的；它必须遵循某些原则。

首先，为了防止我们的 MCMC 徒步者跑到无穷远处，能量景观必须在边缘向上弯曲。形式上，能量必须是**强制的 (coercive)**，即当 $\|x\| \to \infty$ 时，$E_{\theta}(x) \to \infty$。如果能量函数在远离数据的地方变平或向下延伸，采样器会乐呵呵地走向远方，我们的训练就会失败。我们可以通过在能量函数中添加一个简单的[正则化](@article_id:300216)项来强制实现这一点，例如，一个随输入范数线性增长的项，以确保有一堵“墙”来限制采样器。

其次，景观不应过于平坦。考虑一个使用饱和函数（如 $\tanh$）的能量函数。在远离原点的地方，$\tanh$ 函数变得平坦，其[导数](@article_id:318324)趋于零。这意味着分数 $-\nabla_x E_{\theta}(x)$ 也趋于零。我们的徒步者会发现自己身处一个完全平坦的平原上，不知道该往哪个方向走才能下山。这可能在远离数据的地方产生“虚假”的低能量区域，而模型没有梯度信号来将其能量推高，这是[梯度消失问题](@article_id:304528)的一种微妙形式。

### 统一的视角：EBM 无处不在

也许基于能量的视角最美妙之处在于其普适性。一旦你戴上了 EBM 的眼镜，你就会开始在机器学习的各个角落看到它们的身影。

一个使用 softmax 函数的标准多类**分类器**可以被完美地描述为一个条件 EBM。该模型为给定输入 $x$ 的每个类别 $y$ 学习一个能量 $E(x, y)$。一个类别的概率由 $p(y|x) \propto \exp(-E(x,y))$ 给出。我们用来训练这些分类器的[交叉熵损失](@article_id:301965)函数，其作用恰好就是正/负相之舞：它降低真实标签的能量，而分母（一个关于类别的局部配分函数）则隐式地推高所有其他标签的能量。或者，我们可以在数据和标签上学习一个联合能量 $E(x,y)$，并使用贝叶斯规则来找到标签的后验概率 $p(y|x)$。

即使是现代的**[对比学习](@article_id:639980)**方法，如驱动 CLIP 和 SimCLR 等模型的那些方法，也是 EBM 的一种形式。在这个框架中，两个输入之间的“相似性”被视为[负能量](@article_id:321946)。训练目标是为相关的“正样本对”降低能量（最大化相似性），同时为许多不相关的“负样本对”推高能量（最小化相似性）。这是对同样推拉动态的一种简化但强大的应用。

最后，EBM 框架阐明了**生成式建模**中的基本权衡。与像[归一化流](@article_id:336269) (Normalizing Flows, NFs) 这样的模型相比，EBM 的**表达能力**极强；它们可以学习表示几乎任何可以想象的能量景观。然而，这是以**昂贵的采样**为代价的，因为它们需要一个迭代的 MCMC 过程。相比之下，NFs 具有非常快速和精确的采样，但在它们可以表示的分布方面受到架构上的限制。

通过能量的视角审视这些多样化的模型，我们看到的不是一堆零散的技巧，而是一套统一的原则，这些原则植根于能量景观的优雅直观的物理学之中。

