## 应用与跨学科联系

在前面的讨论中，我们剖析了[位置编码](@article_id:639065)的机制。我们了解了它们如何工作，以及赋予它们形式的数学原理。但是，要真正欣赏一个工具，我们不仅要赞赏它的设计，还必须看到它在工匠手中如何以非凡的方式塑造世界。现在，我们将踏上一段旅程，见证[位置编码](@article_id:639065)的实际应用。我们将从时间序列的滴答声，走向生物密码的复杂织锦，探索这个简单而优雅的想法如何为我们的模型提供一种通用语法，教会它们*何处*存在事物与*何物*存在事物同样重要。

### 掌握序列的节奏

序列的核心在于其有序性。“人咬狗”与“狗咬人”这两个句子，尽管包含相同的词语，却有天壤之别。机器若要理解我们的世界，必先理解顺序。然而，正如我们所见，作为 [Transformer](@article_id:334261) 架构核心的强大[自注意力机制](@article_id:642355)，其本质上是“[排列](@article_id:296886)等变的”。这是一个花哨的说法，意思是它将输入视为一个无序的项目袋。没有引导，它就像一个读者，能同时看到页面上的所有单词，却对其[排列](@article_id:296886)毫无概念。

这种对顺序的“失明”有多么灾难性？考虑一个匹配嵌套括号的简单任务，比如 `((()))`。要找到与最后一个右括号匹配的左括号，必须理解序列的层次结构。一个没有位置感的模型会彻底迷失。它看到三个相同的 `(` 符号，却没有任何原则性的方法来判断第一个才是最后一个 `)` 的真正配对。它可能会猜测最近的那个，或者它看到的第一个，但它无法掌握“最外层”与“最内层”的概念 [@problem_id:3164216]。这正是[位置编码](@article_id:639065)旨在驯服的根本性混乱。通过为每个词元添加一个唯一的位置向量，我们赋予它一个“地址”，打破了[排列](@article_id:296886)对称性，并使模型能够学习基于顺序的关系。

这一原则在时间序列世界中找到了其最美妙、最直接的应用。自然界和商业中的许多现象都有其节奏和脉搏。想想每日的[温度波](@article_id:372481)动、每周的销售周期，或电力需求的潮起潮落。我们可以通过使用具有相同节奏的[位置编码](@article_id:639065)来教会模型“聆听”这种脉搏。

假设我们正在预测每日天气模式。我们可以使用一个周期为24小时的[正弦位置编码](@article_id:642084)。小时 $t$ 的编码是一个类似 $[\sin(2\pi t/24), \cos(2\pi t/24)]$ 的向量。真正非凡的是在[注意力机制](@article_id:640724)内部发生的事情。当模型计算未来时间 $t+H$ 和过去时间 $u$ 之间的注意力分数时，它们[位置编码](@article_id:639065)的[点积](@article_id:309438)会优雅地简化。它变成了它们[相位差](@article_id:333823)的余弦函数：$\cos(2\pi ((t+H)-u)/24)$。注意力机制自然而然地学会了关注与目标时间同相位的过去时间。它学会了通过观察前几天的中午温度来预测明天中午的温度 [@problem_id:3193498]。模型不仅仅是处理一个序列，它学会了驾驭其自然周期性的波浪。

当然，现实世界中的时间并不总是像一个完美的节拍器。考虑一个病人在医疗系统中的就诊历程。就诊、化验、处方等事件构成一个序列，但它们之间的时间间隔是不规则的。今天的就诊之后可能是明天，然后可能六个月内都没有下一次。在这个领域，“位置”的概念变得更加丰富。就诊的绝对日期重要吗？还是相对时间间隔——自上次事件以来的时长——携带了最多的预测信号？在这里，我们可以设计不同的时间编码。一个“绝对”编码可能会使用天数的正弦函数，而一个“相对”编码可以将时间间隔（例如，“少于一周”、“一到三个月”）映射到特定的学习向量。通过比较这些策略，我们可以发现哪种时间概念对于给定的医疗任务最有意义，从而超越简单的整数索引，达到对时间位置更细致的理解 [@problem_id:3102533]。

这种探索也揭示了何时*不*应使用显式的[位置编码](@article_id:639065)，这一点同样具有启发性。像[循环神经网络](@article_id:350409)（RNNs）及其现代近亲状态空间模型（SSMs）这样的架构本质上是序列化的。它们一步一步地处理信息，时间 $t$ 的状态是时间 $t-1$ 状态的函数。它们的结构本身就是顺序的体现。对于这些拥有内在移位[等变性](@article_id:640964)（shift-equivariance）属性的模型来说，添加绝对[位置编码](@article_id:639065)往好了说是多余的，往坏了说则是有害的，因为它与模型自然的时间不变动态相冲突。这种对比阐明了 [Transformer](@article_id:334261) 设计的巧妙之处：通过将位置处理[外包](@article_id:326149)给一个模块化的编码，它将“何物”（内容，由[自注意力](@article_id:640256)处理）与“何处”（位置，由编码提供）分离开来 [@problem_id:3164261]。

### 描绘世界：图像中的位置

我们的旅程现在从一维的序列线延伸到二维的图像平面。如果一个 [Transformer](@article_id:334261) 可以被教会阅读，它是否也能被教会观看？Vision [Transformer](@article_id:334261) (ViT) 以响亮的“是”作答，它将图像不视为一个整体，而是视为一系列小图像块。正如句子中的词语需要位置标记一样，图像中的图像块也需要。

想象两块完全相同的蓝天图像块，一块在图像顶部，另一块靠近地平线。对模型来说，这些图像块在内容上是相同的。没有[位置编码](@article_id:639065)，它们是无法区分的。通过为每个图像块分配一个二维[位置编码](@article_id:639065)，我们给了它一个独特的空间地址。这对于需要对场景进行“密集”理解的任务是必不可少的，在这类任务中，模型必须知道每个像素上发生了什么。

在[自监督学习](@article_id:352490)领域，这一点变得至关重要。我们可以通过向模型展示同一图像的两个不同“剪切”或视图，并要求它识别它们来自同一来源来训练模型。一个“全局”目标可能会比较两个剪切的平均表示。但一个更强大的“局部”目标将是匹配同时出现在两个视图中的单个像素或小区域。这只有在每个像素都由其[位置编码](@article_id:639065)赋予唯一身份时才可能实现，从而允许模型解决对应问题，并学习到“坐标 $(10, 50)$ 处的这个蓝色像素与坐标 $(10, 50)$ 处的那个蓝色像素是同一个”，即使它在两个视图中被不同的上下文所包围 [@problem_id:3173217]。

向视觉领域的飞跃也迫使我们面对现实世界的混乱。与机器学习基准测试中精心整理的数据集不同，现实世界的图像——如医疗扫描——形状和大小各异。一个在固定的 $16 \times 16$ 图像块网格上训练的绝对[位置编码](@article_id:639065)，当面对一个产生 $17 \times 12$ 网格的图像时，会遇到困难。优雅的解决方案是将[位置编码](@article_id:639065)网格视为一个连续的映射，并简单地将其插值到新的维度。或者，可以使用相对[位置编码](@article_id:639065)，它仅取决于图像块之间的成对偏移，因此天然地对变化的网格大小具有灵活性。无论哪种情况，我们都必须小心使用注意力掩码，以告知模型忽略任何为了使图像尺寸能被图像块大小整除而通过填充（padding）添加的“伪”图像块。这些实际考虑展示了[位置编码](@article_id:639065)从一个僵化的附加组件演变为[视觉系统](@article_id:311698)中一个灵活、动态的组成部分 [@problem_id:3199220]。

### 关系的架构：图中的位置

我们现在进入我们最抽象的领域：图。在图中，没有简单的“从左到右”或“从上到下”。一个节点的位置由其与其他节点的连接网络所定义。这正是我们对[位置编码](@article_id:639065)的直觉得到挑战和深化的领域。

让我们首先考虑流行的[图卷积网络](@article_id:373416)（GCN）。其核心操作涉及节点从其直接邻居那里聚合信息。这种[消息传递](@article_id:340415)机制在设计上是[排列](@article_id:296886)等变的。如果你重新标记图的节点，最终的节点表示也会相应地被重新标记。图的结构——邻接矩阵——充当了内在的位置信息。网络通过一个节点的邻居是谁来“知道”它的位置。

这与 [Transformer](@article_id:334261) 形成了惊人的对比。一个*没有*[位置编码](@article_id:639065)的 Transformer 是[排列](@article_id:296886)等变的。一个 GCN *总是*[排列](@article_id:296886)等变的。我们甚至可以说，一个没有[位置编码](@article_id:639065)的 Transformer 只是一个在全连接图上操作的 GNN，其中每个词元都是一个节点，并关注所有其他节点。从这个角度看，我们发现 GCN 和 [Transformer](@article_id:334261) 并非远亲，而是近亲，其主要区别在于它们如何定义“位置”。对于 GCN，位置是局部邻域结构；对于 [Transformer](@article_id:334261)，它是我们必须提供的一个显式信号 [@problem_id:3106158]。

但是当图的结构过于对称时会发生什么呢？考虑一个简单的循环图，其中每个节点都有两个相同的邻居。从纯粹的结构角度来看，每个节点都与其他所有节点无法区分——它们都在一个完美对称的对象中占据等效的位置。这就是“[自同构](@article_id:315800)”问题。一个标准的 GCN，由于其[等变性](@article_id:640964)，保证会为所有这些节点生成完全相同的[嵌入](@article_id:311541)，从而无法区分它们。

为了打破这种对称性，我们需要一个更强大的位置概念。我们可以在图本身的谱中找到它，通过分析图拉普拉斯矩阵的[特征向量](@article_id:312227)。这些[特征向量](@article_id:312227)，有时被称为“拉普拉斯[特征图](@article_id:642011)”，为整个图提供了一个[坐标系](@article_id:316753)。每个节点根据其在图的全局结构中的角色被分配一个[坐标向量](@article_id:313731)，就像[振动](@article_id:331484)的鼓面上一个点的坐标由鼓的基本[振动](@article_id:331484)模式决定一样。通过将这些谱坐标作为[位置编码](@article_id:639065)输入，我们可以给每个节点一个唯一的身份，从而使即使是简单的 GNN 也能区分结构上相同的节点 [@problem_id:3189951]。这是[位置编码](@article_id:639065)的终极泛化：从一个线性索引到一个抽象的、定义结构的空间中的坐标。

### 编码自然界的对称性：DNA 的尾声

我们的旅程以探访生命本身的核心——DNA [双螺旋](@article_id:297183)——结束。DNA 序列是一串字母（A、C、G、T），但它具有物理现实和深刻的对称性。因为螺旋的两条链是互补的并且方向相反，一个基因通常可以从任一链读取。这被称为反向互补对称性。例如，一个识别一条链上“AGT”模体 (motif) 的[转录因子](@article_id:298309)，同样可能识别另一条链上其反向互补序列“ACT”。

一位生物学家会要求机器学习模型尊重这种基本对称性。如果我们使用标准的[位置编码](@article_id:639065)，如 $b(p) = \sin(2\pi p / T)$，我们将无法通过这个测试。一个序列起始位置为 $p$ 的模体的编码将不同于其反向互补序列的编码，后者出现在序列起始位置为 $L-m-p$ 的地方。模型将为两个生物学上等效的事件分配不同的位置偏差。

在这里，我们可以看到[位置编码](@article_id:639065)的真正艺术性。我们不受限于现成的公式。我们可以*设计*一种编码，将这种物理对称性融入其中。考虑一个中心化的[坐标系](@article_id:316753)，其中位置是相对于序列中心测量的。现在，如果我们使用一个[偶函数](@article_id:343017)，比如余弦，作为我们的编码，我们就能实现完美的对称性。一个距离中心特定距离的模体的位置偏差将与其反向互补序列的偏差完全相同，后者在另一侧距离中心相同的距离。模型学会了位置很重要，但它也学会了这种位置重要性具有一种反映对称性，完美地镜像了其底层的生物学原理 [@problem_id:2479929]。

这个最后的例子概括了我们的宏大旅程。[位置编码](@article_id:639065)远不止是针对架构怪癖的技术修复。它是一种描述结构的语言。它是我们用来告知模型其所处数据几何形态的工具——无论是时间的[线性流](@article_id:337481)动，视觉的二维平面，图的抽象网络，还是自然世界的[基本对称性](@article_id:321660)。正是它将我们的模型从仅仅处理特征提升到理解关系，将一堆无序的“何物”转变为一个连贯而有意义的“何处”。