## 应用与跨学科联系

在我们之前的讨论中，我们探讨了职业行为的基本原则——忠诚和关照的责任，以及将专业人士与他们所服务的公众紧密联系在一起的庄严准则。人们可能倾向于将这些原则视为高尚、抽象的理想，一套为了应付考试而记忆，然后束之高阁的正式规则。但这与事实相去甚远。这些原则并非装饰品；它们是用于驾驭复杂、混乱、高风险的职业现实生活的工程规范。当通往未来的道路绝非清晰时，它们是我们用来做出痛苦决定的工具。

现在，我们将踏上一段旅程，亲眼见证这些原则的实际应用。我们将从床边走到会议室，从医生与患者之间的轻声交谈，到庞大的人工智能系统的设计。在每一种情境中，我们都将看到职业礼仪的抽象框架如何提供一个强大而实用的指南，揭示出解决我们这个时代一些最具挑战性问题的方案中所蕴含的惊人统一与美感。

### 神圣的信任：作为伦理行为的沟通

在任何服务于人的职业核心，都存在着一份神圣的信任，而承载这份信任的容器便是沟通。但我们所说的并非仅仅是礼貌。我们谈论的是在专家与他们所服务的人之间建立一座理解之桥的深远伦理责任。

思考一下获得“知情同意”这个简单的行为。在繁忙的医院里，它可能感觉像一个官僚仪式——一叠文件和在虚线上签个名。但如果患者说的不是同一种语言呢？如果他们的生活经历使他们的健康素养有限呢？突然之间，专业人士的关照责任所要求的远不止一本小册子和一个令人安心的点头。真正的同意需要真正的理解。职业准则要求临床医生成为一名细致的理解构建者，聘请合格的医学翻译，并至关重要地使用像“回授”法这样的方法——即请患者用自己的话来解释情况。只有当患者能够阐述风险和益处时，理解之桥才算真正建成，也只有那时，同意才是有效的，职业责任才算履行 [@problem_id:4880718]。

理解的障碍并非总是语言上的。它们可以是心理上的，由恐惧、个人经历和无处不在的错误信息所构建。想象一位患者因为听了邻居的故事而害怕一种能救命的药物 [@problem_id:4421596]。一种刻板、权威主义的方法——“指南说你必须服用这个”——既不尊重患者的自主权，也可能失败。在这里，专业人士必须成为一位熟练的教育者和去偏见执行者。他们必须将统计学的抽象语言翻译成人类的语言。他们不应引用听起来很大但常常具有误导性的 $25\%$ 的“相对风险降低”，而必须使用绝对术语和自然频率：“对于每100个像您这样的人，服用这种药十年，可以预防大约两到三例心脏病发作或中风。”他们必须倾听患者的恐惧，用现有最佳证据来回应它们，并进行真正的共同决策。这不是一种“软技能”；这是认知心理学和伦理原则的复杂应用，对于维护患者的福祉和自主权至关重要。

### 房间里的新机器：人工智能时代的职业精神

在这个微妙的人类生态系统中，我们引入了一个新的参与者：人工智能。人工智能决策支持系统承诺增强人类的专业知识，发现我们可能错过的模式，并引导我们走向更好的结果。但这个强大的新工具也带来了一系列新的、深刻的职业挑战。我们职业的“礼仪”现在必须扩展到包括我们如何与这些新的非人类同事互动、管理和信任它们。

一个供应商可能会向一家医院提供一个功能强大的“黑箱”人工智能用于分诊患者，它吹嘘其整体准确率很高，但拒绝透露其工作原理。专业人士应该使用它吗？关照责任与我们可称之为*认知责任*的东西是相辅相成的——即有责任将自己的决定建立在合理、可辩护的知识之上。专业人士不能将他们的判断力委托给一个他们不了解的机器。这样做就等于放弃了他们作为批判性思考者的角色。职业准则迫使我们要求一定程度的可解释性，以便进行临床审计，这样我们就能理解*为什么*这个工具会做出某个建议，并为采纳该建议负责 [@problem_id:4880677]。

此外，“首先，不伤害”的不伤害责任要求我们抱有更深的怀疑态度。一个平均准确率为 $90\%$ 的人工智能模型，对于特定人群可能错得非常危险。众所周知，基于历史数据训练的人工智能会继承并放大社会偏见，这是一个常见的失败模式，会导致本已处境不利的人群获得系统性更差的结果。专业人士不能满足于供应商的总体统计数据。他们的责任要求他们提出尖锐的问题：这个工具对女性的表现如何？对不同种族的患者表现如何？对那些有复杂合并症的患者呢？强制进行严格的偏见评估并非学术活动；它是履行预防可预见伤害责任的直接体现 [@problem_id:4880677] [@problem_id:4421660]。

这引出了一个引人入胜且违反直觉的发现。假设我们必须在两个人工智能模型之间做出选择，以执行像启动败血症治疗这样的关键任务。模型 $M_1$ 是一个黑箱，原始准确率为 $94\%$。模型 $M_2$ 完全可解释，但原始准确率只有 $90\%$。哪一个是正确的选择？幼稚的答案是选择更“准确”的那个。但现实更为微妙。模型 $M_1$ 的不透明性可能导致“自动化偏见”，即临床医生盲目遵循其建议，即使他们自己的直觉发出了警报。然而，模型 $M_2$ 的透明性赋予了临床医生权力。他们可以看到它的推理，理解其局限性，并在必要时智能地否决它。

当我们对这整个*社会技术系统*——即人加机器——进行建模时，我们可能会发现，使用“不太准确”但更可解释的模型的团队实际上产生的灾难性错误更少，并实现了更低的总体患者伤害 [@problem_id:4421794]。这个优美的结果教给我们一个深刻的教训：在高风险的专业工作中，目标不是用一台完美的机器来取代人类的判断，而是创造能使人类判断*更好*的工具。透明度和[可解释性](@entry_id:637759)不仅仅是理想的特性；它们是人机有效和合乎伦理合作的必要条件。

当然，我们必须对我们的新伙伴——人工智能，向患者坦诚相告。忠诚责任要求我们透明地解释人工智能做什么，它擅长什么，以及——最关键的——它的失败模式是什么，比如漏诊的微小但真实的可能性。这是21世纪知情同意的新标准 [@problem_id:4421687]。

### 系统性尺度：从个体行为到公正的制度

职业责任并非存在于真空中。它们在复杂的系统——医院、公司、公共卫生机构——中被履行，而这些系统有其自身的规则和激励机制。因此，一个真正有道德的专业人士不仅必须关心自己的行为，还必须关心他们工作于其中的系统的公正性和完整性。

考虑驱动一个卫生系统的激励措施。如果一家医院的人工智能及其临床医生都被激励去最大化像相对价值单位（RVUs）这样的收入代理指标，他们的行为将不可避免地，尽管是微妙地，从对患者最有利的方向转向最有利可图的方向。这是古德哈特定律的一个例子：“当一个度量标准成为一个目标时，它就不再是一个好的度量标准。”职业的忠诚责任要求我们与之斗争。最合乎伦理的组织是那些勇敢地重新设计其核心激励机制，使其与患者福祉对齐的组织，它们以质量调整生命年和可预防的不良事件等指标为目标，同时将收入视为预算约束，而非目标本身。这不仅仅是政策上的改变；它是一个受托承诺的制度性体现 [@problem_id:4421776]。在一个“学习型健康系统”中，挑战变得更加尖锐，因为人工智能在不断更新。在这里，治理必须包括主动的安全监控，以确保追求“更智能”的算法不会无意中伤害它本应服务的患者 [@problem_id:4421548]。

这些系统性责任在危机中受到的考验最为严峻。当一场大流行病来袭，没有足够的呼吸机给每个需要的人时，我们基于什么来选择？先到先得是武断的。纯粹的抽签忽略了医疗现实。最大化“挽救的生命年”可能歧视老年人和残疾人。在这里，职业准则引导我们走向一个植根于透明和公正的框架。首要标准必须是医疗上的：受益的可能性。但一个公正的系统也承认，一些患者预后较差，正是因为他们一生都面临着结构性的劣势。因此，一个真正公平的分诊政策可能会包括对这种不平等进行适度调整，并使用抽签作为那些存活机会相似者之间的决胜方法。正是在这些不可能的选择时刻，一个清晰、预先承诺的伦理框架才至关重要；它是在秩序与混乱、公正与歧视之间的一道屏障 [@problem_id:4880748]。

即使是在公共卫生紧急情况下服务的责任也不是绝对的。职业精神是一种契约，而非自杀协议。如果临床医生有一种状况使他们面临不成比例的个人风险，他们可能有权拒绝高暴露度的任务。然而，他们的责任并不止于此。他们有相应的义务，不是放弃自己的角色，而是寻求替代的贡献方式，例如通过远程医疗或后勤支持。作为回报，机构有互惠的责任来保护其员工并做出合理的安排。这种责任的平衡反映了一种成熟的理解，即专业人士不是机器中可互换的齿轮，而是与社会签订了相互尊重的契约的人类 [@problem_id:4880728]。

最后，这些宏大的原则甚至在最看似平凡的决定中也产生共鸣。一个得了[流感](@entry_id:190386)的员工必须决定：带病上班（出勤主义）还是待在家里（缺勤）？这个选择可以被分析为正式的“病人角色”（赋予免除职责的权利）与强大的、不成文的工作场所规范（可能惩罚缺勤）之间的协商。使用一个简单的预期效用模型，我们可以权衡成本和收益——带病工作的不适、因缺勤而受斥责的风险、失去的生产力、较慢的恢复——并看到一个理性的人如何驾驭这种冲突。这揭示了即使是这个日常的困境也是一个更大主题的缩影：我们的个人状态、我们的正式权利以及我们所处的社会和经济系统之间持续、复杂的相互作用 [@problem_id:4755767]。

从一次亲密交谈到整个卫生系统的架构，我们看到了相同的核心原则在起作用：诚实、胜任、忠诚和公正。它们不仅仅是礼仪规则。它们是使我们能够在一个日益复杂的世界里以正直和人性的方式行事的智识和道德工具。这就是它们的目的、它们的力量和它们固有的美感。