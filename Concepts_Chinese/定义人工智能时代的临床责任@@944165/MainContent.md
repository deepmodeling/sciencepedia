## 引言
人工智能融入临床实践有望彻底改变医疗保健，提供前所未有的诊断和治疗见解。然而，这一进步带来了一个关键挑战：当人工智能辅助的决策导致患者受到伤害时，谁应负责？这种模糊性造成了潜在的“责任缺口”，临床医生、医院和人工智能开发者都可能推卸责任，使患者处于弱势地位。本文旨在填补这一关键知识空白，提供一个全面的框架，以理解和分配人工智能时代的临床责任。我们将首先深入探讨核心的“原则与机制”，剖析道德、专业和法律责任的不同层次，以及连接行动与结果的因果链。在这一理论基础之上，“应用与跨学科联系”部分将探讨现实世界中的场景，展示这些原则如何应用于临床医生、机构和技术本身之间的复杂互动。通过探索这一新领域，我们不仅可以构建智能的系统，还可以构建负责任的系统。

## 原则与机制

当一辆自动驾驶汽车发生碰撞时，我们会立即产生一个直观的问题：谁的错？是那个没有集中注意力的司机吗？是编写软件的制造商吗？还是那个未能考虑到路上奇怪反光的公司？临床人工智能的世界迫使我们提出同样的问题，但赌注是人类的健康和生命。要驾驭这个复杂的新领域，我们不能依赖简单的指责。我们需要对责任有更精深的理解，一个建立在伦理、法律和技术基本原则之上的框架。这趟旅程不是进入法律的迷宫，而是深入探究人类判断的本质以及我们与自己创造的强大工具之间的关系。

### 责任的织锦

让我们设想一个场景，一个能让问题变得清晰的思想实验。一位皮肤科医生使用人工智能工具评估患者的皮肤病变。该人工智能已获得监管机构批准，它提示病变是良性的。皮肤科医生表示同意，并将患者送回家。几个月后，人们发现那是一个危险的黑色素瘤，并且已经恶化 [@problem_id:5014121]。谁应负责？

首先要认识到，“责任”不是单一的事物。它至少是由三股不同的线索编织而成的织锦：**道德责任**、**专业问责**和**法律责任**。

**道德责任**是最根本的。这是我们对彼此的义务，源于“不伤害”（**nonmaleficence**）和“为患者利益行事”（**beneficence**）等基本伦理原则。正是这种声音告诉临床医生，他们有责任运用自己的专业判断，而不是不加批判地依赖机器，尤其是在患者生命可能受到威胁时。

**专业问责**是一个更正式的概念。它指的是专业人士对其同行、所在机构及其专业所承担的一系列义务，由注意标准、机构政策和行为准则所界定 [@problem_id:4421586]。在我们这个黑色素瘤的案例中，对于医院而言，这可能意味着要有适当的程序来审查、实施和监控新的人工智能工具。对于皮肤科医生而言，这意味着要维持一个合理的审慎皮肤科医生所应达到的执业标准。

最后，**法律责任**是法律和法院系统介入时发生的事情。它关乎确定谁对损害负有法律过错，以及通常由谁支付赔偿金。这可能表现为医生的**临床疏忽**、医院的**企业疏忽**或制造该工具的供应商的**产品责任**。

至关重要的是，这些责任并非相互排斥。在我们的假设案例中，医生、医院*和*供应商三方完全有可能在某种程度上共同承担部分责任，各自以其独特的方式，并依据上述每一种定义来承担 [@problem_id:5014121]。责任不是一块被分割后就变小的蛋糕；它是一种可以同时附加于多个行为者的特质。

### 错误的剖析：追溯原因

要理清谁应负责，我们必须首先化身为侦探，追溯因果链。想象另一个场景：一个人工智能系统为患者推荐了一种血液稀释剂——华法林。在同一屏幕的鲜红色横幅中显示了一条警告：该患者患有颅内动脉瘤，这种情况使得使用该药物极其危险。医生因感到匆忙，点击了忽略提示并仍然开出了该药。患者随后中风 [@problem_id:4429757]。

法律和伦理分析为我们思考这个问题提供了两个绝佳的工具：**“若无”因果关系**和**[近因](@entry_id:149158)**。

**“若无”因果关系**，或称事实原因，是一个简单而有力的反事实检验。我们问：“若无”行为者的行为，损害会发生吗？在我们的[华法林](@entry_id:276724)案例中，人工智能的建议和医生的处方都是“若无”原因。如果人工智能没有提出建议，医生可能会走上另一条路。而如果医生没有签署处方，患者就不会得到这种药物。两个行为都是导致损害的链条中必不可少的环节。

但这产生了一个问题。“若无”链可以无限回溯！我们可以责怪编写人工智能程序的人，责怪雇佣程序员的人，责怪培养他们的大学，等等。我们需要一种方法来停止这种回溯。这就是**[近因](@entry_id:149158)**发挥作用的地方。它是一个法律和伦理概念，旨在将责任限制在那些不过于遥远或不可预见的因果关系上。关键问题变成：损害是否是该行为合理可预见的后果？

这就引出了一个优美而微妙的理念：**介入原因**（superseding cause）。当一个知识渊博的专家——法律上有时称之为**有知识的中间人**（learned intermediary）——在掌握所有必要信息后，做出了一个独立的、且在这种情况下是疏忽的决定时，其行为的力量足以切断从原始行为者到损害的因果链。在我们的华法林案例中，人工智能供应商提供了正确的建议*以及*清晰、不可避免的警告。医生有意识地忽略该特定警告的行为是一种独立的疏忽行为。它成为损害的[近因](@entry_id:149158)，切断了追溯到人工智能供应商的法律因果链，尽管该供应商的工具是“若无”链的一部分 [@problem_id:4429757]。

### 回路中的人：不止是一个开关

这引出了系统中最复杂的组成部分：人。人们很容易说：“如果回路中有人，那么他们最终负责。”但现实要微妙得多。**有意义的人类控制**这一概念本身就要求不仅仅是有一个人在场；它要求这个人有真正的能力去*理解*系统、*指导*其行动，并为结果*承担责任* [@problem_id:4850231]。

我们设计人机交互的方式深刻影响着这种控制。我们可以采用**监督模型**，即人工智能行动，人类进行监控，仅在必要时干预。我们可以采用**否决模型**，即人工智能提出行动建议，但未经人类明确批准不得执行。或者我们可以采用**共同决策模型**，即人类和人工智能必须达成一致 [@problem_id:4850231]。每种设计都以不同的方式分配控制权和责任。

此外，我们并非完全理性的生物。人工智能系统的设计可能会利用我们自身心智软件中已知的缺陷。其一是**自动化偏见**，即我们被证实的倾向于过度信任自动化系统，并遵循其建议，即使这些建议与我们自己的判断相冲突。其二是**技能退化**，即当我们过分依赖工具替我们思考时，我们自身的专业技能会逐渐丧失 [@problem_id:4408749]。一个令人困惑、提供建议却不加解释或造成巨大工作流程压力的系统，都可能将临床医生推[向错](@entry_id:161223)误。

这种动态互动意味着，良好医疗的基准——**注意标准**（standard of care）——并非一成不变。它在不断演变。该标准由“合理的审慎执业者”会怎么做来定义。随着人工智能工具的普及，注意标准将转变为包括*审慎使用*这些工具。这并不意味着盲目遵从。相反，它意味着要培养新的技能，知道何时信任人工智能，何时保持怀疑，以及如何在行动前验证其输出 [@problem_id:4421586]。“回路中的人”不是一个被动的监督者，而是一个积极的、批判性的合作伙伴。

### 机器中的幽灵：创造者的责任

如果临床医生有责任，那么制造工具的人呢？人工智能不是道德主体。它不能被“指责”，就像我们不能因为一张制作拙劣的桌子而指责一把锤子一样。但它的创造者——开发者和销售它的公司——是道德主体。我们能基于什么理由让他们负责呢？

道德哲学为我们提供了一个清晰的框架，通常基于两大支柱：**控制条件**和**认知（知识）条件** [@problem_id:4400489]。如果你满足以下条件，你就需要对一个结果负有道德责任：
1.  **控制**：有能力影响因果链并降低风险。
2.  **知识**：你知道，或理应知道，存在风险。

设想一个供应商，他知道自己的人工智能对某些患者亚群表现不佳，并且部署后的监控显示这个问题正在恶化。如果该供应商同时有能力推送软件补丁甚至远程禁用系统（“控制条件”），并且意识到风险（“认知条件”），那么他们就有明确的道德责任采取行动。

这就引出了**回顾性归咎**和**前瞻性问责**之间的重要区别 [@problem_id:4400489]。回顾性归咎是为过去发生的事件确定罪责。这很重要，但不是全部。前瞻性问责是关于持续监控、纠正和改进系统以防止未来伤害的持续性义务。对于医疗领域的人工智能来说，这种前瞻性责任可以说比前者更重要。这是一种致力于构建能够学习并随着时间推移变得更安全的系统，而不仅仅是在悲剧发生后善于推卸责任的系统。

### 弥合差距：从模糊到负责任的系统

当我们将所有这些部分——临床医生、机构、供应商、复杂的因果链——整合在一起时，我们就能看到一个危险情况是如何产生的：**责任缺口**。这种情况发生在一个复杂的、分布式的系统中出现损害，并且由于每个参与者都可以合理地指向他人的失败，导致几乎不可能分配问责 [@problem_id:4425472]。临床医生指责有缺陷的工具，供应商指责医院的配置不当，而医院则指责临床医生的错误。

解决责任缺口的办法，不是在失败*之后*找到一个更好的归咎方式，而是从一开始就设计内置问责机制的系统。这意味着要从被动反应转向主动预防，在事情出错*之前*（ex ante）就分配好责任。

这可以通过稳健的治理机制来实现。医院和供应商可以创建一个正式的责任矩阵（有时因其包含负责、问责、咨询和知情而被称为 **RACI 图表**），明确界定谁负责什么：供应商负责[模型验证](@entry_id:141140)，医院负责用户培训和流程监督，临床医生负责最终的医疗判断 [@problem_id:4425472]。这些协议必须将问责与控制和知识对齐。

这种治理结构必须有技术支柱的支持。要让任何人负责，我们需要审计追踪。我们需要一个系统的记忆。这就是**数据源流**（记录所有数据的来源及处理方式）和**模型谱系**（记录人工智能模型本身如何构建、训练和更新）的作用 [@problem_id:5201680]。这些记录是证据基础，使我们能够进行反事[实分析](@entry_id:137229)——去问“如果……会怎样？”并追溯一个失败的根源，无论是由于有缺陷的数据集 $X$、糟糕的标注函数 $L$，还是模型参数的漂移 $\theta^{(k)}$。

最终，所有这些复杂的系统都建立在一个简单而不可动摇的基础之上：医生对患者的**信托责任**。这种忠诚和注意的义务是**不可转委托的** [@problem_id:4421813]。医生可以将一项*任务*委托给一个人或一个人工智能，但他们永远不能将对其患者福祉的最终*责任*转委托出去。这意味着一种**认知问责**的义务：医生必须始终处于能够理解并为治疗方案提供理由的位置 [@problem_id:4421813]。在人工智能时代，这并不意味着要了解每一种算法的工作原理。它意味着知道如何明智地、审慎地使用这些强大的工具，并始终为那位将信任托付于你的病人服务。

