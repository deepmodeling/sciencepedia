## 引言
历史上，对新材料的探索一直是一个缓慢而审慎的过程，依赖于科学直觉、艰苦实验和偶然发现的结合。然而，可能存在的材料组合数量之多，形成了一个难以想象的浩瀚“宝库”，仅凭传统方法是无法探索穷尽的。这为开发从更高效的电池到先进合金等下一代技术带来了根本性的瓶颈。机器学习带来了一场革命性的[范式](@article_id:329204)转变，它提供的工具能够以前所未有的速度和准确性驾驭这个巨大的化学空间。本文旨在为读者介绍这个激动人心的、融合了计算机科学、化学和物理学的[交叉](@article_id:315017)领域。您将学习我们如何将原子的语言转化为[算法](@article_id:331821)的语言，如何构建能够预测[材料行为](@article_id:321825)的模型，以及如何应用这些工具来加速科学发现。我们将首先探讨使这一切成为可能的基本原理和机制，然后综述那些正在重塑[材料科学](@article_id:312640)领域的变革性应用。

## 原理与机制

想象一下，您想教一台计算机成为一名[材料科学](@article_id:312640)家。从某种意义上说，这与教一个人并无太大区别。您不会只给他们看一块金属，就指望他们能理解它。您需要建立一种共通的语言、一套可供推理的原则，以及一种判断他们推理是否正确的方法。对于[材料科学](@article_id:312640)中的机器学习（ML）而言，过程正是如此：我们必须将丰富而复杂的原子世界转化为严谨的数学语言，建立能够对这些信息进行推理的模型，并根据物理现实的基石来检验它们的预测。

### 原子语言：从化学到数字

计算机不理解“钴酸锂”——你手机电池的主力材料。它理解的是一列列的数字，也就是向量。因此，首要且最根本的挑战就是翻译。我们如何以一种对[算法](@article_id:331821)有意义的方式来表示一种材料？这个过程被称为**[特征化](@article_id:322076)** (featurization)。

最简单的方法就是列出其成分。对于像 $\text{LiCoO}_2$ 或 $\text{LaNiO}_3$ 这样的化合物，我们可以创建一个向量来表示化学式中每种原子的比例。如果我们感兴趣的元素宇宙包含 (Li, La, Co, Ni, O)，那么 $\text{LiCoO}_2$ 就变成了向量 $(\frac{1}{4}, 0, \frac{1}{4}, 0, \frac{1}{2})$，因为四分之一的原子是锂，四分之一是钴，二分之一是氧 [@problem_id:1312282]。这是一种**元素分数向量**。它是一种简单、明确的材料成分指纹。

但我们可以更巧妙一些。我们从一个多世纪的物理学和化学中得知，[元素周期表](@article_id:299916)并不仅仅是元素的随机集合。每种元素都有其内在属性：原子质量、电负性、[熔点](@article_id:374672)等等。我们可以将我们的成分信息与这些先验的物理知识结合起来，“工程化”出一个更有洞察力的特征。对于像 $\text{Al}_{0.50}\text{Cu}_{0.30}\text{Zn}_{0.20}$ 这样的合金，我们可能会猜测其[熔点](@article_id:374672)是其组分熔点的简单[加权平均](@article_id:304268)值。这是一个非常物理化的假设，有点像根据一杯混合饮料的成分来猜测其味道。对于这种合金，这样的计算得出的预测熔点约为 $1013$ K [@problem_id:1312283]。这个单一的数字，一个**成分加权平均性质**，比简单的分数列表是一个更“有根据”的特征。这是我们首次尝试将物理直觉[嵌入](@article_id:311541)到数据本身中。

### 视角的陷阱：为何尺度很重要

现在我们有了特征——每种材料对应一个数字向量。我们可以直接将它们输入到学习[算法](@article_id:331821)中吗？在这里，我们遇到了一个微妙但至关重要的陷阱。想象一下，你正在绘制一张城市地图，但由于某种奇怪的原因，你用公里来测量东西向的距离，而用毫米来测量南北向的距离。如果你要寻找“最近”的咖啡馆，任何距离计算都将完全由东西向的坐标主导。南北方向上几百毫米的差异与东西方向上零点几公里的差异相比，将显得微不足道。

同样的问题也困扰着许多机器学习[算法](@article_id:331821)。假设我们用材料的[熔点](@article_id:374672)（范围从 $300$ 到 $4000$ K）和[电负性](@article_id:308047)（范围从 $0.7$ 到 $4.0$）来描述一种材料。一个依赖于计算[特征空间](@article_id:642306)中材料之间“距离”的[算法](@article_id:331821)，比如广受欢迎的[k-近邻算法](@article_id:641047)，将几乎完全忽略电负性的变化。熔点巨大的数值范围将主导任何距离计算 [@problem_id:1312260]。

解决方法优雅而简单：我们必须将所有特征置于一个公平的竞争环境中。一种标准技术是**[标准化](@article_id:310343)**（standardization），即我们重新缩放每个特征，使其在整个数据集上的平均值为零，标准差为一。这确保了没有任何一个特征仅仅因为其数值巨大而压过其他特征。它让我们精心策划的每一条信息都有公平的机会为模型的最终预测做出贡献。

### 构建模型：从简单的直线到蜿蜒的曲线

有了数值表示，我们现在可以构建模型了。模型只是一个数学假设：一个函数 $f$，它以我们的特征为输入，输出一个预测的性质，比如能量或硬度。

最简单的假设是一条直线：$P = m \cdot x + c$。这就是**线性回归**。它假设当您改变一个特征 $x$ 时，性质 $P$ 会成正比地变化。但是哪条线是“最好”的呢？我们定义一个**成本函数**，通常是模型预测值与一组训练材料的真实已知值之间的平均误差（或平方误差）[@problem_id:1312320]。最好的模型就是能使这个成本最小化的模型。

然而，我们应该内置一种深刻的物理直觉，一种类似于奥卡姆剃刀的理念：倾向于更简单的解释。想象一下你只有两个数据点。一条直线将完美地拟合它们。但如果这些数据点由于测量或计算而带有一些随机“噪声”呢？这条完美的直线可能会非常陡峭，暗示着性质会随着成分的微小变化而剧烈变化。这通常是不符合物理规律的。为了解决这个问题，我们可以使用**[正则化](@article_id:300216)**。我们修改[成本函数](@article_id:299129)，为模型的复杂性增加一个惩罚项。对于线性模型，我们可以增加一个惩罚大斜率 $m$ 的项，例如 $\lambda m^2$ [@problem_id:90109]。现在，模型必须做出权衡。它试图拟合数据，但同时也试图保持其斜率较小。这种寻求简单而准确的模型的做法，是防止模型“[过拟合](@article_id:299541)”——即记住数据中的噪声而不是学习真实的潜在趋势——的有效方法。

当然，世界很少是线性的。考虑一下[压电材料](@article_id:376380)，它们在受压时会产生电。事实证明，这种性质并不仅仅随着晶体中[电负性](@article_id:308047)差异的增加而增加。它通常会在一个最佳值处达到一个尖峰，然后再次下降 [@problem_id:1312273]。试图捕捉这种关系的线性模型将惨败。它的预测会有巨大的误差，这个误差可以用**[均方根](@article_id:327312)误差 (RMSE)** 等指标来量化。

对于这类问题，我们需要**非线性模型**。这些是更灵活的函数，能够[学习曲线](@article_id:640568)、峰值和谷值。例如，一个受[支持向量机](@article_id:351259)启发的模型可以产生一个高斯“[凸包](@article_id:326572)”函数，$f(x) = A \exp(-B(x-x_0)^2)$，它非常适合捕捉在特定[特征值](@article_id:315305) $x_0$ 处达到峰值的性质 [@problem_id:1312273]。机器学习的美妙之处就在于这种工具的层次结构：我们可以选择数学假设的复杂性，以匹配我们试图揭示的潜在物理过程的复杂性。

### 超越成分：捕捉物质的几何结构

到目前为止，我们基本上忽略了化学的一个基石：结构。金刚石和石墨都是纯碳，但它们截然不同的性质源于其原子在空间中[排列](@article_id:296886)方式的不同。一个简单的成分[特征向量](@article_id:312227)对此是无视的。

为了捕捉结构，我们必须描述一个原子的**局部化学环境**。其思想是，一个原子对材料总能量的贡献取决于它的邻居：它们是什么，距离多远，以及以何种方向[排列](@article_id:296886)。我们需要这个环境的数值指纹，即**描述符**。这个描述符必须遵守物理学的基本对称性 [@problem_id:2648581]：

1.  **平移不变性**：如果我们拿起整个材料并移动它，其中任何一个原子的环境都没有改变。描述符必须只依赖于邻居的相对位置，而不是绝对坐标。
2.  **[旋转不变性](@article_id:298095)**：如果我们旋转材料，环境在根本上也没有改变。
3.  **[置换](@article_id:296886)不变性**：如果两个相同的邻近原子，比如两个氧原子，交换位置，环境也没有改变。描述符必须对相同原子的标记不敏感。

我们如何构建这样的东西呢？考虑一个玩具描述符，它通过列出到所有邻居的距离倒数，然后对该列表进行排序来构成 [@problem_id:91132]。排序是实现[置换](@article_id:296886)不变性的一个非常简单的技巧。无论你以何种顺序列出邻居，排序后的列表总是相同的。通过使用距离——其本身对旋转和平移是不变的——我们创造了一个尊重所有必要对称性的描述符。现代的描述符要复杂得多，但它们都建立在编码物理对称性这一基本原则之上。

### 机器中的物理学：统一与基础

在这里，所有的部分以一种卓越的综合方式汇集在一起。[材料科学](@article_id:312640)中最先进的机器学习模型将系统的总[能量分解](@article_id:372528)为原子能量的总和，其中每个原子的能量由一个机器学习模型根据其局部描述符进行预测 [@problem_id:2648581]。

$$E_{\text{total}} = \sum_{i=1}^{N} E_{\text{atomic}}( \text{descriptor of atom } i )$$

这个“以原子为中心”的框架具有深远的影响。因为描述符是建立在相对位置上的，所以总能量自动地具有[平移不变性](@article_id:374761)。而从这种[不变性](@article_id:300612)中，一个深刻的物理定律免费地浮现出来：系统上的总力被保证为零 [@problem_id:91075]。模型天生就尊重动量守恒。它能学到正确的物理学，因为我们已经将物理定律的对称性编织进了其数学结构本身。

这给了我们一个能量模型。但是要模拟材料如何随时间演变——它们如何熔化、断裂或催化反应——我们需要**力**。在物理学中，力是势能的（负）梯度，$\mathbf{F}_k = -\nabla_{\mathbf{r}_k} E_{\text{total}}$。因为我们的机器学习模型只是一个巨大的数学函数，我们可以解析地计算它的梯度。因此，这些力与能量是完全一致的；它们是**保守的**。

但是我们用什么来训练这个宏大的模型呢？我们需要一个“基准真相”数据的来源——无数原子构型的能量和力。这些数据并非来自实验，因为实验太慢太困难，而是来自量子力学。**[密度泛函理论 (DFT)](@article_id:365703)** 是一种强大的计算方法，可以（近似地）求解薛定谔方程，从而找到一个原子系统的能量以及作用在每个原子上的力。

这就引出了最后一个关键问题。来自 DFT 的力本身是保守的吗？它们是否对应一个明确定义的[势能面](@article_id:307856)？答案在于 **Hellmann-Feynman 定理**。这个优雅的量子力学定理保证，如果一个 DFT 计算被正确地执行（达到自洽并妥善处理[基组](@article_id:320713)），那么计算出的力确实是 DFT 总能量的精确梯度 [@problem_id:2837976]。

这就形成了一个闭环。我们使用量子力学（DFT）生成一个高保真度的能量和力的数据集。然后，我们训练一个具备对称性感知的机器学习模型，来学习从原子结构到能量的映射。由此产生的**[机器学习原子间势](@article_id:344521)**是量子力学计算的替代品——拥有相同的精度，但运行速度快上数千甚至数百万倍。正是这种建立在从[数据表示](@article_id:641270)到模型构建再到基本物理对称性的层层原理之上的惊人加速，使我们能够以前所未有的速度发现和设计新材料。