## 应用与跨学科联系

在我们探究了完整案例分析——或者更直白地说，[列表删除法](@entry_id:637836)——的数学原理之后，人们可能会留下一个看似简单的印象。这种方法似乎很直接，其假设也很清晰。但一个科学工具的真正特性不是在抽象中显现，而是在其应用中。当我们用这个简单的工具来处理现实世界中混乱、不完整的数据集时会发生什么？答案，正如我们将看到的，是一个贯穿整个现代科学领域的、引人入胜且往往具有警示意义的故事。

我们的探索不仅仅是简单的罗列。相反，我们将看到相同的统计学基本原理如何在截然不同的领域中发挥作用，从遗传密码到浩瀚的卫星图像，从患者的健康轨迹到生物组织的复杂重建。我们会发现，丢弃不完整数据这一看似无害的行为可能导致两种主要麻烦：清晰度的丧失，以及更危险的，对现实的扭曲看法。

### 最佳情景：以功效为代价的有效性

让我们从最宽容的环境开始，即数据[完全随机缺失](@entry_id:170286)（MCAR）的世界。在这个理想化的世界里，数据缺失的原因与数据本身完全无关，就像研究人员随机打碎了几支试管一样。

想象一位生物统计学家试图用线性回归模型来理解一组临床预测因子与患者结局之间的关系。如果一些患者有缺失值，且这种缺失是真正随机的（MCAR），那么仅对“完整案例”进行分析所得到的[回归系数](@entry_id:634860)平均而言是正确的。基础的统计检验，如用于模型整体显著性的 ANOVA $F$-检验，仍然有效 [@problem_id:4893796]。类似地，一位遗传学家在检验一个群体的 Hardy-Weinberg 平衡——群体遗传学的基石——时，可以对含有缺失基因型的样本使用[列表删除法](@entry_id:637836)，只要缺失是 MCAR，仍然可以获得有效的卡方检验 [@problem_id:2841842]。

那么，问题出在哪里呢？问题在于功效和精度。通过丢弃每一个哪怕只有一个缺失值的受试者，我们主动减少了样本量。这就像试图在房间的另一头读报纸。文字没有偏见——字母就是那些字母——但它们是模糊的。我们对自己所见之物的信心减弱了。用统计学术语来说，我们的估计值有更大的[标准误](@entry_id:635378)，我们的[置信区间](@entry_id:138194)更宽，我们检测真实效应的能力——即统计功效——也减弱了。诸如[多重插补](@entry_id:177416)等更复杂的技术正是为了解决这种低效率问题而开发的，它们使我们能够利用我们辛辛苦苦收集的所有数据，将报纸上的内容看得更清楚。但关键点在于，这种唯一的代价是功效损失的良性情景，完全依赖于 MCAR 这个脆弱的假设。

### 当假设崩溃时：现实世界中的简单性幻觉

在现实世界中，MCAR 假设很少是可靠的。数据缺失通常是有原因的，而这个原因往往与数据本身有关。正是在这里，[列表删除法](@entry_id:637836)从一个低效的工具转变为一个危险的工具，能够产生系统性偏差。

考虑分析电子健康记录（EHR）这一现代挑战。数据科学家可能会应用像主成分分析（PCA）这样的强大技术来分析成千上万的实验室结果，以寻找患者群体中疾病的主要模式。但谁会接受哪些实验室检查呢？医生怀疑心脏病发作时会开具心肌酶检测，或者因为患者表现出糖尿病迹象而开具血糖检测。一项检测被执行（及其结果被*观测到*）的行为本身就与患者潜在的健康状况相关。执行[列表删除法](@entry_id:637836)——只保留那些由于某种原因接受了每一项实验室检测的少数患者——就等于是在分析一个奇怪且不具代表性的患者群体片段。由此产生的“主成分”将不会反映社区中健康和疾病的真实模式，而是反映了临床决策和患者工作流程中固有的偏见 [@problem_id:5220635]。

这个问题在追踪个体随时间变化的纵向研究中普遍存在。在一项追踪患有[1型糖尿病](@entry_id:152093)青少年的儿科心理学研究中，研究人员可能想了解生活质量在一年内的变化情况。但谁最有可能错过随访预约？通常是那些挣扎最厉害的患者，也许是因为他们的疾病控制不佳或感到沮丧。重复测量[方差分析](@entry_id:275547)是这类分析的经典工具，通常默认使用[列表删除法](@entry_id:637836)，丢弃任何错过一次预约的青少年。因此，分析仅限于最勤奋且可能更健康的参与者，这可能描绘出一幅关于疾病轨迹的、过于乐观的虚假画面。在这里，现代统计学的美妙之处体现在像线性混合效应模型这样的方法中，这些方法从根本上就是为了处理不规则间隔的数据而构建的，并且可以在更现实的[随机缺失](@entry_id:168632)（MAR）假设下提供[无偏估计](@entry_id:756289)，即缺失可以依赖于*观测到*的数据（如患者先前记录的生活质量）[@problem_id:4729502]。

偏差甚至可能更加微妙。在中断[时间序列分析](@entry_id:178930)中，流行病学家可能研究一项新卫生政策对门诊就诊次数的影响。假设人员短缺使得在就诊人数异常多的一周之后，数据录入错误（从而导致数据缺失）的可能性更大。这意味着观察到今天数据的概率取决于昨天的结果。由于时间序列的结果是相关的（大多数时间序列模型的假设），这种选择性缺失会产生一种在整个分析中荡漾的扭曲，从而使政策效果的估计产生偏差 [@problem_id:4604617]。简单地删除缺失点会以非随机的方式打破时间链，从而破坏我们理解因果关系的能力。

### 看见偏差：具体和可视化的例子

有时，理解一个缺陷最深刻的方式是亲眼看到它的作用。让我们设想一家医院筛查患者的抑郁症。筛查工具的“敏感性”是衡量它多好地正确识别真实病例的指标。假设，在真正患有抑郁症的患者中，那些筛查结果本应为阳性的人的结果更有可能丢失——也许他们感到不堪重负而无法完成表格。如果我们应用[列表删除法](@entry_id:637836)，我们就是在优先从数据集中移除真阳性病例。当我们随后计算敏感性——即观测到的真实病例中筛查为阳性的比例——结果将被人为地拉低。我们会错误地得出结论，认为这个筛查工具不如实际有效。这是一个由删除基于与结果本身相关的属性的案例直接引起的、可量化的偏差 [@problem_id:4739951]。这种缺失依赖于未观测值的场景被称为[非随机缺失](@entry_id:163489)（MNAR），这也是[列表删除法](@entry_id:637836)最危险的地方。

在视觉领域，偏差也同样惊人。想象一位病理学家从数百个薄组织切片中创建肾脏的3D重建。如果在处理过程中丢失了一片切片怎么办？[列表删除法](@entry_id:637836)无法提供解决方案。如果一片切片被撕裂，或者有折痕遮挡了部分组织怎么办？一种天真的方法可能是删除这些受损的切片。但这样做就像试图通过扔掉任何有洞的面包片来计算一条面包的体积一样——你肯定会得到错误的答案。物理上正确的方法是认识到组织在那个位置是存在的，并利用来自相邻、高度相关的切片的信息来智能地“修复”或[插补](@entry_id:270805)缺失的信息。丢失的切片可以从其上方和下方的切片来估计。撕裂处可以通过观察其周围的完整组织以及相邻切片中的相应区域来填补。这种基于模型的插补尊重了数据的物理现实和[光滑结构](@entry_id:159394)，而[列表删除法](@entry_id:637836)则违背了它 [@problem_id:4313243]。

### 高维灾难：千刀万剐之死

如果说[列表删除法](@entry_id:637836)在这些例子中是有问题的，那么在现代高维生物学的世界里，它就变得完全是灾难性的。考虑一位系统生物学家，他旨在整合患者的基因组学（DNA）、转录组学（RNA）和蛋白质组学（蛋白质）数据，以获得对其疾病的整体看法。这是[个性化医疗](@entry_id:152668)的前沿。

现在，假设每个患者的[转录组学](@entry_id:139549)数据完整度为95%，而更难测量的蛋白质组学数据完整度仅为70%。如果我们坚持只使用同时拥有*两种*完整数据集的患者，我们就必须应用[列表删除法](@entry_id:637836)。一个患者拥有完整[转录组](@entry_id:274025)数据的概率是 $0.95$，拥有完整蛋白质组数据的概率是 $0.70$。由于这些是独立的，同时拥有两者的概率是 $0.95 \times 0.70 = 0.665$。在分析开始之前，我们就被迫丢弃了三分之一的队列 [@problem_id:4389260]。现在想象一下，我们想整合十个不同的“组学”数据集，每个都只有5%的缺失数据。完整案例的比例将是 $(0.95)^{10} \approx 0.60$，损失了我们40%的数据。如果缺失更多，完整案例的数量很容易骤降至零。

这就是完整案例分析在大数据时代的巨大失败。它无法处理多个、部分重叠的信息来源的整合。相比之下，现代机器学习方法，如[潜变量模型](@entry_id:174856)，正是为此而设计的。它们可以观察一个只有转录组数据的患者，并用它来学习共享的生物过程。它们可以观察另一个只有[蛋白质组](@entry_id:150306)数据的患者，并做同样的事情。通过综合所有可用的碎片，它们在整个数据集中“借用力量”，描绘出一幅单一、连贯的图景——这是[列表删除法](@entry_id:637836)以其粗暴的简单性所无法完成的壮举。

### 倡导有原则的分析

完整案例分析在科学领域的历程是一个有力的教训。它的简单性是海妖的歌声，诱使我们走向一种通常是无力、有偏见或完全错误的简单分析。从遗传学到心理学，从流行病学到遥感 [@problem_id:3806570]，我们已经看到，如何处理缺失数据的问题不是一个边缘的清理任务。它是一种核心的[统计建模](@entry_id:272466)行为，要求我们批判性地思考我们的数据*为何*不完整。

统计学的真正美妙之处不在于忽略复杂性，而在于开发能够拥抱复杂性的工具。[列表删除法](@entry_id:637836)的有原则的替代方法——[多重插补](@entry_id:177416)、混合效应模型、[逆概率](@entry_id:196307)加权和[潜因子模型](@entry_id:139357)——可能看起来更复杂，但这是因为它们试图做一些更深刻的事情：从我们拥有的不完美证据中重建一个更真实的世界表征。它们提醒我们，科学的目标不仅仅是分析我们拥有的数据，而是理解产生这些数据的世界。