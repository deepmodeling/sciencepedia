## 引言
在当今这个数据爆炸的时代，从基因组序列到互联网文本，瓶颈已不再是数据收集，而是数据理解。手动标注海量信息是不可行的，这造成了严重的知识鸿沟。[表示学习](@article_id:638732)提供了一种强大的解决方案，它让机器能够在没有明确人类监督的情况下，学习数据底层的结构和意义。本文旨在揭开这一关键领域的神秘面纱。首先，文章将探讨其核心的**原理与机制**，详细介绍自监督技术（如预测式学习和对比式学习）如何使模型能将原始数据提炼成强大、结构化的表示。随后，文章将深入探讨**应用与跨学科联系**，展示这些学习到的表示如何给从计算生物学到[自然语言处理](@article_id:333975)等领域带来革命性变化，甚至如何应对[算法公平性](@article_id:304084)等社会挑战。这段旅程始于理解机器如何在没有老师的情况下进行学习。

## 原理与机制

想象一下，你是一位考古学家，发现了一座来自失落文明的图书馆。你拥有数百万本书籍，但只有一小块罗塞塔石碑，上面仅有寥寥数语的译文。面对这座浩瀚无垠、未经标注的图书馆，你该如何着手理解其中的语言、语法和概念？这正是我们在现代数据世界中面临的挑战。我们淹没在原始信息的海洋中——图像、文本、[生物序列](@article_id:353418)——但人类提供的标签却如涓涓细流。采用暴力方法为所有数据打上标签是不可能的。我们需要一种更巧妙的策略，需要让数据自我教学。

这正是**[表示学习](@article_id:638732)**的核心前景。其目标不仅仅是存储数据，更是提炼数据，将其转换为一种新的形式——一种**表示**——从而使后续的分类或预测等任务变得异常简单。这关乎在没有人类导师手把手指导的情况下，发掘数据的精髓、其底层的结构和意义。

### 无师自通：自监督的艺术

机器如何在没有标签的情况下学习？一个巧妙的答案是：从数据本身创造标签。这种策略被称为**[自监督学习](@article_id:352490)**。机器被赋予一个任务，一种“前置”（pretext）谜题，即利用数据的一部分来预测另一部分。

可以这样理解：你取一个句子，去掉其中一个词，然后让机器填空。或者你拍一张照片，剪掉一小块，然后让它补全缺失的部分。数据为自己提供了监督。

一个来自生物学的绝佳现实世界案例是：我们拥有像 [UniProt](@article_id:336755) 这样庞大的数据库，其中包含数百万个蛋白质序列——生命的基本构件。对于其中大多数序列，我们没有任何关于其功能或结构的外部标签。我们如何从中学习呢？我们可以玩一个类似于句子填空的游戏。我们取一个蛋白质序列，以数字方式“掩码”或隐藏其中的几个氨基酸，然后训练一个大模型，让它根据周围的上下文来预测原始的氨基酸。要在这个游戏中取胜，模型不能只靠记忆；它必须学会“生命语言”中深层的语法和语义规则——即支配蛋白质如何构建的生物物理约束和进化模式。这整个过程，作为一种[自监督学习](@article_id:352490)的形式，正是像 ESM-2 这样的突破性模型的动力来源。这些模型学习蛋白质的丰富表示，却从未被明确告知蛋白质的功能 [@problem_id:2432861]。由此产生的表示非常强大，后续仅需少量标记数据，就能以惊人的准确度预测蛋白质的结构和功能。

当我们只有少量标记样本和海量未标记样本时，这种方法巧妙地弥合了两者之间的鸿沟。我们不是忽略未标记数据，而是首先在*所有*数据上使用自监督方法学习一个强大的表示。然后，我们用这个小规模的标记数据集针对特定任务对该表示进行微调。这在[计算生物学](@article_id:307404)等领域是一种标准且强大的技术。在这些领域，少数昂贵的 CRISPR 筛选实验提供了标记数据，而大量的测序数据可用于无监督[预训练](@article_id:638349) [@problem_id:2432801]。

### 两大路径：预测与对比

[自监督学习](@article_id:352490)在创建这些前置任务时，主要遵循两种哲学：预测和对比。

#### 预测路径

预测路径，正如我们在蛋白质模型中看到的那样，旨在根据数据的可见部分预测其隐藏或未来的部分。[自然语言处理](@article_id:333975)中的经典例子是 **Word2Vec**。为了学习词语的含义，我们可以强制模型根据其邻近词来预测一个词（连续[词袋模型](@article_id:640022)，即 CBOW 模型），或者反过来，根据一个中心词来预测其邻近词（Skip-gram 模型）。

为什么有两种方法？它们之间有什么区别？这取决于我们想要捕捉什么样的信息。CBOW 模型通过对上下文进行平均来进行单次预测。这种平均过程平滑了信息，使模型非常擅长学习常见的模式和句法规律，而这些规律通常由“the”或“in”等高频词决定。这有点像学习语法。另一方面，Skip-gram 模型则强制一个词成为多个不同上下文词的良好预测器。这使得罕见但意义丰富的词（如“axion”或“sonnet”）在训练中拥有更大的影响力，因为它们每一次的少数出现都会产生多个学习信号。结果是，Skip-gram 往往能为语义——即内容丰富的词语的实际意义——生成更好的表示 [@problem_id:3200063]。这种微妙的架构差异导致了在学习句法和语义之间的深刻权衡，完美地说明了自监督任务的设计如何塑造最终的表示。

#### 对比路径

第二条主要路径是**[对比学习](@article_id:639980)**。其原理简单而直观：“相似的事物应有相似的表示，而不同的事物应有相异的表示。”

想象一下，你有一张猫的图片。然后你对它进行轻微修改，生成两个略有不同的版本——通过裁剪、改变颜色或轻[微旋转](@article_id:363623)。这两个版本就是“正样本对”。接着，你再取一张不同的图片，比如一张狗的图片，它就成了“负样本”。[对比学习](@article_id:639980)模型的目标是学习一种表示，在这种表示空间中，两张猫的图片被拉近，而猫的图片和狗的图片则被推远。

像 SimCLR 这样的现代方法背后有一个惊人的洞见：这个游戏可以被构建成一个大规模的分类问题。数据集中的每个实例（例如，每张图片）都被视为一个独立的类别。对于一张猫的图片的增强视图，模型的目标是在数百万张其他“负样本”图片中，正确地将其“分类”为隶属于原始的猫图片。这个过程的数学原理由一种名为 **InfoNCE** 的[损失函数](@article_id:638865)主导，其代数形式与一个拥有数百万个类别的分类器所使用的标准[交叉熵损失](@article_id:301965)函数完全相同 [@problem_id:3173290]。通过学习解决这个极其困难的实例判别游戏，模型被迫去发现区分不同物体的本质视觉特征。它学会了是什么让猫之所以为猫，而从未被告知“猫”这个词。

### 意义的几何学：何为好的表示？

那么，我们已经学习到了一种表示。一个“好的”表示应该是什么样子？一个好的表示不仅仅是一堆杂乱无章的数字；它具有有意义的几何结构。

#### 对称性与解耦

物理世界受对称性支配。物理定律不会因为你把实验搬到另一个房间（平移不变性）或将其颠倒（[旋转不变性](@article_id:298095)）而改变。一个好的物理系统表示应该遵循同样的对称性。例如，一个分子的能量取决于其原子的相对位置，而不是它在空间中的绝对位置或朝向。因此，一个预测这种能量的机器学习模型必须学习一种对全局平移和旋转**不变**的表示 [@problem_id:2760102]。实现这一点的一种方法是，完全基于原子间的距离和角度来构建表示，因为当整个分子被移动或旋转时，这些量是不会改变的。

一个更强大的思想是**[等变性](@article_id:640964)**（equivariance）。在这种思想下，表示并非保持不变，而是以一种可预测的方式进[行变换](@article_id:310184)，以反映输入中的变换。想象一下，你有一个代表图像的[潜空间](@article_id:350962)。对于一个等变表示，将输入图像旋转30度，将对应于[潜空间](@article_id:350962)中一个特定的、已知的变换——也许是一个简单的旋转。这是迈向**[解耦](@article_id:641586)**（disentanglement）的关键一步，在解耦的表示空间中，不同的坐标轴控制着数据中不同的、有意义的变化因素，如姿态、光照或身份 [@problem_id:3100694]。构建这种结构化的表示使我们能够以有意义的方式操纵数据，例如，只需沿着[潜空间](@article_id:350962)中的特定方向移动，就可以生成同一物体在新视角下的图像。

#### 终点：神经坍塌

对于一个分类任务，理想的几何结构是什么？假设我们正在对动物图片进行分类。在一个训练良好的深度网络中，会出现一种被称为**神经坍塌**（neural collapse）的显著现象。随着训练的进行，所有不同“狗”的图像的表示将坍塌到表示空间中的一个单点上——即它们的类均值。同样的情况也发生在所有“猫”的图像、所有“鸟”的图像等等。

此外，这些类均值点本身并不会[随机分布](@article_id:360036)。它们会[排列](@article_id:296886)成一种高度对称且最大化分离的结构，称为**单纯形等角紧框架**（simplex equiangular tight frame）。想象一下三维空间中一个四面体的顶点，它们彼此之间的距离尽可能远。神经坍塌描述了这种在高维空间中涌现的几何结构，其中*类内*变异消失，而*类间*变异以最对称的方式被最大化 [@problem_id:3123405]。这种优美而简洁的结构，正是[监督学习](@article_id:321485)力求达到的几何终点。

### 友情提醒：[无监督学习](@article_id:320970)的局限

人们很容易将自监督[表示学习](@article_id:638732)视为一颗万能的“银弹”。只要将一个庞大的未标记数据集扔给一个大模型，它就能发现你需要的所有特征。但我们一直以来都做出了一个至关重要且隐含的假设。我们假设数据本身的结构 $p(x)$ 包含了与我们关心的任务 $p(y|x)$ 相关的信息。

想象一个合成世界，其中数据 $x$ 分布在两个非常清晰、分离良好的簇中。一个无监督[算法](@article_id:331821)可以轻松找到这些簇。但如果标签 $y$（比如“红色”或“蓝色”）是完全随机分配的，与一个点属于哪个簇毫无关系，那会怎么样？在这种情况下，学习 $x$ 优美的簇结构对于预测标签 $y$ 来说是完全无用的。[特征和](@article_id:368537)标签之间的[互信息](@article_id:299166)为零，无论你看到什么样的 $x$，你所能做的最好的事情就是猜测多数类 [@problem_id:3134079]。

这凸显了一个基本原则：要使[表示学习](@article_id:638732)对下游任务有效，定义[数据结构](@article_id:325845)的底层变化因素也必须对该任务的标签具有预测性。幸运的是，在现实世界中，情况往往如此。在一张图片中区分猫和狗的特征，实际上正是对它们进行分类所需要的特征。[表示学习](@article_id:638732)之所以有效，是因为我们的世界充满了有意义的结构，不像上面提到的病态案例。其艺术和科学之处就在于设计出能帮助我们的模型找到这种结构的任务。

