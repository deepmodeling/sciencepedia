## 引言
分析事件*何时*发生是几乎所有科学和工程学科中的一项关键任务，从临床试验中追踪患者的生存情况，到在线课程中预测学生的辍学。然而，我们收集的数据往往是对现实不完整且不断演变的记录，而非一本完美的账本。我们的认知常常因为事件仅被部分观测到而变得复杂——这种现象被称为删失——它对传统的分析方法构成了重大挑战。当我们不知道研究中每个对象的最终结局时，我们如何能够对生存率或失败时间得出准确的结论？

本文为事件时间数据分析的原理和应用提供了全面的指南，阐明了使我们能够处理这些不完美、真实世界数据集的核心概念。我们的旅程将从“原理与机制”一章开始，在其中我们将探讨时态数据的基本性质，揭示 [Kaplan-Meier](@entry_id:169317) 方法处理删失观测的统计精妙之处，并讨论[竞争风险](@entry_id:173277)和[区间删失](@entry_id:636589)等更复杂的情景。随后，“应用与跨学科联系”一章将展示这些工具卓越的通用性，说明相同的统计逻辑如何跨越不同领域——将医学和流行病学与数据科学、教育技术乃至系统生物学联系起来——从而将“何时”这个简单的问题转变为深刻洞见的源泉。

## 原理与机制

要真正理解随时间分析事件意味着什么，我们必须首先解决一个出人意料的深刻问题：在我们的数据中，时间究竟*是*什么？我们常常将数据库想象成记录现实的完美、全知的账本。但事实，正如科学中常见的那样，要有趣和复杂得多。

### 墙上的影子：什么是事件时间？
想象一下，一名护士在医院为病人静脉注射抗生素。输液从下午 2:10 开始，到下午 2:40 结束。这 30 分钟的时间窗口就是**事件时间**——我们希望捕捉的现实片段，也就是“地面真实”(ground truth)。但我们对它的了解从来都不是直接的。护士完成工作后，走到电脑前，在下午 3:05 输入了这些信息。片刻之后，医院的数据库服务器在下午 3:07 提交了这条新记录。这个下午 3:07 的时间戳就是**记录时间**，即信息进入数字世界的时刻。护士*认为*真实的时间区间，即从下午 2:10 到 2:40，是**有效时间**——即在现实模型中，该记录的事实被断定为真的时期。

现在，假设护士后来意识到她的手表不准，输液实际上是在下午 2:45 结束的。她在下午 3:11 提交了更正，数据库在下午 3:12 将其保存为一个新版本。一个复杂的系统并不会简单地覆盖旧的事实。它会保留旧事实，并注明“输液时间从 2:10 到 2:40”这一事实根据下午 3:07 的记录被认为是有效的，但这一认知已被一个新事实“输液时间从 2:10 到 2:45”（记录于下午 3:12）所取代。事件时间、有效时间和记录时间之间这种优雅的互动是所有时态[数据建模](@entry_id:141456)的基础 [@problem_id:4833255]。我们的数据并非现实本身，而是现实投下的影子，是我们对所发生事件的认知不断演变的历史。

### 从未知中求知的艺术：删失与 Kaplan-Meier 方法
当我们完全无法观测到某个事件时，现实与我们记录之间的这种分离变得更为深刻。这是生存分析的核心挑战。假设我们正在测试一种新药，并对 100 名患者进行跟踪，观察他们的存活时间。研究计划持续五年。五年期满时，一些患者不幸已经去世；对于他们，我们知道确切的事件发生时间。但那 60 名仍然在世的患者呢？我们不能简单地忽略他们，也不能假设他们会永生。他们的故事尚未结束。

这就是**右删失**的典型案例。我们知道，对于这 60 名患者，他们的事件时间 $T$ *大于*五年，但我们不知道具体大多少。同样，一个患者可能在两年后搬到另一个城市而失访。我们知道他们至少存活了两年。这并非研究的失败，而是我们所收集数据的一个基本特征。我们的观测在时间轴的右侧被系统性地“切断”了 [@problem_id:4806041]。

那么，我们如何才能计算像平均生存时间这样简单的事情，或者绘制一条随时间变化的生存概率曲线呢？如果我们只对我们确实观测到的事件时间进行平均，我们将严重低估生存率，因为我们忽略了所有活得更久的人。如果我们将删失的人包括在内，并将他们的删失时间视为事件时间，那么我们就是假设他们在离开研究的那一刻就死亡了，这显然也是错误的。

这时，一种真正优美的统计推理方法应运而生：**[Kaplan-Meier](@entry_id:169317) 估计量**。这个由 Edward Kaplan 和 Paul Meier 在 1958 年提出的想法，不是将生存视为单一的结果，而是看作一连串的条件概率。要存活五年，你必须首先存活第一年，*然后*，在存活了第一年的条件下，你必须存活第二年，依此类推。总生存概率是每一步这些[条件概率](@entry_id:151013)的乘积 [@problem_id:1961439]。

Kaplan-Meier 方法的巧妙之处在于它如何在这个链条中处理[删失数据](@entry_id:173222)。假设在特定时间 $t_j$，发生了一些事件。为了计算存活过这个时刻的概率，我们关注在 $t_j$ 之前仍在研究中且面临事件“风险”的每一个人。我们将这个人数称为 $n_j$。如果此时有 $d_j$ 个人发生了事件，那么在这一刻*失败*的估计概率就是 $\frac{d_j}{n_j}$。因此，*存活*过这一刻的概率就是 $1 - \frac{d_j}{n_j}$。

那么删失的人呢？一个在时间 $\tilde{t}$ 被删失的人，在此前的一瞬间，是风险集的一部分。他们为在他们离开之前发生的任何事件的分母 $n_j$ 做出了贡献。在他们被删失后，他们就从所有未来的风险集中消失了。我们不假设他们死亡，也不假设他们存活。我们利用他们离开前提供的信息——即他们至少存活到时间 $\tilde{t}$——然后与他们告别。生存曲线就是这些在每个事件时间点的条件生存概率的累积乘积。

在许多真实世界的数据集中，事件的记录并非无限精确。例如，由于诊所的访视时间是离散的，几名患者可能被记录在同一天发生事件。这些被称为**时间结**（tied events）。[Kaplan-Meier](@entry_id:169317) 的逻辑以其非凡的优雅处理了这种情况。如果在时间 $t_j$ 有 $n_j$ 人处于风险中，并发生了 $d_j$ 个事件，那么在该时刻失败的[条件概率](@entry_id:151013)的最佳估计仍然是简单的比例 $\frac{d_j}{n_j}$。我们可以将其视为 $n_j$ 次独立[伯努利试验](@entry_id:268355)中的 $d_j$ 次“成功”，或者是在一个无穷小的时间窗口内发生 $d_j$ 个事件的伸缩积。两种概念路径都导向同一个稳健的公式，这证明了其基本性质 [@problem_id:5216342]。

### 步骤详解：实践中的生存分析
让我们用一个小例子来具体说明。想象一个只有 6 个个体的小型研究。我们跟踪他们并记录他们的结局为（时间，状态），其中状态 1 表示事件，0 表示删失 [@problem_id:4576787]。
-   个体 1：(2, 1) - 在时间 2 发生事件
-   个体 2：(3, 0) - 在时间 3 删失
-   个体 3：(4, 1) - 在时间 4 发生事件
-   个体 4：(5, 1) - 在时间 5 发生事件
-   个体 5：(6, 0) - 在时间 6 删失
-   个体 6：(7, 1) - 在时间 7 发生事件

让我们计算在时间 $t=5$ 时的生存概率，记为 $\hat{S}(5)$。

1.  **在时间 $t=0$**：所有 6 个体都存活且在研究中。生存概率为 $1$。
2.  **第一个事件在 $t=2$**：在时间 2 之前，有 $n_1=6$ 人处于风险中。发生一个事件 ($d_1=1$)。
    -   条件生存概率 = $1 - \frac{d_1}{n_1} = 1 - \frac{1}{6} = \frac{5}{6}$。
    -   截至此时的总生存率：$\hat{S}(2) = 1 \times \frac{5}{6} = \frac{5}{6}$。
    -   此事件后，剩余 5 个个体。
3.  **在 $t=3$ 删失**：个体 2 离开研究。他们在时间 3 之前一直处于风险中，但现在他们从未来的风险集中移除。处于风险中的人数降至 4 人。
4.  **第二个事件在 $t=4$**：在时间 4 之前，有 $n_2=4$ 人处于风险中。发生一个事件 ($d_2=1$)。
    -   条件生存概率 = $1 - \frac{d_2}{n_2} = 1 - \frac{1}{4} = \frac{3}{4}$。
    -   截至此时的总生存率：$\hat{S}(4) = \hat{S}(2) \times \frac{3}{4} = \frac{5}{6} \times \frac{3}{4} = \frac{15}{24} = \frac{5}{8}$。
    -   此事件后，剩余 3 个个体。
5.  **第三个事件在 $t=5$**：在时间 5 之前，有 $n_3=3$ 人处于风险中。发生一个事件 ($d_3=1$)。
    -   条件生存概率 = $1 - \frac{d_3}{n_3} = 1 - \frac{1}{3} = \frac{2}{3}$。
    -   在 $t=5$ 时的总生存率：$\hat{S}(5) = \hat{S}(4) \times \frac{2}{3} = \frac{5}{8} \times \frac{2}{3} = \frac{10}{24} = \frac{5}{12}$。

因此，我们对存活超过时间 5 的概率的最佳估计是 $\frac{5}{12} \approx 0.417$。一种忽略删失的简单分析可能只会计算六个观测时间中有多少个大于 5（只有时间 6 和 7），得出的生存估计为 $\frac{2}{6} = \frac{1}{3} \approx 0.333$。这种简单的估计存在向下偏倚，正是因为它错误地因个体被删失而惩罚了整个队列。[Kaplan-Meier](@entry_id:169317) 方法通过恰当考虑来自删失受试者的信息，提供了一个更准确、更深刻的生存图景。

### 更广阔的未知世界：[区间删失](@entry_id:636589)及其他
右删失只是不完全信息的一种。如果我们通过定期筛查来追踪潜伏性[结核病](@entry_id:184589)的转化情况呢？一名参与者在 1 年期检查中结果为阴性，但在 2 年期检查中结果为阳性。我们不知道确切的转化时刻；我们只知道它发生在 $(1, 2]$ 年这个区间内的某个时间。这就是**[区间删失](@entry_id:636589)**。如果他们在第一次访视时就检测为阳性，我们就遇到了**[左删失](@entry_id:169731)**——我们只知道事件发生在那次访视之前的某个时间 [@problem_id:5228324]。

[Kaplan-Meier](@entry_id:169317) 方法依赖于事件的精确排序，因此无法直接处理这[类数](@entry_id:156164)据。简单地应用它，例如假设事件发生在区间的中点，会引入偏倚 [@problem_id:4605653]。为了解决这个问题，我们需要一个更通用的工具。这个工具就是**Turnbull 估计量**，即用于[区间删失](@entry_id:636589)数据的[非参数最大似然估计](@entry_id:164132)量 ([NPMLE](@entry_id:164132))。其内部工作原理更为复杂，通常需要迭代算法，但其原理是优美的：它找到一条生存曲线，该曲线能最大化我们实际观测到的观测区间集合出现的概率。它不是将概率[质量分配](@entry_id:751704)给特定的时间点，而是分配给一组与数据一致的不相交的“Turnbull 区间”。值得注意的是，如果我们的数据恰好只包含精确的事件时间和[右删失](@entry_id:164686)观测，Turnbull 估计量在数学上会简化，并给出与 Kaplan-Meier 估计量完全相同的结果。这表明 Kaplan-Meier 是处理不完全事件发生时间数据的一个更普适原理的特例 [@problem_id:4605653]。

### 使叙事复杂化：多种结局与重复故事
世界往往比单一的最终事件更为复杂。有时事件会一再发生，比如哮喘发作或再入院。在许多研究中，我们无法进行连续监测。我们可能只能在每次预定的门诊访视时获得事件的累积计数。这就产生了**面板计数数据**——一个在观测区间内的计数数据集。在这里，我们的未知程度更深；我们甚至不知道单个事件的子区间，只知道总数 [@problem_id:4834694]。针对这些数据的统计方法必须建立在观测到某个区间内特定*计数*的概率之上，而不是事件本身的时间点。

当一个故事有多种互斥的结局方式时，会出现另一个关键的复杂性。在一项癌症研究中，患者可能死于正在研究的癌症，也可能死于心脏病发作，或者死于车祸。这些就是**竞争风险**。如果我们想估计死于癌症的概率，就不能简单地将心脏病发作死亡视为“删失”。一个死于心脏病发作的人不再有死于癌症的风险，而这种从风险池中的移除绝非随机。

在时间 $t$ 之前经历类型 $k$ 事件的概率，即**累积发生率函数** $F_k(t)$，具有一个优美且富有启发性的结构。它取决于原因 $k$ 的瞬时速率（其**特定原因风险率**，$\lambda_k(u)$），但也取决于到那个时间点为止，从所有其他原因中存活下来的概率 ($S(u)$)。完整的关系是 $F_k(t) = \int_0^t \lambda_k(u) S(u) \, du$。这意味着死于癌症的概率不仅受[癌症侵袭](@entry_id:172681)性的影响，还受你先死于心脏病的可能性的影响。这个框架使我们能够将风险因素对特定疾病的直接“病因学”效应，与其对消耗风险人群的竞争事件的间接效应分离开来 [@problem_id:4579882]。

### 最后的警示：信息性删失的陷阱
几乎所有这些强大方法的基础都有一个关键的“无辜”假设：我们假设，在给定模型中的协变量的情况下，删失行为与个体的预后是独立的。这被称为**非信息性删失**。

但如果这个假设是错误的呢？考虑一项研究，其中测量了一个随时间变化的生物标志物，比如不断上升的病毒载量。假设这个生物标志物不仅对事件有预后作用（高值意味着病情更重），而且还使患者更有可能感到不适并退出研究。这就是**信息性删失**。被删失的个体不是一个随机样本；他们不成比例地是病情最重的那些人。如果我们使用标准的 Kaplan-Meier 或 Cox 模型（这些模型假设删失是非信息性的），我们剩下的样本会看起来比实际更健康。分析将会有偏倚，可能使无效的治疗看起来有益 [@problem_id:4985843]。

这不是一个致命的缺陷，而是要求我们采用更复杂方法的信号。统计学家已经开发出先进的方法来对抗这种陷阱。**删失概率逆加权 (IPCW)** 创建了一个伪总体，其中留在研究中的个体的贡献被加权放大，以代表那些被信息性删失的相似个体。**联合模型**采取了更为宏大的方法，在一个统一的框架内同时对生物标志物过程、事件过程和退出过程进行建模。这些方法提醒我们，在使用每一种统计工具时，我们都必须深刻地意识到其基本假设。进入事件时间数据的旅程，是一堂持续的课程，教我们如何从现实投射到我们记录上的那些不完整、不完美但又优美的影子中，得出稳健的结论。

