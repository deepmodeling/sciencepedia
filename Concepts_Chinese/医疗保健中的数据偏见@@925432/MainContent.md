## 引言
人工智能有望彻底改变医疗保健，提供前所未有的强大工具来诊断疾病、个性化治疗和优化护理。这些基于逻辑和数据构建的算法，通常被视为通往更客观、更高效医疗未来的途径。然而，一个关键的悖论已经出现：这些为追求客观性而设计的工具，可能会在无意中学习并放大最深层次的社会不平等，导致产生有偏见的结果，从而伤害它们本应服务的患者。这在人工智能的承诺与其实际应用之间造成了巨大差距，在现实世界中，一个工具的影响可能在不同的人口群体中存在系统性差异，从而加剧而非减轻不公正。

本文直面这一挑战，对医疗保健人工智能中的数据偏见进行了全面探讨，旨在让读者对该问题及其潜在解决方案有细致入微的理解。在第一章 **原理与机制** 中，我们将剖析[算法偏见](@entry_id:637996)的构成，追溯其从有缺陷的数据收集过程和使用不完美的代理变量，到模型训练的统计机制的起源。我们将定义它可能造成的关键伤害形式。随后，在 **应用与跨学科联系** 这一章中，将把这些原理应用于真实的临床场景，审视偏见如何在特定的工具和患者路径中表现出来。该章节还将重点介绍各种创新解决方案——涵盖统计技术、稳健的治理框架和法律改革——这些方案正为实现一个更公平、更值得信赖的人工智能驱动的医疗保健系统铺平道路。

## 原理与机制

要理解一个为客观性而设计的工具如何会延续不公，我们必须探其究竟。算法并非一个深不可测的魔法黑箱；它是一系列逻辑步骤，一个可以被理解、检查并在必要时进行修复的机制。就像物理学家追踪粒子从源头到其撞击点的路径一样，我们也可以追踪偏见从其在现实世界中的起源到其在临床决策中有害影响的路径。这段旅程并非为了谴责，而是为了发现，它揭示了社会、数据和计算之间深刻的相互作用。

### 机器中的幽灵：什么是[算法偏见](@entry_id:637996)？

想象一下，你正在建造一台用来分拣水果的机器。你给它看了成千上万个完美的、红色的、圆形的苹果作为例子。它出色地学会了这项工作。但接着，你让它分拣一篮混合水果。它可能会把一个红色的油桃标记为“苹果”，并丢弃一个完好但绿色的青苹果（Granny Smith）。这台机器并非对青苹果有偏见，它只是根据它所看到的那个不完整、不具代表性的“世界”来行动。

这就是医疗保健领域中**[算法偏见](@entry_id:637996)**的本质。它通常不是程序员恶意所为的结果。相反，它是一种系统性的、不公平的错误模式，是从本身就带有历史和持续存在的社会不平等回响的数据中学习而来的。当我们在此背景下讨论偏见时，我们必须精确。我们所指的并非科学家们所熟悉的*[统计估计](@entry_id:270031)偏差*，即估计量的[期望值](@entry_id:150961)与参数[真值](@entry_id:636547)之间的差异，如 $\mathrm{Bias}(\hat{\theta}) = \mathbb{E}[\hat{\theta}] - \theta$。那种偏差是估计过程本身的属性。

相反，我们关心的是另一回事：最终部署的模型对人们产生影响的一种属性。在医疗保健领域，当一个模型的性能或其造成的伤害在不同人群中存在系统性差异时，[算法偏见](@entry_id:637996)就显现出来了 [@problem_id:4849723]。如果一个用于预测心脏病发作的模型，在识别高风险女性方面的可能性低于识别具有完全相同潜在心脏状况的男性，那么这就是偏见。伦理上的相关问题是，该模型是否公平地分配了其错误和由此产生的伤害。

这一思想与一个关键的法律概念相吻合：**差异性影响**。一个算法可能是“表面中立的”——例如，它可能不直接使用患者的种族作为输入。然而，如果它使用了其他数据，比如患者的邮政编码（由于历史上的种族隔离，这可能是种族的强代理变量），并导致某个种族群体的结果系统性地更差，那么它就产生了差异性影响。这与**差异性对待**不同，后者是明确使用种族等受保护特征来区别对待人们 [@problem_id:4489362]。机器中的幽灵正是这种差异性影响，一个天真的算法会不假思索地学习并放大社会不平等的幽灵。

### 幽灵从何而来？从现实世界到数据的旅程

如果算法只是从数据中学习，那么要找到偏见的源头，我们必须审视数据本身。数据并非现实的完美镜子；它更像一张照片，从特定的角度、用特定的镜头拍摄，并且只捕捉了摄影师选择对准的景物。这种数据生成过程充满了潜在的陷阱。

#### 数据并非世界的全部：选择性偏见

首先要问的问题是：我们的数据集中包含了谁？通常，医疗保健数据并非从每个人身上收集，而是从那些以特定方式与医疗保健系统互动的人那里收集。以血清乳酸这项实验室检测为例，它是脓毒症的一个关键指标。医生不会为每位患者都开这项检测，而是在他们已经有所怀疑、患者看起来身体不适时才会开具 [@problem_id:4849724]。这意味着我们的乳酸值数据集并非人群的随机样本；它严重偏向于病情更重的患者。这种数据是我们所说的**[非随机缺失](@entry_id:163489) (Missing Not At Random, MNAR)**，因为数据缺失的原因（未开具检测）恰恰与我们试图预测的数值（患者的健康状况）相关。一个仅在这一预选群体上训练的模型，在被要求对普通患者群体进行预测时，其表现可能会很差。

#### 数据是扭曲的反映：特征偏见与标签偏见

即使对于我们数据中包含的患者，我们所测量的也往往不是我们真正关心的东西。我们被迫使用代理变量，而这些代理变量往往被我们希望避免的不平等所扭曲。这或许是数据偏见中最[隐蔽](@entry_id:196364)的机制，我们可以通过一个简单而优美的模型来理解它 [@problem_id:4824156]。

假设我们想预测一个患者的真实健康需求，称之为 $Y$。但真实需求是一个抽象概念，我们无法直接测量。因此，我们使用一个代理变量：他们实际发生的医疗费用 $\tilde{Y}$。这似乎很合理——病情越重的人使用的医疗服务越多，所以费用也越高。然而，一个人的医疗费用不仅是其需求的函数，也是其获得医疗服务*途径*的函数。让我们对此进行建模。假设一个患者获得医疗服务的途径 $H$ 是一个介于 0（无途径）和 1（完全畅通）之间的数字。那么，实际发生的费用可以建模为：

$$
\tilde{Y} = c \cdot H \cdot Y + \varepsilon
$$

在这里，$c$ 只是一个价格常数，$\varepsilon$ 是一些随机噪声。现在，让我们考虑两组患者，第 0 组和第 1 组。为论证起见，假设两组的平均真实健康需求 $Y$ 是相同的。但第 1 组是一个历史上服务不足的社区，因此他们获得医疗服务的平均途径远低于第 0 组（$\mathbb{E}[H|A=1] = 0.3$ 对比 $\mathbb{E}[H|A=0] = 0.8$）。

那么，测得的平均费用会是多少？一个群体的期望费用是 $\mathbb{E}[\tilde{Y}|A=a] = c \cdot \mathbb{E}[H|A=a] \cdot \mathbb{E}[Y]$。由于两组的 $\mathbb{E}[Y]$ 相同，算法将会看到第 1 组的平均费用远低于第 0 组。它将学到一个灾难性的错误结论：它会断定第 1 组的患者更健康，而实际上他们病情同样严重，只是在获得护理方面面临更大的障碍。

这是一个**结构效度错误** [@problem_id:4524553]。代理变量（$\tilde{Y}$，费用）并非预期结构（$Y$，需求）的有效度量。这同时造成了：
*   **标签偏见：** 我们试图预测的结果（高费用）是真实关注结果（高需求）的有偏见的代表 [@problem_id:4866413]。
*   **特征偏见：** 模型中使用的预测变量，如“前一年支出”或“门诊次数”，也同样被扭曲了。它们并非纯粹的健康度量，而是透过“可及性”这面滤镜过滤后的健康度量。

### 从坏数据到坏决策：放大与部署

偏见的旅程并不会随着数据而结束。我们在构建和部署算法时所做的选择可能会放大这些潜在问题。

一个旨在最大化整体准确率的算法，会自然而然地专注于在多数群体上做对。如果一个少数群体规模较小，或者其数据中存在不同的统计模式（由于我们刚才看到的代理变量问题），算法可能会学到，通过牺牲在该少数群体上的准确率，可以获得更高的总分。这是训练过程本身引入的一种**[算法偏见](@entry_id:637996)**——一种多数群体的统计暴政 [@problem_id:4866413]。

此外，一旦模型建成，其使用方式至关重要。想象一下，我们那个有偏见的费用预测模型被用来决定谁能在一个名额有限的护理管理项目中获得一席之地。假设任何预测风险评分高于 $0.7$ 的人都能入选。由于该模型对于真实需求水平相同的患者，会系统性地为来自低可及性群体的患者分配较低的风险评分，这个单一阈值将系统性地剥夺他们获得该资源的机会。这就是**部署偏见** [@problem_id:4866413]。其中一个关键方面是**校准**。如果一个风险评分（比如 $0.8$）对应着 80% 的事件发生概率，并且这与患者所属的群体无关，那么这个模型就是良好校准的。如果这个评分对于不同群体意味着不同的东西，那么单一阈值本身就是不公平的 [@problem_id:4567584]。最后，临床医生自己也可能成为**自动化偏见**的受害者，他们过分相信计算机给出的“客观”数字，而忽略了自己认为患者需要帮助的临床直觉，从而进一步将算法的错误逻辑固化到工作流程中 [@problem_id:4824163]。

### 量化不公：公平性的语言

要解决一个问题，我们必须能够衡量它。算法公平性领域已经发展出一种精确的语言来做到这一点。让我们看一个具体的例子。一个模型被构建用于预测谁会患上糖尿病，并设定一个阈值来决定谁能获得预防性护理。一项审计揭示了以下情况 [@problem_id:4567584]：

*   对于 A 组（多数群体）：在 100 名真正需要干预的人中，有 80 人被正确识别。
*   对于 B 组（少数群体）：在 100 名真正需要干预的人中，只有 60 人被正确识别。

我们在此关注的指标是**真正例率（True Positive Rate, TPR）**，或称灵敏度：即一个需要帮助的人实际被提供帮助的概率。在这里，$TPR_A = 0.80$ 而 $TPR_B = 0.60$。这种不平等违反了一个被称为**[机会均等](@entry_id:637428)**的关键公平性标准。该标准规定，在需要帮助的条件下，获得帮助的机会应该对所有群体都是相同的。这里深重的不公显而易见：一个人获得潜在救生干预的机会，取决于其群体身份，而不仅仅是其需求。

### 伤害的两个方面：分配与代表

这些有偏见的系统造成的伤害不仅仅是统计上的，它们是深刻关乎人性的。我们可以认为它们有两个方面 [@problem_id:4862115]。

第一种是**分配性伤害**。这是[资源分配](@entry_id:136615)不公所造成的伤害。Rivera 女士是一位患有多种慢性病的老年人。由于她过去的医疗支出很低（这是她需求的劣质代理变量），一个算法拒绝了她在护理协调项目中的名额。她被拒绝了有形的资源，因为系统未能看到她真正的需求。这是我们刚刚讨论的真正例率不等的直接后果。

第二个更微妙的方面是**代表性伤害**。这是错误识别、刻板印象和从属化所造成的伤害。Johnson 女士是一位黑人怀孕患者，由于需要照顾家人和交通不便而错过了预约。一个不了解这些背景的算法将她标记为“不依从”。一位受此标签影响的临床医生在病历中将她描述为“不遵从医嘱”。这个标签（最初）并没有剥夺她的资源。相反，它通过用一种个人失败的污名化叙事，取代了她处境的真相——一个负责任的人在与结构性障碍作斗争——从而对她造成了伤害。它强化了有害的刻板印象，并损害了治疗关系中至关重要的信任。

理解分配性伤害和代表性伤害揭示了全部的伦理风险。修复[算法偏见](@entry_id:637996)不仅仅是纠正统计上的不平衡；它关乎在如何分配护理中恢复正义，并肯定我们服务的每个人的尊严。这引出了我们最后一个、也是至关重要的悖论。为确保我们的系统不会因种族或残疾而产生差异性影响，我们不能对这些属性视而不见。“通过无知实现公平”——即简单地不告诉算法患者的种族——是一个谬论，因为像邮政编码这样的代理变量携带了同样的信息。要审计我们的模型是否符合[机会均等](@entry_id:637428)，检查各群体的真正例率是否相同，我们必须（根据定义）使用这些群体数据。要治愈编码在我们系统中的不平等，我们必须首先有勇气去正视它们 [@problem_id:4429846]。如何负责任且合乎伦理地做到这一点，是我们接下来要面对的巨大挑战。

