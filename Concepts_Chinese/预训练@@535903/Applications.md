## 应用与跨学科联系

在了解了[预训练](@article_id:638349)的原理和机制之后，我们可能感觉自己刚刚学会了一款全新而强大的游戏的规则。我们理解了策略、目标和潜在的陷阱。但任何伟大科学思想的真正魔力、真正的美，不仅仅在于知道规则，而在于看它如何在世界中发挥作用。这个“先学习再学习”的抽象概念究竟能把我们带到哪里？事实证明，答案是几乎无处不在。

一个[预训练](@article_id:638349)模型从通才到专家的历程，与进化生物学中的一个概念——**[功能变异](@article_id:350010)（exaptation）**——有着惊人的呼应。在自然界中，为某种目的（比如羽毛用于[体温调节](@article_id:307751)）进化出的性状，可以被借用并重新用于一个全新的、壮观的功能，比如飞行。原始结构并未被抛弃；它被调整、精炼和构建。同样，一个[神经网络](@article_id:305336)在花费大量计算资源学习了图像、文本甚至基因组的通用“语法”之后，就成了一个极其强大的起点，可以用于新的、专门的任务 [@problem_id:2373328]。它已经完成了学习如何看或如何读的艰苦工作；现在，我们只需要为特定目的微调其能力。本章将是对这些“[功能变异](@article_id:350010)”的一次巡礼，展示了[预训练](@article_id:638349)这个单一而优雅的思想如何成为连接不同科学和工程领域的统一线索。

### 锐化我们的感官：从像素到篇章

让我们从一个熟悉的世界开始：我们自己感官的世界。多年来，训练计算机视觉模型的黄金标准是使用像 ImageNet 这样的大型、手动标注的数据集。这是[监督学习](@article_id:321485)的巅峰之作，但它对人力有着无尽的渴求。[预训练](@article_id:638349)提供了另一条道路。通过使用自监督目标——一些巧妙的游戏，让模型从数据本身学习，比如填补图像中缺失的色块——我们可以在互联网上几乎无限的无标签图像海洋中进行训练。结果呢？以这种方式学到的特征如此丰富和稳健，以至于一个在无标签数据上[预训练](@article_id:638349)的模型，在微调后，其在[物体检测](@article_id:641122)等任务上的表现往往能超过一个在大型标注数据集上[预训练](@article_id:638349)的模型 [@problem_id:3146124]。似乎通过迫使模型自己学习视觉世界固有的结构，我们为其配备了一种比简单地告诉它一百万次“这是一只猫”更根本、更通用的理解力。

同样的原则也适用于语言。当我们要求一个[模型检测](@article_id:310916)网络上的虚假评论时，我们实际上是在要求它掌握细微差别、上下文和文体特征。一个在浩瀚的互联网语料库上[预训练](@article_id:638349)的模型已经学会了人类语言的节奏和韵律。然而，产品评论的语言可能有其自己的“方言”。在这里，我们看到了另一层复杂性：**[领域自适应](@article_id:642163)[预训练](@article_id:638349)**。简单地使用一个通用语言模型效果不错，但在教它检测虚假评论这一特定任务之前，先在一个评论风格的文本语料库上进一步微调，会产生更好的性能。这个过程提高了模型区分真实和虚假评论“得分分布”的能力，从而带来了更高的 ROC 曲线下面积（AUC）——这是对其分类能力的一个直接衡量 [@problem_id:3167129]。[预训练](@article_id:638349)不是一次性的技巧；它是一个渐进专业化的过程。

### 解码生命与物质的蓝图

也许[预训练](@article_id:638349)最令人叹为观止的应用超出了日常图像和文本的范畴。如果我们能将这些学习原则应用于科学本身的语言，会怎么样？

考虑基因组。脱氧[核糖核酸](@article_id:339991)（DNA）在某种意义上是一种用 A、C、G、T 四个字母写成的语言。它的“句子”和“段落”支配着整个生命的机器。通过将整个人类基因组视为一篇巨型文本，我们可以像训练“DNA-BERT”一样，使用与人类语言相同的[掩码语言建模](@article_id:641899)目标来[预训练](@article_id:638349)一个模型。模型学习 DNA 的统计模式和“语法”。这个[预训练](@article_id:638349)的基础非常强大。只需少量带标签的样本，就可以对其进行微调，以惊人的准确性定位[启动子](@article_id:316909)等特定调控元件的位置。这种知识迁移之所以有效，是因为模型学到了关于 DNA 序列的通用、可复用的特征，从而大大减少了特定任务所需的带标签数据量——这是[样本效率](@article_id:641792)提高的一个绝佳例证 [@problem_id:2429075]。

我们可以将这个想法从生命密码推向物质定律。在[量子化学](@article_id:300637)中，预测分子内的能量和力是一项计算量极其巨大的任务，传统上需要庞大的超级计算资源。[神经网络](@article_id:305336)能否学习底层的[势能面](@article_id:307856)？通过在一个庞大的[量子化学](@article_id:300637)计算数据库上进行[预训练](@article_id:638349)，模型确实可以学到一个通用的“[神经网络势](@article_id:351133)”。这个模型吸收了各种有机分子原子间相互作用的基本物理原理，然后只需少量数据进行微调，就可以对一个新的、特定的分子家族做出极快且高度准确的预测 [@problem_id:2903813]。为了实现这一点，科学家们甚至开发了像弹性权重固结（EWC）这样的技术，这是一种正则化形式，可以防止模型在适应新数据时“忘记”在[预训练](@article_id:638349)期间学到的基本物理知识——这一挑战被称为[灾难性遗忘](@article_id:640592)。

这种数据驱动学习与物理定律之间的协同作用也延伸到了经典工程领域。想象一下，要预测[喷气发动机](@article_id:377438)涡轮叶片内部复杂的肋状通道中的热传递。精确的物理过程很复杂，但我们有经过充分检验的近似关联式，通常是[幂律](@article_id:320566)形式，如 $\mathrm{Nu} = C \cdot \mathrm{Re}^{a} \cdot \mathrm{Pr}^{b}$。我们可以设计一个其结构反映这一物理定律的[代理模型](@article_id:305860)。然后，我们可以在一个简单、易于理解的系统（如光滑平板）上[预训练](@article_id:638349)这个模型。这个已经学习了[对流](@article_id:302247)基本尺度律的[预训练](@article_id:638349)模型，随后只需利用来自复杂肋状通道的少量数据点进行微调，就能产生一个比在同样小的数据集上从头训练的模型精确得多的预测器 [@problem_id:2502983]。在这里，[预训练](@article_id:638349)充当了一座桥梁，使得从一个简单的、理想化的物理系统获得的知识能够加速在复杂、真实世界系统中的学习。

### 机器中的幽灵：实践与风险

这种不可思议的力量并非没有代价。有效地运用它需要对实践细节有深入的理解，并对其潜在后果有深刻的责任感。

微调的过程本身就是一门艺术。优化景观——一个代表[模型误差](@article_id:354816)的高维山丘和山谷地带——从[预训练](@article_id:638349)时宽阔平滑的盆地，急剧变为特定任务下陡峭狭窄的山谷。驾驭这片地貌的策略也必须改变。一个平滑的、指数衰减的[学习率](@article_id:300654)可能非常适合[预训练](@article_id:638349)的探索阶段，但“[阶梯式衰减](@article_id:640323)”——即学习率保持恒定然后急剧下降——通常对于快速稳定到微调任务所需的精确最小值更为有效 [@problem_-id:3176526]。

此外，随着这些模型变得越来越大，提高它们的效率成为一个关键的工程挑战。在这里，[预训练](@article_id:638349)也提供了一个优势。通过在[预训练](@article_id:638349)期间加入一个简单的 $L_2$ 正则化（或“[权重衰减](@article_id:640230)”）项，我们鼓励模型找到权重较小的解，将其所学分散到许多参数上，而不是依赖少数几个非常大的参数。这个看似微不足道的选择带来了重大的下游好处：以这种方式[预训练](@article_id:638349)的模型对**剪枝**（一种移除小量级权重以创建更小、更快模型的过程）具有更强的鲁棒性。结果是在[稀疏性](@article_id:297245)和准确性之间取得了更好的权衡，使我们能够将这些巨型[模型提炼](@article_id:343241)成更适合部署的实用模型 [@problem_id:3141357]。

然而，我们也必须小心翼翼。[预训练](@article_id:638349)期间做出的设计选择可能会产生意想不到的后果。考虑一个用大量“颜色[抖动](@article_id:326537)”增强进行[预训练](@article_id:638349)的模型，其中图像的颜色被随机改变。这迫使模型变得对颜色不敏感，而专注于形状和纹理，这通常是可取的。但是，当我们将这个模型微调用于一个依赖细微颜[色差](@article_id:353872)异的任务时会发生什么？我们精心设计的这种[不变性](@article_id:300612)现在变成了一种障碍，一种降低性能的偏见。这说明了一个“没有免费午餐”的原则：在[预训练](@article_id:638349)期间植入模型的归纳偏见必须与下游任务的需求相一致 [@problem_id:3129335]。

然而，最严重的问题是社会性的：隐私。这些模型通过内化训练数据的模式来学习。如果处理不当，这个过程可能会越过界限，变成记忆。特别是微调，它会反复向模型展示一个小数据集，可能会增加其存储该数据特定细节的风险。这为**[成员推断](@article_id:640799)攻击（MIA）**打开了大门，攻击者可以通过查询模型，以高于随机猜测的准确率来判断某个特定个人的数据是否是训练集的一部分 [@problem_id:3149369]。随着这些模型被部署到医疗保健和金融等敏感领域，理解和减轻这种[信息泄露](@article_id:315895)不仅是一个技术挑战，更是一项伦理责任。

### 宏伟的挑战：通用的罗塞塔石碑？

展望未来，我们看到[预训练](@article_id:638349)的雄心正在向其逻辑终点扩展：创建一个通用的“基础模型”。例如，我们能否构建一个单一的[图神经网络](@article_id:297304)，理解所有化学领域的语言？一个能够以同等的流畅度预测小药物分子、巨大蛋白质和周期性晶体的性质，甚至生成新颖、有效的化学结构的模型 [@problem_id:2395467]？

挑战是巨大的。这样的模型需要内在尊重物理学的[基本对称性](@article_id:321660)，对[三维旋转](@article_id:308952)和平移具有[等变性](@article_id:640964) [@problem_id:2395467]。它必须克服局部[消息传递](@article_id:340415)的局限性，以捕捉支配分子相互作用的长程力 [@problem_id:2395467]。它需要用一系列复杂的自监督目标进行训练，以便从异构和稀疏标记的数据中学习 [@problem_id:2395467]。并且，它的生成能力必须受到化学价态硬性规则的约束，以确保其创造物在物理上是可能的 [@problem_id:2395467]。

对通用模型——一块解读自然模式的罗塞塔石碑——的追求，是[预训练](@article_id:638349)的前沿。它证明了一个简单思想的力量：通过学习一个世界的普遍结构，我们能以不可估量的方式更好地装备自己，去理解其具体的奇迹。从屏幕上的像素到恒星中的原子，学习的原则为我们观察宇宙提供了一个深刻而统一的视角。