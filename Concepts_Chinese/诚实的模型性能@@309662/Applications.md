## 应用与跨学科联系

我们花时间学习了构建预测模型的原理和机制，这个过程类似于一个学生勤奋地学习课本和笔记。这个学生可能在回答他已经见过的问题上变得非常出色，在从学习材料中抽取的模拟考试中获得满分。但他真的*学懂*了这个科目吗？真正的考验——有着全新问题的期末考试——是唯一能知道答案的方法。在科学和工程的世界里，宇宙就是我们的期末考试。我们的模型必须能够泛化到它们训练数据之外，才能有任何实际用途。确保它们能做到这一点的过程，就是对*诚实模型性能*的追求。

这种追求不仅仅是一项技术性的杂务；它是一次深刻的科学之旅，几乎跨越了所有可以想象的学科。它迫使我们面对数据中隐藏的结构、复杂性的诱人魅力，以及我们自身视角中的微妙偏见。让我们踏上这段旅程，看看这些原则在现实世界中是如何生动体现的。

### 随机性的幻觉：当数据具有隐藏结构时

统计学中最简单的假设是，我们的数据点是*[独立同分布](@article_id:348300)*（i.i.d.）的——就像从一个非常大、混合均匀的袋子里一个接一个地取出弹珠。像$k$折交叉验证这样的标准技术，它随机打乱并划分数据，完全建立在这个假设之上。但如果弹珠不是独立的呢？如果它们被家庭、时间或地理的隐藏线索联系在一起呢？忽略这些线索是自欺欺人最快的方法之一。

#### 数据的社交网络：分组数据和[聚类](@article_id:330431)数据

想象一下，开发一个模型来从10秒的[心电图](@article_id:313490)（ECG）条带中检测[心律失常](@article_id:357280)。我们的数据集包含1000个条带，但它们仅来自100位不同的患者，每位患者10个条带。如果我们为5折[交叉验证](@article_id:323045)随机打乱所有1000个条带，会发生什么？对于任何给定的患者，他们的一些ECG条带几乎肯定会落入训练集，而另一些则落入[验证集](@article_id:640740)。这样，模型就在一个它某种意义上已经“见过”的患者身上进行了测试。由于每个个体都有独特的生理基线，模型可以轻易地学会识别患者的特征，而不是[心律失常](@article_id:357280)的普遍迹象。这导致了一个极其乐观的准确率估计。当模型最终部署到一个真正的新患者身上时，其性能会急剧下降。这种折间的[数据泄露](@article_id:324362)造成了一种可测量的“乐观偏见”，使得我们的验证结果成为一种幻想[@problem_id:1912488]。

解决方案在原则上很简单：尊重结构。这里的“独立单元”是患者，而不是ECG条带。我们必须执行*分组*划分，确保来自单个患者的所有数据完全存在于一个折中——要么用于训练，要么用于测试，但绝不能同时存在于两者之中。

这个原则是普遍适用的。在[量子化学](@article_id:300637)中，当预测分子的性质时，我们可能对每个分子的许多不同空间构型，即*构象异构体*，拥有数据。随机打乱所有构象异构体将是一个错误，因为它们都只是同一化学实体的不同姿态。为了测试[模型泛化](@article_id:353415)到*新分子*的能力，我们必须按分子ID对我们的划分进行分组[@problem_id:2903800]。在[系统免疫学](@article_id:360797)中，当分析数十万个单个细胞时，我们必须记住它们来自少数人类捐赠者。要构建一个能够诊断新患者疾病的分类器，我们的交叉验证必须按*捐赠者*划分，而不是按细胞划分[@problem_id:2892433]。在所有这些领域，从医学到物理学再到生物学，教训都是一样的：理解你数据的社交网络，不要拆散“家庭”。

#### 时间之箭：序列数据

我们的数据可能具有的另一种结构是时间。世界上的事件按序列展开，时间之箭只朝一个方向流动。如果我们正在构建一个模型，根据例如基因专利申请等特征来预测金融指数，我们的数据就是一个时间序列。如果我们在这里使用标准的随机交叉验证会发生什么？我们可能会用2022年的数据来训练模型，以预测2020年的结果。这太荒谬了；模型接触到了未来！这种“前视偏见”完全使评估无效。模型可能看起来具有非凡的预测能力，但其性能是建立在因果关系违背之上的幻觉[@problem_id:2383450]。

对于[时间序列数据](@article_id:326643)，我们的验证必须模仿现实。我们必须使用像*前向移动*或*滚动窗口*评估这样的技术。我们将模型训练到某个时间点，比如说2020年底，然后在下一个时期，即2021年1月进行测试。然后我们向前滑动窗口，用截至2021年1月的数据进行训练，并在2021年2月进行测试，依此类推。这严格地模拟了模型在真实世界中的使用方式。

在计量经济学等领域，更高级的应用甚至走得更远。在分析波动的金融序列时，我们不仅必须使用滚动窗口，还必须考虑到系统的潜在动态可能会随时间变化（参数不稳定性）。一个复杂的评估会涉及在每个滚动窗口中重新估计模型，并使用对波动性变化具有稳健性的统计检验来检查模型的预测准确性是否随时间保持稳定[@problem_id:2378216]。

#### 土地法则：空间数据

第三种基本结构是空间。想象一下，在一片小鼠大脑切片上绘制基因表达图谱。我们有来自数千个微小、空间分辨的斑点的数据。我们很快发现，彼此靠近的斑点往往具有相似的基因表达谱——这种现象称为*[空间自相关](@article_id:356007)*。一个斑点的邻居并不独立于它。如果我们使用随机划分，一个测试斑点将被高度相关的训练斑点包围。模型可以简单地通过查看其邻居来高精度地“预测”测试斑点的值，而没有学到任何关于将组织结构与基因表达联系起来的有意义的生物学知识[@problem_id:2752899]。

同样，我们必须设计一个尊重数据性质的验证方案。这里的解决方案是*空间区块交叉验证*。我们不能只按单个斑点划分。我们必须将整个组织图谱划分为大的、连续的区块。我们保留一个完整的区块用于测试，并在其他遥远的区块上进行训练。为了更加严谨，我们应该在测试区块周围创建一个“[缓冲区](@article_id:297694)”，并将该缓冲区中的斑点从训练集中排除，以确保任何训练点和测试点之间的最小距离超过空间相关的范围。只有这样，我们才能确信我们的模型正在学习普遍原则，而不仅仅是在邻近点之间进行插值。

### 复杂性的塞壬之歌：[维度灾难](@article_id:304350)

到目前为止，我们一直关注数据点*之间*的结构。但是它们*内部*的结构呢？现代数据集通常是高维的，每个观测值都有数千甚至数百万个特征。一个[算法交易](@article_id:306991)模型可能使用数百个技术指标；一个基因组学研究可能测量20,000个基因的表达。认为“数据越多总是越好”并把我们拥有的每一个特征都扔给模型是很诱人的。这是一个陷阱。

随着我们添加更多特征（维度），特征空间的“体积”呈指数级增长。我们固定数量的数据点变得稀疏，就像浩瀚星系中几颗孤独的星星。“局部邻域”的概念失效了；每个点都远离其他任何点。在这个巨大、空旷的空间里，找到仅仅是随机噪声的“模式”变得异常容易。一个灵活的模型会抓住训练数据中的这些[虚假相关](@article_id:305673)性，完美地拟合噪声。这解释了为什么增加更多特征会改善样本内拟合，而样本外性能却逐渐变差的悖论。这就是*[维度灾难](@article_id:304350)*[@problem_id:2439742]。

这种现象可以从另一个角度来看：[多重假设检验](@article_id:350576)。当一位计量经济学家在数千种不同的模型设定中搜索（例如，尝试20个控制变量的所有可能组合）以找到一个产生“显著”结果的模型时，他正在从事通俗所说的“$p$值操纵”。如果你进行足够多的测试，你几乎肯定会纯粹偶然地找到一个统计上显著的相关性，即使没有真正的效应存在。例如，在20个控制变量的所有子集中搜索意味着进行超过一百万次测试。如果我们的显著性阈值为$0.05$，偶然发现至少一个“显著”结果的概率几乎是100%[@problem_id:2439719]。

应对这塞壬之歌的方案是克制和纪律。首先是*[简约原则](@article_id:352397)*，或称奥卡姆剃刀原则：如无必要，勿增实体。在比较几个模型时，我们不应盲目选择样本内拟合最好的那一个。我们必须使用像赤池[信息准则](@article_id:640790)（AIC）或[贝叶斯信息准则](@article_id:302856)（BIC）这样的正式工具，它们会对复杂性进行惩罚。通常，一个样本内拟合稍差但更简单的模型，会具有更好、更稳健的样本外性能[@problem_id:2501919]。第二个，也是更强大的解决方案是*预先指定*。我们必须在查看数据*之前*定义我们的模型和分析计划，致力于一个单一的、有充分理由的方法。这种纪律防止我们在高维的[分岔](@article_id:337668)路径花园中徘徊，被虚假的发现所愚弄[@problem_id:2439719]。

### 纵览全局：透镜自身的偏见

最后，有时最深的偏见不在于我们模型的[算法](@article_id:331821)，而在于我们选择收集的数据和我们选择重视的指标本身。我们的数据集不是一扇通往世界的完美窗户；它是一个透镜，而每个透镜都有扭曲。

#### 幸存者的胜利

在[材料科学](@article_id:312640)中，研究人员可能会训练一个模型来预测一种新的化合物是否可以被合成。一个常见的方法是在一个包含所有已知、成功创造的材料的数据库上训练模型。但这个数据集存在根本性的偏见。它只包含“幸存者”。它没有告诉我们关于那些曾尝试但失败了的，或者在计算上被提出但被认为太不稳定而甚至没有尝试的庞大化合物宇宙的任何信息。一个在这种数据上训练的模型可能会非常擅长识别现有材料的共同属性，但它可能完全不知道如何区分一个有前途的新候选物和一个无望的候选物。当在一个包含可合成和不可合成化合物的更具代表性的集合上评估时，它的准确性可能与在有偏见的“幸存者”集合上测量的准确性截然不同——而且通常更差[@problem_id:1312332]。通往诚实评估的道路是丰富我们的数据集，积极寻找并包含“失败案例”，以便让我们的模型看到现实的全貌。

#### 对公平性的追求

也许诚实评估最关键的应用在于具有直接人类影响的领域，例如临床医学。想象一个预测患者疾病风险的模型。我们可能会发现它有非常出色的总体准确率，比如说90%。但如果这90%的准确率是一个群体95%准确率和另一个代表性不足的少数群体仅75%准确率的平均值呢？一个总体指标掩盖了一个严重且可能有害的失败。这个模型对每个人来说并非同样有用。

测试这种性能偏见需要一个异常严格的协议。仅仅检查一个指标是不够的。在一个独立的、保留的验证集上，我们必须按群体分解性能，并评估模型的多个方面：它区分病例与[对照组](@article_id:367721)的能力（区分度，通过$\text{AUROC}$衡量）对所有群体是否相同？它的风险预测对所有群体是否同样校准良好？在给定的决策阈值下，它在不同群体中是否具有相同的[真阳性率](@article_id:641734)和[假阳性率](@article_id:640443)？回答这些问题需要一个预先指定的分析计划，使用置信区间进行仔细的统计检验，并对多重假设进行校正。这是一个具有深远伦理意义的技术挑战。确保我们的模型不仅准确而且公平，是诚实评估的最终目标之一[@problem_id:2406433]。

### 诚实科学家的工具箱

从[量子化学](@article_id:300637)的亚原子世界到人类基因组的浩瀚，诚实模型评估的原则是一条统一的线索。这段旅程揭示，我们最大的挑战往往是我们自己被愚弄的能力——被隐藏的结构、被诱人的复杂性以及我们数据收集中的盲点所愚弄。

一项现代、复杂的[系统免疫学](@article_id:360797)研究可能需要一个嵌套的、分组的[交叉验证](@article_id:323045)方案，该方案尊重捐赠者级别的数据结构，严格在每个训练折内执行所有预处理和[特征选择](@article_id:302140)以防止泄露，使用捐赠者级别的平均值来公平评估性能，并且完全预先指定以防止$p$值操纵[@problem_id:2892433]。这样的协议是我们所讨论的一切的美妙综合。它不仅仅是一份程序清单；它是在大数据时代科学方法的体现。这种有纪律的、严谨的对诚实的追求，使我们能够将数据转化为真正的、可靠的知识，从而改变世界，而不是炼成愚人金。