## 引言
深度学习与物理科学的融合正在激发科学发现的新[范式](@article_id:329204)。虽然机器学习在海量数据集中发现模式的能力令人惊叹，但传统模型通常是“黑箱”，对支配宇宙的基本定律一无所知。反之，我们基于数百年知识建立的物理模型，通常只是对更复杂现实的一种近似。本文旨在弥合这两个世界之间的鸿沟，探索一种强大的综合方法，即教会[深度学习](@article_id:302462)模型像物理学家一样“思考”。

在接下来的章节中，我们将踏上一段从[第一性原理](@article_id:382249)到实际应用的旅程。首先，在“原理与机制”一章中，我们将深入探讨机器如何学习物理学。我们将探索对称性、能量最小化和守恒定律等概念如何不仅仅是抽象理念，而是可以直接编码到神经网络的架构和训练过程中。然后，在“应用与[交叉](@article_id:315017)学科联系”一章中，我们将见证这些物理知识启发的模型在实际应用中的表现，它们正在彻底改变从工程学、[材料科学](@article_id:312640)到生物学等多个领域。读者将全面理解数据与物理定律之间的这种对话是如何为科学研究打造出一套新工具的。

## 原理与机制

那么，机器是如何学习物理学的呢？这个过程不像学生背诵公式，更像是一位学徒工匠在培养对其材料的直觉。机器会接收大量的示例——比如，数百万种不同分子的原子排布及其对应的能量，这些能量是用量子力学极高的精度计算出来的。它的目标是创建自己的内部模型，在看到一个新的、未曾见过的原子排布时，能够预测其能量。

这个学习过程是一场旅程，和所有伟大的旅程一样，它由基本原理引导。这些原理并非计算机科学的任意规则；引人注目的是，它们正是支撑物理世界本身的原理：对称性、守恒以及能量最小化。

### 沿[能量景观](@article_id:308140)下降

机器学习模型的核心是一个数学景观，一个由山丘和山谷构成的抽象地形。这被称为**[损失景观](@article_id:639867) (loss landscape)**。对于模型内部参数的每一种可能配置——我们称之为**权重 (weights) 和偏置 (biases)** 的旋钮和刻度盘——在这个景观上都有一个对应的高度。这个高度，即“损失”，衡量了模型表现得有多差。高损失意味着模型的预测与训练数据中的真实值相去甚远；低损失则意味着它正在接近真实值。

训练过程就是一种下降行为。模型从这个高维山脉的某个随机点开始，一步步地试图找到最深的山谷。它如何知道哪条路是向下的？它会计算当前位置最陡的斜率——即**梯度 (gradient)**——然后朝相反方向迈出一小步。这个简单而强大的[算法](@article_id:331821)被称为**[梯度下降](@article_id:306363) (gradient descent)**。

如果这对物理学家来说听起来很熟悉，那理应如此。这与物理系统演化以寻找其最小能量状态的方式完全类似。想象一个在丘陵表面滚动的球，它总是会朝着势能下降最快的方向移动。这样一个系统的动力学可以用**梯度流 (gradient flow)** 来描述，其中球的速度与势能函数的负梯度成正比。对这个物理系统应用一个简单的数值模拟方法，比如[前向欧拉法](@article_id:301680)，其数学形式与机器学习中使用的[梯度下降](@article_id:306363)[算法](@article_id:331821)完全相同 [@problem_id:2172192]。因此，当我们训练一个神经网络时，我们可以把它想象成一个物理系统，正在稳定到其最稳定、能量最低的构型，这里的“能量”是其“错误程度”的一种度量。

模型通过调整其参数来优化预测，以最小化一个由**[交叉熵](@article_id:333231) (cross-entropy)** 等度量量化的“意外因子”。如果我们试图将粒子碰撞事件分为三类，并且有一个可信的物理理论能给出真实概率，我们就可以调整我们简单模型的参数，使其[概率分布](@article_id:306824)尽可能地与真实分布相匹配。最小化[交叉熵](@article_id:333231)能确保我们的模型对真实结果“不再那么意外”，从而有效地使其世界观与物理现实保持一致 [@problem_id:1615207]。

### 物理学的语言：教会机器对称性

在机器开始学习之前，我们必须决定如何向它描述世界。这不是一项简单的任务。物理学不仅仅是事实的集合；它是一套遵循深刻对称性原理的定律。物理定律不应依赖于你在宇宙中的位置（平移对称性）、你面对的方向（[旋转对称](@article_id:297528)性），或者你如何标记相同的粒子（[置换对称性](@article_id:365034)）。如果我们的机器学习模型要学习物理学，它们就必须使用这种对称性的语言。

#### 不变性：什么是不变的

想象一下你正在使用原子力显微镜测量一种材料的硬度。硬度是材料的内在属性，它不会因为你旋转样品，或者在实验室的不同角落测量而改变。硬度的*标量*值在这些变换下是**不变的 (invariant)**。然而，你收集的原始数据——你施加的力矢量、材料内部的应力和应变张量——*确实*会改变。当你[旋转坐标系](@article_id:349521)时，矢量的分量会改变。[张量](@article_id:321604)的分量以一种更复杂但同样明确的方式进[行变换](@article_id:310184)。如果你只是将所有这些变化的分量输入模型，你就会迫使模型去解决一个更难的问题：它必须首先学习[坐标变换](@article_id:323290)的规律，然后再设法“逆变换”数据，以找到你关心的不变属性。这样做效率低下，并会导致泛化能力差。

更优雅的解决方案是从一开始就将对称性构建进去。与其将力矢量 $\mathbf{F}$ 的分量输入模型，我们可以给它矢量的模长 $\|\mathbf{F}\|$，这是一个标量，也是一个[旋转不变量](@article_id:349651)。与其提供应力张量 $\boldsymbol{\sigma}$ 的九个分量，我们可以提供它的**[主不变量](@article_id:372469) (principal invariants)**（比如它的迹和[行列式](@article_id:303413)），这些也是在旋转下不变的标量。通过将我们的[数据预处理](@article_id:324101)成自身就是[不变量](@article_id:309269)的特征，我们就将一个基本物理原理[嵌入](@article_id:311541)到学习过程中，使模型更稳健、数据效率更高 [@problem_id:2777646] [@problem_id:1312296]。例如，如果一个在多种材料上训练的模型在处理含碲（Tellurium）等[重元素](@article_id:336210)的化合物时系统性地失败，这是一个警示信号。模型可能过高地预测了[带隙](@article_id:331619)，因为其简单的、基于平均属性的特征无法捕捉**自旋轨道耦合 (spin-orbit coupling)** 的复杂物理学——这是一种在重元素中显著的[相对论](@article_id:327421)效应，通常会*减小*[带隙](@article_id:331619)。模型看到了一个物理现象，但其语言过于简单而无法描述，这个问题源于其训练数据和描述能力的不足 [@problem_id:1312296]。

#### [等变性](@article_id:640964)：可预测的变化

有些物理量不是不变的；它们以一种可预测的方式变换。力是矢量。如果你旋转一个分子，每个原子上的力也会随之旋转。一个预测力的模型不应该是不变的；它应该是**等变的 (equivariant)**。这意味着如果你变换输入（旋转分子），输出（力矢量集合）会以相应的方式变换。形式上，如果一个对称操作 $g$（比如旋转 $Q$ 和平移 $t$）作用于原子构型输入 $X$，且模型是一个函数 $f$，那么一个等变模型满足 $f(g \cdot X) = D(g) f(X)$。这里，$D(g)$ 是作用于输出的[对称操作的表示](@article_id:380708)。对于力而言，$D(g)$ 就是[旋转矩阵](@article_id:300745) $Q$ [@problem_id:2784668]。将[等变性](@article_id:640964)直接构建到神经网络的架构中——创建所谓的 **E(3)-[等变神经网络](@article_id:297888)**——是现代研究的一个前沿。这些模型不需要从数据中*学习*旋转和平移的规律；这些规律被硬编码到它们的结构中，就像它们被编码在[时空](@article_id:370647)的结构中一样。这使得它们在学习分子和材料等物理系统时异常强大和高效。

#### 表示的艺术

即使正确处理了对称性，如何表示一个物理系统仍然是一门艺术。想象一下，你想预测一个分子的生成能。你可以使用**库仑矩阵 (Coulomb matrix)** 来表示这个分子，这是一个 $N \times N$ 的矩阵，包含了所有原子对之间的静电排斥项。这种表示信息丰富，捕捉了所有的成对相互作用。然而，它的计算成本很高（如果使用其[特征值](@article_id:315305)来确保[置换对称性](@article_id:365034)，其计算复杂度为 $O(N^3)$），而且更成问题的是，它不具有**尺寸广延性 (size-extensive)**。两个不相互作用的分子的能量应该是它们各自能量的总和。但是，组合系统的库仑矩阵并不是简单地将单个矩阵相加或拼接。这种表示方式没有尊重它试图预测的属性的加和性。作为替代方案，你可以使用**键袋 (bag-of-bonds)** 表示法。在这种方法中，你只需创建一个包含所有“键”类型（例如 C-H、C-C、O-H）及其相应成对相互作用值的列表。这种表示法舍弃了库仑矩阵所保留的大量全局几何信息。但它的巨大优点是它天然具有尺寸广延性，并且计算成本更低（计算复杂度为 $O(N^2)$）。对于像生成能这样本身就是广延量的属性，这种更简单的表示通常是更好的选择，因为它的结构反映了目标量的物理特性。它让模型在学习正确的尺度变化行为方面抢占了先机 [@problem_id:2837992]。

这揭示了一个深刻的教训：最好的表示不总是[信息量](@article_id:333051)最大的那个，而是其结构与主导问题的物理原理最契合的那个。

### 学习的对话：从数据到发现

一旦我们有了合适的语言，学习就可以开始了。这是模型与数据之间的一场对话，由我们设计的“评分标准”——**[损失函数](@article_id:638865) (loss function)**——来引导。

#### 站在巨人的肩膀上：$\Delta$-学习

在将机器学习应用于科学领域时，最强大的思想之一就是不让它从零开始学习一切。我们有数百年物理学知识编码在近似模型中，例如[量子化学](@article_id:300637)中的[密度泛函理论](@article_id:299475) (Density Functional Theory, DFT)。DFT 是一个强大但并不完美的计算分子能量的工具。我们可以训练一个神经网络直接学习“真实”的高精度能量。但这是一项艰巨的任务；分子的总能量是一个巨大而复杂的量。一个更聪明的办法是 **$\Delta$-学习** (delta-learning)。我们不让模型学习总能量 $E^{\mathrm{CC}}$，而是让它学习对成本较低的 DFT 能量 $E^{\mathrm{DFT}}$ 的*修正量* $\Delta$。我们的模型于是预测 $E^{\mathrm{CC}} = E^{\mathrm{DFT}} + \Delta_{\theta}$。这个[残差](@article_id:348682) $\Delta$ 是一个比总能量小得多、平滑得多、也“简单”得多的函数。它包含了 DFT出错的那部分关键物理。学习这个更简单的函数，能极大地提高数据利用效率。它尊重了来自物理学的现有知识，并要求机器将其能力集中在最需要的地方 [@problem_id:2903824]。基准 DFT 能量和真实能量都具有尺寸[广延性](@article_id:313063)，这意味着[残差](@article_id:348682) $\Delta$ 也具有尺寸广延性，这使得它成为一个易于机器学习的、性质良好的量 [@problem_id:2903824]。

#### 以优异成绩毕业：学习[导数](@article_id:318324)

一个好的物理模型不应只把能量算对，还必须把*[导数](@article_id:318324)*算对。原子上的力是能量对其位置的负梯度。晶体上的应力与能量对模拟盒子形变的[导数](@article_id:318324)有关。这些[导数](@article_id:318324)关系是基础性的。我们可以通过将这些关系包含在[损失函数](@article_id:638865)中来教会模型。在为晶体训练[势函数](@article_id:332364)时，我们不仅可以惩罚模型在能量 $U$ 上的误差，还可以惩罚其在原子受力 $\mathbf{F}$ 和[应力张量](@article_id:309392) $\boldsymbol{\sigma}$ 上的误差。然而，这些量有不同的单位和尺度。一个简单的误差总和会被能量项主导，模型会学会忽略力和应力。正确的方法是构建一个平衡的、无量纲的损失函数。我们用每个项的典型尺度和分量数来[归一化](@article_id:310343)，然后使用权重来调整它们的相对重要性。如果我们的目标是正确计算材料的**[弹性常数](@article_id:306627) (elastic constants)**，这取决于能量的二阶[导数](@article_id:318324)，那么我们必须确保模型密切关注应力，也就是一阶[导数](@article_id:318324)。通过明确地提高应变晶体数据中应力项的权重，我们引导模型学习[势能面](@article_id:307856)的正确曲率，从而获得对[材料属性](@article_id:307141)的准确物理预测 [@problem_id:2908447]。

### 使用不完美模型：前沿挑战

当模型训练完成后，旅程并未结束。在实际环境中、为了真正的科学发现而使用这些模型时，一系列新的挑战随之出现。

#### 领域漂移的危险

一个模型的好坏取决于训练它的数据。假设你训练了一个出色的模型来识别人类[激酶抑制剂](@article_id:296968)。它以惊人的准确性学会了人类蛋白质的微妙模式。但如果你让同一个模型去寻找细菌激酶的抑制剂，它可能会完全失败，表现得不比随机猜测好。这不是一个程序错误，而是一种被称为**领域漂移 (domain shift)** 的现象。人类和细菌之间的进化差异如此之大，以至于[蛋白质结合](@article_id:370568)的“游戏规则”已经改变。模型在人类“领域”学到的模式在细菌领域不再有效 [@problem_id:1426743]。这是一个至关重要的教训：模型的失败本身可以是一项科学发现，它预示着我们已经进入了一个新的范畴，在这个范畴里，我们的假设——以及我们模型的知识——不再成立。

#### “黑箱”之内

神经网络常被称为“黑箱”，因为它们的内部工作原理出了名的难以解释。在经典物理模型中，参数通常具有直接的物理意义——[弹簧常数](@article_id:346486)、[部分电荷](@article_id:346450)。在一个用作[原子间势](@article_id:356603)的[深度神经网络](@article_id:640465)中，数以百万计的单个[权重和偏置](@article_id:639384)没有这样简单、直接的物理意义。它们是一个高度灵活的数学函数中的抽象参数。许多不同的参数集可以导致完全相同的物理预测 [@problem_id:2456341]。这是否意味着它们对科学毫无用处？绝对不是。虽然单个参数无法解释，但模型*作为一个整体*是一个具有物理意义的对象。它的输入是尊重对称性的、有物理动机的描述符，它的输出是可观察的物理量，如能量和力。我们可以像对待一个未知的物理系统一样审视这个“黑箱”：我们可以探测它，用已知定律检验它的预测，并在模拟中分析其行为，以验证其物理真实性。

#### 保持运动定律

对于一个[机器学习势](@article_id:362354)函数来说，最严苛的考验或许是当我们用它来运行**[分子动力学模拟](@article_id:321141) (molecular dynamics simulation)** 时。在这种情况下，仅仅准确是不够的。模拟还必须遵守物理学的基本守恒定律，最显著的是在[微正则系综](@article_id:301954) (NVE) 中[能量守恒](@article_id:300957)。经典力学具有优美的几何结构。一个系统的运动在由位置和动量构成的“相空间”中展开。[哈密顿动力学](@article_id:316680)定律确保了这个相空间中任何区域的体积在随[时间演化](@article_id:314355)时保持不变。为了复制这一基本特性，物理学家使用特殊的**辛积分器 (symplectic integrators)**（如 Verlet [算法](@article_id:331821)）。这些积分器在有限时间步长内并不能精确地守恒真实能量，但它们确实守恒一个“[影子哈密顿量](@article_id:299200)”——一个与真实哈密顿量非常接近的函数。这一非凡的特性防止了能量在长时间模拟中系统性地漂移，而是使其在正确值附近有界地[振荡](@article_id:331484) [@problem_id:2903799]。但是，当力不是由精确的[解析函数](@article_id:300031)提供，而是由一个不完美的[机器学习势](@article_id:362354)函数提供时，会发生什么呢？即使是微小的、看似随机的力误差也会打破这种魔力。力的误差 $\delta\mathbf{F}$ 就像一个外部驱动力，不断地向系统注入或从中抽取能量。瞬时注入的功率是 $\dot{\mathbf{q}} \cdot \delta\mathbf{F}$。如果力的误差存在任何系统性偏差，总能量将随时间线性漂移。令人震惊的是，减小模拟的时间步长并*不能*解决这个问题。更小的时间步长使模拟更准确地表示了*在错误力作用下*的动力学，但它无法消除这些力所固有的能量漂移 [@problem_id:2903799]。这为物理学中的[深度学习](@article_id:302462)提出了一个重大的前沿课题：开发不仅准确，而且尊重物理动力学的深层守恒定律和几何结构的模型。我们正在寻求能够作为科学模拟和发现的真正可靠引擎的、[能量守恒](@article_id:300957)且尊重对称性的模型。