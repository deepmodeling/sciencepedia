## 引言
量化基因表达是理解细胞功能的基础，而 RNA 测序 (RNA-seq) 为我们提供了一个前所未有的窗口来观察转录组。然而，这些实验的原始数据——即每个基因的测序读取（read）的简单计数——可能会产生严重的误导。这些计数会受到技术性人为因素的扭曲，主要是基因的长度和实验的总[测序深度](@entry_id:178191)，这使得直接比较变得不可靠。本文旨在通过全面介绍“[每百万转录本](@entry_id:170576) ([TPM](@entry_id:170576))”这一优雅且广泛使用的标准化方法，来解决这一关键问题。在接下来的章节中，我们将首先探讨“原理与机制”，剖析原始数据中的偏差，并详细介绍使 [TPM](@entry_id:170576) 成为一种直观的比例表达度量方法的逐步计算过程。随后，在“应用与跨学科联系”部分，我们将考察 TPM 在肿瘤学、植物学等领域的应用，同时探讨对其正确解读至关重要的统计陷阱和高级注意事项。

## 原理与机制

要理解细胞如何运作，我们常常需要问一个简单的问题：哪些基因正在被使用？使用的程度如何？几十年来，人们的梦想是能够给细胞拍一张“快照”，并对其中每一个 RNA 分子进行计数，从而对细胞的“转录组”进行一次彻底的普查。随着现代 RNA 测序 ([RNA-seq](@entry_id:140811)) 技术的发展，这个梦想已近乎成为现实。我们可以将样本中的 RNA 分子打碎，[对产生](@entry_id:154125)的数百万个片段进行测序，然后将它们比对回其来源的基因。量化一个基因活性的最简单方法就是计算有多少测序读取比对到了这个基因上。这个数字被称为**原始读取计数 (raw read count)**。

这看起来很直接，不是吗？如果基因 A 有 10,000 次读取，而基因 B 有 500 次，那么基因 A 的活性必定是基因 B 的 20 倍。但科学中常有的情况是，最简单的答案虽然漂亮，却是错误的。事实证明，原始计数具有极大的欺骗性。

### 计数的困境：长基因与短基因

想象一下，你是一位考古学家，试图通过计算发现的陶器碎片数量来评估两座古城的重要性。在 A 城，你进行了一次为期一个月的大规模挖掘，发现了 10,000 个碎片。而在 B 城，你只有一个周末的时间，发现了 500 个碎片。你能断定 A 城的陶器产量更高吗？当然不能。你的取样投入截然不同。

这正是 RNA-seq 中的第一个问题。从一个样本中测序得到的总读取数——我们称之为**[测序深度](@entry_id:178191) (sequencing depth)** 或 **文库大小 (library size)**——是一个技术参数。一次“更深”的测序会对*每个*基因产生更多的读取，就像更长时间的挖掘会发现更多的碎片一样。比较一个总读取数为 1 亿的样本和一个总读取数为 2000 万的样本的原始计数，就像拿苹果和橘子作比较；这更多地反映了你的测序预算，而不是潜在的生物学信息。

还有一个更微妙的第二个问题。让我们回到古城的例子。假设你了解到 A 城的人们使用巨大的双耳瓶（amphorae），而 B 城的人们则偏爱小巧的杯子。即使两座城市都恰好生产了 1,000 个陶器，如果将它们全部打碎成碎片，使用巨型双耳瓶的城市会产生多得多的碎片。

这就是**基因长度偏差 (gene length bias)**。基因的大小并非完全相同。一个 10,000 个碱基对长的基因为片段化和测序提供了比一个仅 500 个碱基对长的基因大得多的靶标。即使细胞从这两个基因中各自只产生一个 RNA 转录本，较长的基因几乎肯定会产生更多的测序读取 [@problem_id:1530903]。

因此，原始读取计数受到两个独立技术因素的干扰：实验的[测序深度](@entry_id:178191)和每个基因的物理长度。为了进行任何有意义的比较——无论是在同一样本中比较不同基因，还是在不同样本中比较同一基因——我们都需要一种方法来校正这些偏差。我们需要对数据进行**标准化 (normalize)**。

### 一个优雅的解决方案：以比例思维

我们如何进行公平的比较呢？关键在于停止考虑绝对计数，转而思考比例。这正是被称为**[每百万转录本](@entry_id:170576) ([TPM](@entry_id:170576))** 的标准化指标背后的精妙见解。TPM 并不将偏差作为独立问题来处理，而是完全重构了问题。它问的是：在所有转录本的总池中，每个转录本的相对丰度是多少？

TPM 的计算是一个极具逻辑性的两步过程 [@problem_id:2579641]：

**第一步：校正基因长度。**

首先，我们解决“双耳瓶 vs. 杯子”的问题。对于每个基因，我们取其原始读取计数 $c_i$，然后除以该基因的长度 $L_i$（通常以千碱基 kb 为单位，即数千个碱基对）。

$$
\text{Read Density}_i = \frac{c_i}{L_i}
$$

这个值，我们称之为“读取密度 (read density)”，它告诉我们单位转录本长度上的读取数量。它有效地消除了长度偏差。一个长基因和一个短基因，如果它们在细胞中真正以相同的摩尔浓度存在，那么现在它们的读取密度应该相同 [@problem_id:4589994]。注意到一个美妙之处：我们现在得到了一个与转录本摩尔丰度成正比的量。

**第二步：通过计算比例来校正文库大小。**

现在，我们得到了样本中每个基因经过长度校正的丰度。为了使它们能与其他样本进行比较，我们需要考虑[测序深度](@entry_id:178191)。但我们不直接除以原始读取的总数（正如我们讨论过的，这是一个有偏差的数字），而是采用一种更巧妙的方法。我们首先将刚刚计算出的所有单个读取密度相加：

$$
S = \sum_{j} \text{Read Density}_j = \sum_{j} \frac{c_j}{L_j}
$$

这个值 $S$ 是什么？它是整个样本中“每碱基读取数”的总和。它代表了我们文库中经过长度标准化的总转录本丰度。它是一个样本特异性的缩放因子，相比于原始读取总数，它能更公平地估计样本的总“转录产出” [@problem_id:2424967]。

最后一步是将每个基因的贡献表示为这个总和的一部分，然后将其放大以便于阅读。基因 $i$ 的 [TPM](@entry_id:170576) 是其读取密度除以总读取密度，再乘以一百万。

$$
\text{TPM}_i = \left( \frac{c_i / L_i}{\sum_{j} (c_j / L_j)} \right) \times 10^6
$$

结果是一个代表比例的数字。如果一个基因的 [TPM](@entry_id:170576) 为 1,500，这意味着在一个包含一百万个经过长度标准化的转录本的假设文库中，有 1,500 个属于该基因。由于这种优雅的构造，如果你将一个样本中所有基因的 [TPM](@entry_id:170576) 值相加，总和将永远恰好是一百万 [@problem_id:4562784] [@problem_id:5157628]。正是这一特性使得 [TPM](@entry_id:170576) 值如此直观且具有可比性。

### [TPM](@entry_id:170576) 实践：两个文库的故事

让我们通过一个例子来看看它是如何运作的。假设我们有一个非常简单的[转录组](@entry_id:274025)，包含两个基因：基因 X（$L_X = 2$ kb）和基因 Y（$L_Y = 1$ kb）。我们进行了两个实验。

-   **样本 1（高深度）：**我们得到 2000 万总读取。基因 X 获得 40,000 次读取，基因 Y 获得 20,000 次读取。
-   **样本 2（低深度）：**我们得到 1000 万总读取。基因 X 获得 20,000 次读取，基因 Y 获得 10,000 次读取。

单看原始计数，很难判断发生了什么。但让我们来计算 TPM。

对于样本 1：
-   读取密度 X: $40000 / 2 \text{ kb} = 20000$
-   读取密度 Y: $20000 / 1 \text{ kb} = 20000$
-   总读取密度: $20000 + 20000 = 40000$
-   $\text{TPM}_X = (\frac{20000}{40000}) \times 10^6 = 500,000$
-   $\text{TPM}_Y = (\frac{20000}{40000}) \times 10^6 = 500,000$

对于样本 2：
-   读取密度 X: $20000 / 2 \text{ kb} = 10000$
-   读取密度 Y: $10000 / 1 \text{ kb} = 10000$
-   总读取密度: $10000 + 10000 = 20000$
-   $\text{TPM}_X = (\frac{10000}{20000}) \times 10^6 = 500,000$
-   $\text{TPM}_Y = (\frac{10000}{20000}) \times 10^6 = 500,000$

看！尽管样本 2 的[测序深度](@entry_id:178191)和原始计数减半，但每个基因的 TPM 值在两个样本中是完全相同的。TPM 标准化成功地穿透了技术差异，揭示了潜在的生物学现实：在两个样本中，基因 X 和 Y 对[转录组](@entry_id:274025)的贡献是相等的 [@problem_id:5157628]。这种稳定性是 [TPM](@entry_id:170576) 通常优于 **FPKM**（每千碱基转录本每百万比对片段数）等旧指标的原因，后者使用的分母不太稳定，其值在样本内总和不为常数 [@problem_id:4591071]。

### 世界是组成性的

TPM 的比例特性是其最大的优点，但它也揭示了关于这类数据的一个深刻事实：它是**组成性的 (compositional)**。任何一个基因的数值都与所有其他基因的数值内在相关。

想象一个细胞被病毒感染。病毒劫持了细胞的机制，一个单一的病毒基因 ($V$) 开始以极高的水平表达。假设我们对这个受感染的细胞进行测序，并将病毒基因纳入我们的 [TPM](@entry_id:170576) 计算 [@problem_id:2424939]。病毒基因的读取密度 $c_V / L_V$ 将会非常巨大。当我们计算总读取密度 $S = \sum (c_j / L_j)$ 时，这个新的病毒项将导致 $S$ 急剧上升。

一个完全无辜的宿主基因 $H_A$ 的 [TPM](@entry_id:170576) 会发生什么变化？它的 TPM 计算公式为 $(\frac{c_A / L_A}{S}) \times 10^6$。即使细胞仍在为 $H_A$ 产生完全相同数量的转录本，其 [TPM](@entry_id:170576) 值也会骤降，原因仅仅是分母 $S$ 变大了。实际上，*所有*宿主基因的 TPM 值都会以相同的倍数因子下降。

这是一个错误吗？不，这是现实的反映。[TPM](@entry_id:170576) 告诉你，宿主基因现在只占细胞总转录产出的一个更小的*比例*。病毒已经占据了主导，其他所有东西的[相对丰度](@entry_id:754219)必然下降。这是一个关键特性，而不是一个缺陷，但在解读 TPM 值时我们必须始终牢记这一点。

### 标准化的局限：为何我们仍钟爱原始计数

鉴于 [TPM](@entry_id:170576) 的优雅，你可能会认为我们可以永远抛弃原始计数。但在这里，我们必须小心。对于某些关键的统计任务，特别是**[差异表达](@entry_id:748396) (DE) 分析**，TPM 是不合适的工具。

DE 分析问一个非常具体的问题：“基因 A 的表达水平在我的[对照组](@entry_id:188599)和处理组之间有差异吗？” 这个问题关注的是该基因活性的变化，而与其他基因的动态无关。由于 [TPM](@entry_id:170576) 是组成性的，一个基因 [TPM](@entry_id:170576) 值的变化可能源于该基因自身，也可能是其他基因急剧变化（如我们的病毒例子）所造成的人为现象。

更根本的是，用于 DE 分析的强大[统计模型](@entry_id:755400)（通常基于**负二项分布**）是为直接处理原始整数计数而构建的 [@problem_id:2417796]。它们明确地模拟了测序过程的[随机和](@entry_id:266003)离散特性。一个 10 的计数本质上比一个 10,000 的计数有更大的不确定性，而这些模型会利用这一信息。将计数转换为像 TPM 这样连续的、比例性的度量，会丢弃关于[测量精度](@entry_id:271560)的重要信息，并从根本上违反了模型的统计假设。

这些先进的 DE 工具拥有自己更复杂的标准化方法，例如 **TMM（M 值的修剪均值）**。这些方法不会将[数据转换](@entry_id:170268)为比例。相反，它们使用原始计数来计算稳健的缩放因子，以同时考虑文库大小和组成偏差。然后，这些缩放因子在[统计模型](@entry_id:755400)内部使用，以确保比较的公平性 [@problem_id:2424929]。

因此，我们得出了一个统一的理解。原始计数是基本事实，包含最多的信息，但受到偏差的干扰。TPM 是一种出色的标准化方法，它校正这些偏差，为我们提供了一个直观的、比例化的[转录组](@entry_id:274025)视图，非常适合可视化和比较基因的相对表达水平。但对于严格的[统计假设检验](@entry_id:274987)，我们必须回到原始计数，并使用那些为尊重其独特统计特性而构建的专门工具。每种工具都有其用途，智慧在于知道何时使用哪一种。

