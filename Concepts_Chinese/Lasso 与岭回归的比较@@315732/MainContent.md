## 引言
在构建预测模型的过程中，一个核心挑战是管理[复杂性](@article_id:329807)以避免“[过拟合](@article_id:299541)”，即模型学习了数据中的噪声而非其潜在模式。[正则化](@article_id:300216)通过施加偏好简单性的约束，提供了一种强大的解决方案。在最基本的[正则化技术](@article_id:325104)中，Lasso 回归和[岭回归](@article_id:301426)占有重要地位。尽管两者都旨在提高模型的泛化能力，但它们基于不同的理念运作，并产生截然不同的结果。本文旨在解决何时以及为何选择其中一种方法的关键问题。通过深入了解它们的核心机制和在现实世界中的影响，您将对这些基本工具获得深刻而直观的理解。我们将首先剖析定义它们的“原理与机制”，从它们独特的几何约束到对模型系数的影响。随后，“应用与跨学科联系”部分将展示这些方法如何在各个科学领域中应用，以提取有意义的见解，从识别关键基因到稳定经济预测。让我们从探索这两种强大技术之间优雅的数学差异开始。

{'center': {'figure': {'img': {'figcaption': '[损失函数](@article_id:297237)的扩大[椭圆](@article_id:354490)首先接触圆形[岭回归](@article_id:301426)约束于一个两个系数都不为零的点。相比之下，它们很可能首先接触菱形的 Lasso 约束于一个角点，从而迫使一个系数为零。', 'src': 'https://i.imgur.com/8Qe8aT9.png', 'alt': '[岭回归](@article_id:301426)和 Lasso 回归的几何解释。[椭圆](@article_id:354490)[等高线](@article_id:332206)代表[损失函数](@article_id:297237)，而圆形（[岭回归](@article_id:301426)）和菱形（Lasso）代表惩罚约束。[最优解](@article_id:350611)是[等高线](@article_id:332206)首次[接触约束](@article_id:350746)的点。', 'width': '600'}}}, 'applications': '## 应用与跨学科联系\n\n在探寻了[岭回归](@article_id:301426)和 Lasso 回归的数学核心之后，我们可能会感到一种满足感，就像物理学家刚刚推导出一个优美的方程一样。但真正的乐趣，任何科学思想的真正考验，在于看到它在现实世界中焕发生机。这些关于[收缩](@article_id:311574)和选择的优雅概念在哪里留下了它们的[印记](@article_id:302202)？事实证明，无处不在。[岭回归](@article_id:301426)的温和说服与 Lasso 的果断削减之间的[张力](@article_id:302298)，反映了我们在试图理解任何[复杂系统](@article_id:298515)时必须做出的一个基本哲学选择：这个系统是由少数几个强大力量主导，还是由许多相互关联的、较弱的影响构成的网络？\n\n答案取决于你观察的领域。通过探索这两种方法的应用，我们踏上了一场穿越现代科学前沿的旅程，从[控制系统](@article_id:315701)的工程设计到生命本身的解码。\n\n### 对简洁的追求：Lasso 与[稀疏性](@article_id:297245)艺术\n\n自然界尽管复杂，但其运作常常遵循经济原则。许多现象是由少数关键因素驱动的，其余的只是噪音或细节。Lasso 的哲学——即许多系数应该恰好为零——是这种[稀疏性](@article_id:297245)原则的完美数学体现。它是一个大海捞针的工具。\n\n一个非常清晰的例子来自工程世界，用于识别像[数字滤波器](@article_id:360442)这样的系统属性 [@problem_id:1597912]。想象一下，你向一个“黑箱”系统发送一个单一、尖锐的输入脉冲，并测量其响应。如果你相信该系统的记忆很短，只有少数几个关键的输入回声真正重要，那么 Lasso 就是你的工具。在一个简单的实验中，如果一个回声很强而另一个很弱，[岭回归](@article_id:301426)会[收缩](@article_id:311574)两者但都保留在模型中。而 Lasso，以其智慧，可能会判定较弱的回声太微不足道，不值得其存在，并通过将其系数精确设置为零来外科手术般地移除它。它提供了一个更简单、更[稀疏](@article_id:380562)的模型，捕捉了系统行为的精髓。\n\n这种对简洁的追求不仅仅是为了工程上的便利；它帮助我们回答根本性的问题。考虑一个非常实际的问题：预测房价。一个数据集可能包含数百个特征，从像浴室数量这样的基本特征，到像前门颜色这样的琐碎特征。一个普通的[回归模型](@article_id:342805)可能会给门的颜色分配一些微小、统计上嘈杂的重要性。然而，Lasso 使我们的直觉形式化。它权衡了包含一个特征所带来的预测收益与为其[复杂性](@article_id:329807)付出的 $\\ell_1$ 惩罚。它可能会发现，知道门的颜色所带来的微小准确性提升不值得这个“成本”，于是将其系数精确设置为零。而浴室数量的系数则提供了足够的预测能力，可以轻松克服惩罚，并作为模型的关键部分保留下来 [@problem_id:1928629]。从这个意义上说，Lasso 自动化了[奥卡姆剃刀](@article_id:352397)。\n\n这种从[复杂性](@article_id:329807)中提炼出简洁性的能力，在现代生物学这个高风险、高维度的世界里变得真正具有变革性，在这里，特征数量 $p$ 可能远远超过样本数量 $n$。\n\n在[遗传学](@article_id:305596)中，科学家们正在努力解决“[上位性](@article_id:297028)”问题——即一个基因的效应被另一个基因修饰的现象。有数十万个基因，可能的两两相互作用的数量是天文数字。全部测试是不可能的。在这里，Lasso 成为了一个不可或缺的发现引擎。通过将生物体的适应度建模为其基因及其相互作用的函数，我们可以在只有一小部分相互作用具有生物学意义的假设下应用 Lasso。从数百万个候选相互作用的海洋中，Lasso 可以精确定位出一组[稀疏](@article_id:380562)、可管理的基因对，这些基因对可能驱动着生物体的[进化](@article_id:304208)命运 [@problem_id:2703951]。\n\n同样，在[免疫学](@article_id:302667)领域，预测一种[疫苗](@article_id:356049)在某个人身上是否有效是一个关键挑战。现代技术允许我们从单个血液样本中测量数千种[蛋白质](@article_id:328709)和[基因转录](@article_id:315931)本。面对来自有限数量研究参与者的海量数据，我们如何找到预测强烈[免疫反应](@article_id:302246)的少数几个关键[生物标志物](@article_id:327619)？Lasso 就是答案。它可以在这些“组学”数据中进行筛选，选出一个最小的早期反应分子组合，以预测长期[疫苗效力](@article_id:373290)，为[疫苗](@article_id:356049)的作用机制提供线索，并为[个性化医疗](@article_id:313081)铺平[道路](@article_id:317005) [@problem_id:2830959]。在经济学中，同样的原则允许分析师从大量国内外指标中确定一个国家金融风险的关键驱动因素 [@problem_id:2426340]。在每种情况下，Lasso 都像一个透镜，将[复杂系统](@article_id:298515)中的少数关键元素清晰地聚焦起来。\n\n### 对稳定性的追求：[岭回归](@article_id:301426)与[多重共线性](@article_id:302038)的驯服\n\n并非所有系统都是[稀疏](@article_id:380562)的。有时，现实是一个错综复杂的网络，其中许多因素都有贡献，更糟糕的是，这些因素本身是相关的。想象一个[生态系统](@article_id:383375)，其中生物体的几十个特征通过[遗传](@article_id:326229)和发育[交织](@article_id:332451)在一起。这就是[多重共线性](@article_id:302038)问题，也正是[岭回归](@article_id:301426)大放异彩的地方。在这里，目标不是选出几个“赢家”，而是获得对效应*整体模式*的稳定、可靠的估计。\n\n一个经典的例子来自[进化生物学](@article_id:305904)，在[自然选择](@article_id:301499)的研究中 [@problem_id:2519793]。想象一下，我们正在研究一个岛上的雀鸟，想了解选择如何作用于它们的喙。我们测量了喙长、喙深和喙宽。这些性状不是独立的；喙长的鸟很可能喙也深。如果我们使用标准回归来估计“[选择梯度](@article_id:313008)”——即作用于每个性状的选择力——我们测量值之间的高度相关性会使结果极不稳定。数据中一点微小的噪音就可能导致模型判定选择强烈偏好增加长度，但强烈反对增加深度，而另一个几乎相同的数据集可能会得出相反的结论。这些估计是摇摆不定、不可靠的。\n\n[岭回归](@article_id:301426)解决了这个困境。通过添加 $\\ell_2$ 惩罚，它引入了一个稳定的锚。它说：“我本身不信任任何单个的大系数。”它[收缩](@article_id:311574)所有相关性状的估计值，将它们拉向一个更保守、更集体的解决方案。得到的系数是有偏的——它们的值被系统性地低估了——但总体的选择向量要稳定得多，对抽样噪音的敏感性也低得多。我们可能在喙长*与*喙深的确切重要性上失去了一些精确性，但我们获得了关于选择推动雀鸟[种群](@article_id:378996)的总体*方向*的更可信的图景。在这个世界里，稳定性比[稀疏性](@article_id:297245)更有价值。\n\n### 统一的原则：[偏差-方差权衡](@article_id:299270)的一瞥\n\n所以，我们有两种方法，两种哲学。一种通过消除无关紧要的东西来寻求简单的真理，另一种通过调节一切来寻求稳定的真理。它们真的如此不同吗？在更深的层次上，两者都只是在同一个基本领域中航行的不同策略：[偏差-方差权衡](@article_id:299270)。\n\n我们建立的每个模型都是在[平衡](@article_id:305473)两个相互竞争的目标。我们想要一个忠实于底层过程的模型（低偏差），但我们也想要一个不被特定数据集中的随机噪音[干扰](@article_id:323376)的模型（低[方差](@article_id:379478)）。一个无偏但高[方差](@article_id:379478)的模型在每次我们收集新数据时都会给出截然不同的答案。一个有偏的模型可能不完全“正确”，但如果它的[方差](@article_id:379478)很低，它将始终如一地接近真相。\n\n[正则化](@article_id:300216)是一门艺术，它有意引入少量偏差，以换取[方差](@article_id:379478)的大幅降低，从而提高模型在新的、未见过的数据上的整体预测性能。让我们通过一个来自[神经科学](@article_id:309447)的简化但富有启发性的例子来一探究竟 [@problem_id:2727212]。假设我们正在根据一组 10 个[正交](@article_id:331620)的“基因模块”预测一个[神经元](@article_id:308519)的兴奋性。在一个假设的、已知噪音水平和真实底层效应的情况下，我们实际上可以计算标准无偏回归（OLS）和[岭回归](@article_id:301426)的预期预测误差。\n\n计算结果揭示了一些美妙的东西。无偏 OLS 模型的总误差由固有的、不可约的噪音（$\\sigma^2$）加上一个代表模型[方差](@article_id:379478)的项（$\\frac{p \\sigma^2}{n}$）组成。岭[回归模型](@article_id:342805)的误差也包含不可约的噪音，但其另外两个组成部分是一个新的偏差项（由[收缩](@article_id:311574)引起）和一个*减小了*的[方差](@article_id:379478)项。当[方差](@article_id:379478)的减少大于我们引入的偏差的平方时，奇迹就发生了。在这个问题的特定情景中，数字显示，[岭回归](@article_id:301426)通过进行这种精确的权衡，将预期预测误差减少了一个虽小但确定的量，大约为 $4.15 \\times 10^{-3}$。\n\n这一个思想统一了[岭回归](@article_id:301426)和 Lasso。它们不是随意的技巧；它们是管理[偏差-方差权衡](@article_id:299270)的原则性方法。Lasso 的策略是采取激进的偏差，迫使许多系数为零，如果底层系统真的是[稀疏](@article_id:380562)的，这可以导致[方差](@article_id:379478)的急剧减少。[岭回归](@article_id:301426)的策略是一种更温和的偏差，按比例[收缩](@article_id:311574)所有系数，这提供了一种更平缓但稳健的[方差](@article_id:379478)减少，尤其是在许多预测变量相关时。因此，选择是一个科学问题，它基于我们的领域知识和我们对所研究系统的信念。这是一个美丽的例子，说明了一个深刻的统计学原理如何为跨越无数个学科的科学发现提供了一个实用的框架。', '#text': '## 原理与机制\n\n想象你是一位雕塑家。你从一块巨大、未经雕琢的大理石开始——这是你复杂的现实，充满了无数潜在的因素，即“预测变量”。你的目标是雕刻出一座美丽、简洁的雕像，捕捉隐藏在其中的形态精髓。这座雕像就是你的预测模型。普通的方法，就像一把蛮力凿子，可能会试图刻画每一个微小的凸起和瑕疵，结果得到的模型“[过拟合](@article_id:299541)”了——它完美地代表了*那块*大理石，但与它本应捕捉的[理想](@article_id:309270)形态毫无相似之处。它无法泛化。\n\n要创作一件杰作，你需要一个指导原则，一个迫使你追求简洁和优雅的约束。在统计建模的世界里，这种指导来自[正则化](@article_id:300216)。[岭回归](@article_id:301426)和 Lasso 是两种最受推崇的雕刻技术，每种技术都有其独特的驯服[复杂性](@article_id:329807)的哲学。尽管它们都旨在创建更简单、更稳健的模型，但它们的方法却截然不同，这源于一个单一而优美的数学区别。让我们逐层揭开，看看它们是如何工作的。\n\n### 两种形状的故事：惩罚项的[几何学](@article_id:378469)\n\n[岭回归](@article_id:301426)和 Lasso 之间差异的核心在于[几何学](@article_id:378469)。让我们将世界简化为只有两个预测变量，其系数我们称之为 $\\beta_1$ 和 $\\beta_2$。我们可以将任何潜在的模型表示为由这两个轴定义的平面上的一个点。在这个平面上的某个地方，存在一个如果我们不关心[复杂性](@article_id:329807)时的“完美”解——即最能拟合我们数据的普通[最小二乘](@article_id:323339)（OLS）解。我们称之为“[理想](@article_id:309270)点”。我们的目标是找到一个好的模型，但我们不被允许一直走到这个点。我们受到一个关于系数总大小的“预算”的限制。\n\n这就是两种方法[分歧](@article_id:372077)的地方。在[惩罚回归](@article_id:357077)中，“预算”由一个以原点为中心的形状定义，我们的最终模型必须是这个形状内尽可能接近我们[理想](@article_id:309270)点的那个点。可以将[损失函数](@article_id:297237)（我们拟[合数](@article_id:327260)据的愿望）想象成以 OLS 解为中心的一系列不断扩大的[椭圆](@article_id:354490)[等高线](@article_id:332206)。最佳约束模型就是这些扩大的[椭圆](@article_id:354490)首次接触预算形状的那个点。\n\n*   **[岭回归](@article_id:301426)**使用数学家所说的 **$L_2$ 惩罚**。其预算由系数的*平方*和定义：$\\beta_1^2 + \\beta_2^2 \\le s$。你可能还记得，这个方程定义了一个完美的圆形（或在更高维度下的超[球面](@article_id:331282)）。[@problem_id:1928628] 圆是光滑、圆润的，没有角。当我们的[椭圆](@article_id:354490)[等高线](@article_id:332206)扩大时，它们几乎总是在一个独特的[切点](@article_id:351997)上接触到圆形，在这个[切点](@article_id:351997)上，*$\\beta_1$ 和 $\\beta_2$ 都*不为零。这个解是一个折衷，将两个系数都向原点[收缩](@article_id:311574)，但很少会迫使其中任何一个恰好为零。\n\n*   另一方面，**Lasso 回归**使用 **$L_1$ 惩罚**。其预算由系数的*[绝对值](@article_id:376284)*之和定义：$|\\beta_1| + |\\beta_2| \\le s$。这个方程定义了一个菱形，或者说一个旋转了 45 度的正方形。[@problem_id:1928628] 与圆形不同，这个菱形有尖锐的角，这些角恰好位于坐标轴上。例如，在顶部的角点，$\\beta_1=0$ 且 $\\beta_2=s$。因为这些角向外突出，我们的[理想](@article_id:309270)[椭圆](@article_id:354490)[等高线](@article_id:332206)有很高的概率会首先碰到其中一个角。[@problem_id:1928625] 那么，落在一个角上意味着什么呢？这意味着其中一个系数恰好为零！这个简单的几何事实正是 Lasso 最为人称道的力量之源：它能够执行[特征选择](@article_id:302140)。'}

