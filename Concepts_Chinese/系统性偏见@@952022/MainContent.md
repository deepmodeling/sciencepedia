## 引言
系统性偏见是数字时代最严峻的挑战之一，它并非技术故障，而是嵌入我们自动化系统中的历史性不平等的反映。随着我们日益依赖人工智能在医学和法律等领域做出关键决策，我们面临一个重大问题：这些强大的工具可能会无意中延续甚至放大我们努力克服的社会偏见。本文将直面这一挑战。第一章“原理与机制”将解构系统性偏见，探讨其在有缺陷的数据、[算法设计](@entry_id:634229)和人类互动中的起源。随后，“应用与跨学科联系”一章将展示这些原理在现实世界中的后果，审视其在医疗保健、法律体系乃至科学探究基础本身中的深远影响。

## 原理与机制

想象一下，你正在教一台计算机在照片中识别猫。你给它看了成千上万张猫的照片——虎斑猫、三花猫、暹罗猫——它开始学习。现在，如果出于某种奇怪的原因，你收藏的所有暹罗猫照片都是在阴天拍摄的呢？计算机会在其对模式的不懈追求中，得出结论：“是暹罗猫”与“灰色、阴天的光线”有某种关联。它产生了一种偏见。它没有做任何恶意的事情；它只是从一个有缺陷的数据世界中得出了一个完全合乎逻辑但根本错误的结论。

简而言之，这就是**系统性偏见**的本质。它不是机器中的幽灵，而是我们世界中幽灵的反映——那些我们无意中输入到自动化系统中的历史性不平等、隐藏的假设和结构性失衡。要理解这种偏见，我们必须成为侦探，追溯其从我们收集的数据到我们构建的算法，最终到它们被部署的复杂人类世界的起源。

### 偏见的剖析：一份问题[分类学](@entry_id:172984)

当我们谈论偏见时，将其与简单的[随机误差](@entry_id:144890)区分开来至关重要。[随机误差](@entry_id:144890)就像一次不幸的抛硬币结果是正面；它是个偶然事件。相比之下，**系统性偏见**就像一枚被巧妙加权的硬币。即使你抛一千次，它平均下来也会偏向某一面。用统计学的语言来说，[算法偏见](@entry_id:637996)是模型性能的一种*系统性*偏差，即使我们对无数可能的训练数据集进行平均，这种偏差依然存在 [@problem_id:5225896]。这是学习过程本身的缺陷，而不仅仅是掷骰子的运气不好。

这种系统性的不公平与统计学家在分析参数估计器时可能谈论的“偏差”不同。那是估计程序的一种技术属性。我们在此关心的偏见关乎正义和伤害——关乎模型的系统性误差如何使可识别的人群处于不利地位 [@problem_id:4849723]。这种伤害可以从系统基础的许多不同裂缝中渗入。我们可以将这些来源归为一种问题[分类学](@entry_id:172984) [@problem_id:4824163] [@problem_id:4406676]。

#### 数据偏见：垃圾进，“真理”出

系统性偏见最常见的来源是数据本身。一个模型的好坏取决于它所看到的世界，而我们收集的数据往往是现实的扭曲漫画。

**选择偏见**发生于我们的数据集并非我们所关心群体的[代表性样本](@entry_id:201715)之时。想象一下，仅使用被一家大型城市医院收治的患者数据来训练一个医疗AI。该模型对农村患者、无力承担旅行费用的患者或病情严重无法成行的患者一无所知。数据以一种使整个群体变得“不可见”的方式被“选择”了。

**测量偏见**发生于我们观察世界的工具本身存在偏见之时。典型的例子是[脉搏血氧仪](@entry_id:202030)，一种测量血氧水平的设备。研究表明，它在肤色较深的患者身上准确性较低。对基本生物状态的“测量”在不同群体之间并不一致。在这种情况下，观测到的数据，我们称之为 $x$，是真实潜在临床状态 $z$ 的一个系统性扭曲版本，且这种扭曲取决于个体的群体成员身份 [@problem_id:4406676]。模型从扭曲的 $x$ 世界中学习，而不是从真实的 $z$ 世界中学习。

**标签偏见**可能是医疗保健中最隐蔽的数据偏见形式之一。“标签”是我们在训练期间提供给算法的“答案”。但如果答案以一种系统性的方式出错呢？考虑一个旨在预测患者真实“健康需求”的AI。我们无法直接测量“需求”，所以我们可能会使用一个代理指标，比如“患者是否在30天内再次入院？”这似乎是合理的。但现在考虑一个来自[边缘化](@entry_id:264637)社区的患者，他有很高的健康需求，但缺乏可靠的交通工具或保险。他可能病得很重，却无法回到医院。他的“标签”是`readmission = false`，但他的真实“需求”很高。因为医疗服务的获取在社会上并非平等分配，使用像再入院这样的结果作为需求的代理指标，可能会构建出一个系统性地低估弱势群体需求的模型 [@problem_id:4866413]。标签本身就是有偏见的。

#### [算法偏见](@entry_id:637996)：有缺陷的配方

即使我们拥有完美、无偏见的数据——现实的真实镜像——我们设计算法的方式仍然可能引入偏见。算法只是一个配方。想象一个配方说：“为了做出最好的汤，要专注于让鸡肉风味完美，即使蔬菜风味会受点影响。”如果你的目标是取悦喜爱鸡肉的普通食客，这可能行得通。但桌上的素食者将只得到一碗平淡乏味的汤。

许多机器学习模型都以类似的逻辑进行训练：最小化整个数据集的*总*误差。这被称为**[经验风险最小化](@entry_id:633880)** [@problem_id:4406676]。如果一个少数群体只占数据的一小部分，模型可以在对该特定群体极其不准确的情况下，实现非常低的总误差。“多数人的暴政”被直接构建在优化目标中。模型学到，它可以承受对少数群体犯错，因为这样做对其总体得分影响不大。这是其最纯粹形式的[算法偏见](@entry_id:637996)：模型对成功的定义本身就存在缺陷 [@problem_id:4824163] [@problem_id:4866413]。

### 从代码到临床：现实世界中的偏见

偏见并不仅止于算法的输出。模型不是一段孤立的代码；它是一个更大、更混乱的社会技术系统中的一个组件。

首先，存在**部署偏见**。一个在某家医院的数据上训练的模型，该医院有特定的人口构成和资源配置，当部署到镇上另一个地区、拥有不同人口的社区诊所时，其表现可能就不同了 [@problem_id:4866413]。背景变了，模型的可靠性和公平性也可能随之下降。

更深层次的是，算法的输出几乎总是由人来解释和执行。这就创建了一个**人在回路中**的系统，其中人类和机器的偏见可以以令人惊讶的方式相互作用和复合。考虑一个败血症检测AI，它给患者一个风险评分。问题是双重的：

1.  AI可能存在偏见。由于少数群体的历史数据更稀疏，它可能会系统性地给他们[分配比](@entry_id:183708)多数群体患者更低的风险评分，即使在真实病情严重程度相同的情况下。
2.  临床医生可能*也*存在偏见，也许是隐性的，并且在为同样来自该少数群体的患者采取行动之前，需要更高程度的确定性。

结果呢？AI的偏见降低了分数，而临床医生的偏见提高了行动的门槛。两种偏见相互放大，导致弱势群体的漏诊率高得多 [@problem_id:4849720]。

另一方面，我们有**自动化偏见**，这是一种认知捷径，即人类过度相信自动化系统的输出 [@problem_id:4824163]。临床医生可能会看到AI给出的低风险评分而感到放心，从而忽略了自己关于患者看起来不适的直觉。计算机产生的看似“客观”的数字可能感觉比人类的直觉更具权威性，即使这个数字是一个深度偏见过程的产物。

### 伤害的形式：衡量和理解影响

所以，我们知道了偏见的来源。但它实际上*做*了什么？我们如何看到它，它的后果是什么？

首先，我们可以测量它。通过审计模型在不同群体间的性能，我们可以量化这种差异。假设我们审计一个旨在标记出院后需要随访电话的患者的AI [@problem_id:4367362]。对于A和B两个群体，我们可以计算关键指标：

*   **[真阳性率](@entry_id:637442)（TPR）：** 被正确标记的高风险患者的比例。B组的TPR较低意味着模型系统性地未能识别和帮助该组的患病人群。例如，如果$TPR_A = 0.70$但$TPR_B = 0.50$，则系统在A组中找到了70%的高风险患者，但在B组中只找到了50%。
*   **[假阳性率](@entry_id:636147)（FPR）：** 被错误标记的低风险患者的比例。B组的FPR较高意味着其成员收到了更多的错误警报，导致不必要的焦虑和资源浪费。

这些比率的差异违反了像**[均等化赔率](@entry_id:637744)**这样的公平性标准，该标准要求模型对两个群体都应同等有效，即对每个群体具有相同的TPR和FPR [@problem_id:4367362] [@problem_id:4968683]。另一个度量标准**[人口统计学](@entry_id:143605)对等**，则简单地询问模型是否在每个群体中标记了相同比例的人，无论他们的实际风险如何。

我们还可以看**校准**。如果一个预测风险为80%的事件，实际上发生的概率确实是80%，那么这个模型就是校准良好的。但如果一个模型按群体进行了错误的校准，它就可能存在偏见。例如，0.8的分数对A组可能意味着80%的败血症几率，但对B组却只有40%。同一个数字根据你是谁而具有完全不同的含义，这使得应用单一、公平的决策阈值成为不可能 [@problem_id:4968683]。

然而，这些数字只讲述了故事的一部分。伤害的*性质*也很重要。女性主义生物伦理学帮助我们区分两种[基本类](@entry_id:158335)型的伤害 [@problem_id:4862115]：

*   **分配性伤害：** 这是由资源或机会的不公平分配造成的伤害。当一个AI系统因使用患者先前的医疗支出来作为需求代理指标而存在偏见，从而拒绝为低收入患者提供重症监护协调时，这是一种分配性伤害。一种有形的利益被剥夺了。
*   **代表性伤害：** 这是由强化刻板印象或错误识别个人身份和能动性造成的伤害。当一个系统将一名因交通问题错过预约的怀孕黑人患者标记为“不遵从医嘱”时，直接的伤害不是剥夺资源。伤害在于标签本身。它延续了一种破坏性的刻板印象，忽略了她面临的结构性障碍，并将她框定为一个“麻烦”的病人，毒害了她与医疗团队的关系。这是一种代表性伤害。

### 结构性视角：幽灵的真正来源

如果我们追溯这些偏见的源头，我们常常会发现它们并非孤立的技术故障。它们是 preexisting 的、现实世界结构性不平等的数字指纹。

考虑一个医院，审计发现A组患者被非自愿地送入精神病院的比率高于B组患者，即使他们表现出完全相同的临床严重程度 [@problem_id:4870848]。是因为临床医生个人有偏见吗？也许是。但更深入的观察揭示，医院的协议允许使用“无稳定住所”作为“严重残疾”的代理指标——这是收治的法律标准之一。由于长期的社会因素，A组的人们更有可能经历住房不稳定。因此，协议本身——即“系统”——创造了一种将社会经济差异转化为临床差异的机制。这种偏见是结构性的。

这是最终的教训。系统性偏见很少诞生于代码中。它诞生于世界。它是使用过去支出的数据来预测未来需求，而没有考虑收入不平等的结果 [@problem_id:4862115]。它是使用再入院率作为健康代理指标，而没有考虑医疗服务获取不平等的结果 [@problem_id:4866413]。它是一个没有以“终身自我反思……和机构问责的姿态”设计的系统的结果 [@problem_id:4367362]。

要真正解决系统性偏见，我们必须做的不仅仅是调试我们的算法。我们必须调试它们所服务的系统中的假设和实践。这不仅需要更优秀的数据科学家，还需要与被服务的社区建立伙伴关系，致力于理解他们的背景，并谦卑地接受最整洁的数学解决方案可能不是最公正的解决方案。

