## 应用与跨学科联系

我们花了一些时间来理解保形预测的机制，它的齿轮和杠杆由[分位数](@article_id:323504)和非符合性分数构成。它是一件精美的理论钟表。但它究竟有何*用途*？就像任何伟大的科学思想一样，其真正的价值不在于其抽象的优雅，而在于它让我们能够*做*什么。它打开了哪些门？哪些曾经棘手的问题，它突然间变得可以解决了？

事实证明，答案是，保形预测不仅仅是一个工具；它是一种审视预测世界的新视角。它在我们的数学模型与它们试图描述的那个混乱、不可预测的现实之间提供了一份诚实契约。这份契约在各个领域都具有深远的影响，从科学发现的前沿到在社会中部署人工智能的伦理核心。

### 实验室里的新伙伴

让我们从科学实验室开始，在那里，对真理的探索是一个缓慢而艰苦的假设、实验和观察过程。科学家们越来越多地利用人工智能来加速这一过程，创建能够设计并运行自己实验的“自驱动实验室”。但这带来了一个令人畏惧的问题：人工智能如何知道它自己对世界的内部模型是错误的？它如何知道何时应该停止模拟，而去实际执行一个实验来检验自己的假设？

想象一个人工智能任务是发现一种用于清洁能源生产的新[催化剂](@article_id:298981) [@problem_id:77114] [@problem_id:30004]。这个人工智能有一个深度学习模型，可以根据化学结构预测[催化剂](@article_id:298981)的效率。一个[标准模型](@article_id:297875)可能会预测：“[催化剂](@article_id:298981) X 的效率将为 0.85。”如果人工智能相信这个点预测，它可能会在寻找效率为 0.90 或更高的[催化剂](@article_id:298981)时跳过 X。但如果模型的真实不确定性巨大，实际效率可能在 0.6 到 1.1 之间呢？人工智能将愚蠢地丢弃一个潜在的革命性发现。

保形预测改变了游戏规则。通过校准[深度学习](@article_id:302462)模型，人工智能不仅得到一个单一的数字，它还得到一个数学上严格的[预测区间](@article_id:640082)：“我可以以 95% 的置信度保证，[催化剂](@article_id:298981) X 的效率在 $0.75$ 和 $0.95$ 之间。”这是变革性的。狭窄的区间表明信心十足。宽阔的区间则是对无知的诚实承认。现在，人工智能知道自己何时在其知识的边缘操作。一个宽阔的区间成为一个[触发器](@article_id:353355)：“我的模型在这里不确定。是时候合成并测试这种材料以收集真实世界的数据了。”这创造了一个美妙的反馈循环，其中不确定性主动而智能地引导着科学发现的路径。

同样的原理正在彻底改变[计算生物学](@article_id:307404)等领域。当试图确定一个新发现蛋白质的功能时，一个典型的分类器可能会给出它唯一的最佳猜测。如果生物学家依赖那个猜测，而猜测是错误的，他可能会在失败的实验上花费数月时间。相反，保形预测提供一个预测*集* [@problem_id:2406434]。它可能会说：“我有 99% 的把握，其真实功能在这个集合中：{`新陈代谢`, `[信号转导](@article_id:305040)`}。”这远比单一猜测有价值得多。它为生物学家提供了一套完整的、有统计依据的假设去研究。我们甚至可以将这些保证构建到复杂的[生物学层级](@article_id:298208)中，让模型能够说，例如，“我不确定确切的物种，但我能以 95% 的置信度保证它是一种犬科动物” [@problem_id:3179656]。

### 构建更智能、更安全的人工智能

保形预测的影响超出了科学实验室；它帮助我们构建更好的人工智能系统本身。现代机器学习充满了极其强大但往往不透明的模型，比如可以创造出惊人逼真的图像、文本乃至科学数据的[生成对抗网络](@article_id:638564)（GANs）。但我们如何能信任一个本质上是创造力发电站的模型的输出呢？

考虑一个旨在模拟复杂物理现象的[生成模型](@article_id:356498) [@problem_id:3108953]。该模型可能有些“错误设定”——其内部世界与我们自己的物理定律不完全匹配。保形预测提供了一个绝妙的解决方案。我们可以将这个复杂的[生成模型](@article_id:356498)视为一个黑箱，并在其外部“包装”一个保形预测器。通过向它展示少量其预测与现实偏差的例子（校准集），我们可以调整其输出，提供新的[预测区间](@article_id:640082)，这些区间保证在真实世界中是有效的。我们通过一小部分现实来纠正模型的幻想，使其脚踏实地。

这种在另一个系统之上建立信任层的能力，催生了新的学习方式。人工智能最大的挑战之一是标记数据的稀缺性。在一种称为[自训练](@article_id:640743)的技术中，我们希望模型能通过生成自己的“[伪标签](@article_id:640156)”从海量未标记数据中学习。其危险在于，如果模型生成了错误的标签，它会教给自己错误的东西，陷入错误的螺旋。保形预测提供了一种有原则的方法来控制这种风险 [@problem_id:3172773]。对于每个未标记的样本，我们生成一个预测集。如果该集合只包含一个类别——例如，{猫}——那么模型就非常有信心。我们可以决定信任这个预测，并将其用作新的训练标签。如果集合是{猫, 狗, 浣熊}，模型就在告诉我们它不确定，我们应该避免使用它。保形集的大小成为一个严格的、具有统计意义的“置信度分数”，让人工智能能够更安全、更有效地进行自我教学。

### 前沿：责任与信任

这就把我们带到了所有应用中最关键的一个：在影响人类生命的高风险决策中使用人工智能。当一个模型被用于医疗诊断、司法，或者像我们一个驱动性问题那样，用于预测威胁生命的风暴潮时，一个错误的答案不是不便，而可能是一场灾难 [@problem_id:3117035]。

在这些领域，说“我不知道”的能力不是弱点，而是一个至关重要的安全特性。保形预测为此提供了一个自然的框架。我们可以构建这样的系统，当面临高度不确定性（一个大的预测集）时，它会拒绝做出自动决策，而是将案例移交给人类专家 [@problem_id:3182609]。这在人与机器之间创造了一种协作伙伴关系，发挥了双方的优势。我们甚至可以重新定义我们经典的评估指标，如[混淆矩阵](@article_id:639354)，以考虑集值输出，从而让我们对模型性能的理解超越简单的准确率。

然而，保形预测所做的诚实契约带有一个关键的细则：保证 $\mathbb{P}\{Y \in \Gamma(X)\} \ge 1-\alpha$ 是在*[可交换性](@article_id:327021)*假设下成立的。粗略地说，这意味着我们进行预测的数据与我们用于校准的数据来自同一个统计世界。当世界发生变化时会发生什么？这就是所谓的[协变量偏移](@article_id:640491)，它是可信赖人工智能面临的最大挑战之一。

想象一个用于化学领域的[机器学习势](@article_id:362354)，用于在特定温度下模拟分子 [@problem_id:2648634]。如果我们随后用它在更高的温度下进行预测，它遇到的原子构型可能与训练期间看到的完全不同。基础假设被打破了。在这种情况下，即使是模型自身的[不确定性估计](@article_id:370131)也可能变得不可靠，并出现虚假的过度自信。解决方案不是放弃对不确定性的追求，而是变得更加精细。研究人员现在正在开发与保形预测器并行运行的方法——[分布外检测](@article_id:640393)器——作为[第二道防线](@article_id:352393)。这些检测器监控模型的输入，当它们感觉到世界变化太大时就会发出警报，表明模型的覆盖率保证可能不再成立。这就是前沿：构建如此稳健的系统，以至于它们甚至知道何时该不信任自己的[不确定性估计](@article_id:370131)。

归根结底，这就是保形预测提供的宏伟愿景。它提供了一个框架，用以构建不是教条式神谕，而是谦逊诚实的协作者的人工智能。通过承诺其预测集在特定比例的时间内是正确的，它与现实建立了一个可验证的契约。这个可验证的承诺，比任何关于超人准确性的声明都更重要，是我们能够构建不仅强大，而且最终值得我们信任的人工智能系统的基石。