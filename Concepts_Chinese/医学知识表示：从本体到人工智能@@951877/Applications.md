## 应用与跨学科联系

在走过医学知识表示的原理之旅后，我们现在来到了探索中最激动人心的部分：见证这些思想在实践中的应用。欣赏本体的优雅架构或知识图谱的形式化精度是一回事；而亲眼目睹它们在复杂、高风险的医学世界中发挥作用，则完全是另一回事。在这里，我们将看到这些结构如何不仅仅是信息的被动存储库，而是发现、决策甚至追求正义过程中的积极伙伴。

你或许可以把医学想象成拥有两种心智。一种是经验丰富的临床医生的直觉——一种强大的、近乎潜意识的[模式匹配](@entry_id:137990)器，经过多年经验的磨练，能够瞥一眼病人就对病情有“直觉”。这是诊断大师的心智，也是[现代机器学习](@entry_id:637169)出色模仿的心智。另一种是系统的、演绎的心智——它遵循指南，核对鉴别诊断，并从第一性原理出发进行推理。这是科学家的心智，也是基于知识的系统力求捕捉的心智。医学信息学的巨大前景在于在这两种心智之间建立对话，而正如我们将看到的，形式化的知识表示正是使这种对话成为可能的语言 [@problem_id:4846805]。

### 编织知识之网

在我们的系统能够推理之前，它们必须首先学习。它们必须阅读。但与人类不同，计算机不能简单地从一页纸上吸收意义。它必须被教会如何构建一个知识之网，一次一个事实。这个过程是语言学、计算机科学和深厚领域专业知识的迷人结合。

想象一下每天在生物医学期刊上发表的信息洪流。像“基因 *BRCA1* 参与[DNA修复](@entry_id:146977)”这样一个简单的句子就包含了丰富的意义。一个信息提取管道面临着一项艰巨的任务：首先，它必须执行命名实体识别（NER）来识别“*BRCA1*”是一个基因，“[DNA修复](@entry_id:146977)”是一个生物过程。然后，它必须执行关系提取来理解它们之间的关系是“参与”。但这还不够。为了使其有用，这些信息必须被标准化。系统必须知道“BRCA1”指的是具有HUGO基因命名委员会标识符`HGNC:1100`的特定基因，而不是其他名称相似的实体。关系“参与”必须映射到像基因[本体](@entry_id:264049)（Gene Ontology）这样的受控词表中的一个精确术语。

这个过程要求近乎苛刻的科学保真度。考虑这样一个陈述：“基因$g$[抑制蛋白](@entry_id:194935)质$p$。”一个天真的系统可能会创建一个直接链接：`Gene g ---[inhibits]---> Protein p`。但生物学家会立刻提出异议！基因是一段DNA；它不会在细胞里四处漂浮来抑制东西。是它的*产物*——它编码的蛋白质——才是活性剂。一个真正智能的系统必须捕捉到这一点。它必须推断出抑制的主体是基因$g$的蛋白质产物，从而创建一个更丰富、更准确的表示：`Gene g ---[encodes]---> Protein Product of g ---[inhibits]---> Protein p`。这个区别，尽管看起来很微妙，却是一个脆弱的数据库和一个能够支持真正生物学发现的知识图谱之间的差异 [@problem_id:4846316]。

对共同语言的需求在临床中更为迫切。一份电子健康记录（EHR）简直就是一座巴别塔。像“心脏病发作”这样一个单一概念，根据上下文的不同——无论是用于临床记录、计费还是研究——可能会用十几种不同的同义词、缩写或代码来记录。为了弥合这一鸿沟，医学信息学依赖于一套标准术语，每套术语都有其独特的角色。

-   **SNOMED CT（医学系统化命名法——临床术语）** 是最全面的。它不仅仅是一本词典；它是一个真正的本体，拥有丰富的“is-a”关系的多重继承结构。这种结构使得强大的*概念包含推理*成为可能。系统知道“伴有足部溃疡的[2型糖尿病](@entry_id:154880)”*是*“糖尿病”的一种。这意味着为一般病症`Diabetes mellitus`编写的规则将自动适用于具有更具体诊断的患者，而无需列出所有可能的变体 [@problem_id:4606569]。

-   **LOINC（逻辑观察标识符名称和代码）** 回答的是“问题”，而不是“答案”。一个LOINC代码精确指定了测量了什么（例如，“血液中的[糖化血红蛋白](@entry_id:150571)A1c”），而不是结果（例如，$8.1\%$）。这种关键的分离使得系统能够聚合所有相同类型的测量值，无论其数值如何 [@problem_id:4606569]。

-   **RxNorm** 为药物所做的工作，就如同LOINC为实验室检查所做的一样。它对药物进行标准化，将品牌产品（“格华止500mg片剂”）与其通用等效物，以及最重要的是，与其活性成分（“二甲双胍”）联系起来。这使得决策支持规则可以针对成分编写，从而自动覆盖所有包含该成分的药品 [@problem_id:4606569]。

通过将这些来自文献、临床记录、实验室结果和处方的零散信息编织在一起，我们创建了一个统一的、可计算的医学视图。这项工作甚至延伸到了医学进步的核心引擎：临床试验。像ClinicalTrials.gov这样的注册库包含了成千上万项研究的数据，但通常，病症和结局都是用非结构化的自由文本描述的。巨大的挑战是使这些数据变得FAIR——可查找（Findable）、可访问（Accessible）、可互操作（Interoperable）和可重用（Reusable）。这需要一个复杂的策略，使用统一医学语言系统（UMLS）将自由文本的病症映射到协调一致的概念，并且至关重要的是，将复杂的结局表示为*复合*描述，而不是单一术语，或许可以使用一个本体来描述测量过程（如生物医学研究本体，OBI），另一个来描述分析物本身（LOINC）。这是一个艰苦的过程，充满了模糊性和持续的整理工作量等挑战，但它是实现大规模、可重复的[荟萃分析](@entry_id:263874)，从而推动循证医学前进的基础工作 [@problem_id:4999124]。

### 推理引擎：让知识发挥作用

有了这个精心构建的知识之网，我们就可以开始向它提问——并得到非常智能的答案。知识图谱不仅仅是一个数据库；它变成了一个推理引擎。

现代医学的基石之一是循证医学（EBM），即基于现有最佳研究做出决策的实践。想象一位临床医生面对一个特定病人：一位65岁男性，患有[2型糖尿病](@entry_id:154880)和早期肾病。哪种治疗最好？答案可能埋藏在成千上万个临床试验中的某一个。一个基于知识的系统可以使这种搜索变得轻而易举。每个试验都可以使用PICO框架来表示：其人群（Population）、干预（Intervention）、对照（Comparator）和结局（Outcome）。患者的特征也表示在图中。然后系统可以执行一个查询：“找到所有试验$t$，其中患者`matchesPopulation`（匹配人群）$t$。”这种匹配不是简单的关键词搜索；它可以利用[本体](@entry_id:264049)层次结构来知道一个65岁的患者符合被描述为“中老年”的试验人群。一旦找到匹配项，系统就会检索试验的干预措施和结局。它在图谱中遵循的路径——从患者到人群，再到试验，最后到结局——为其推荐提供了一个透明、可审计的解释。这不是一个黑箱；它是一个玻璃引擎 [@problem_id:4839012]。

将知识图谱的符号世界与机器学习的统计世界联系起来的能力，是人工智能最激动人心的前沿之一。[深度学习模型](@entry_id:635298)，特别是在[医学影像](@entry_id:269649)等领域，功能极其强大，但常被批评为“黑箱”。我们可能不知道一个[卷积神经网络](@entry_id:178973)为什么判定一张肺部CT扫描显示为恶性。知识表示为实现可解释性提供了一条路径。我们可以训练一个模型，将其从图像中学到的抽象、高维特征映射到一个由像RadLex这样的医学本体定义的共享语义空间中。同时，我们可以从放射科医生的文本报告中提取概念。然后，可以训练系统不仅做出预测，还要确保它在图像中“看到”的概念与文本中描述的概念一致。这是通过在模型的训练目标中添加一个惩罚项来实现的，该惩罚项最小化两种表示之间的差异。通过这样做，我们鼓励黑箱学习与人类可理解的临床发现相对应的特征，使其更透明、更稳健、更值得信赖 [@problem_id:4829909]。

利用知识来指导学习的思想有着深刻的理论根源。从[统计学习理论](@entry_id:274291)的角度来看，添加领域知识是一种智能约束的形式。考虑一个根据数千个随时间变化的诊断代码来预测患者风险的模型。一个天真的模型将不得不独立学习每个时间点上每个代码的重要性——这是一个巨大而复杂的[假设空间](@entry_id:635539)。但是如果我们使用本体，我们就可以施加结构。我们可以将参数联系在一起，强制要求子概念（例如，不同类型的肺炎）的权重应与其父概念的权重相关。这大大减少了模型需要学习的“自由”参数的数量。通过缩小[假设空间](@entry_id:635539)，我们降低了模型的方差——即它对训练数据中噪声的敏感度。这使得它不太可能过拟合，更有可能泛化到新患者身上。当然，这需要权衡：如果我们的[本体](@entry_id:264049)是错误的，我们就会引入偏差。但是通过编码公认的医学知识，我们正在下一个很好的赌注，为模型在通往正确解决方案的道路上提供了一个强大的开端 [@problem_id:5197338]。同样的原则也适用于理解临床记录中的海量文本。通过为[主题模型](@entry_id:634705)提供基于[本体](@entry_id:264049)的信息先验——例如，告诉它一个“心脏病学”主题更有可能包含来自“症状”、“心脏药物”和“诊断测试”类别的词语——我们可以引导它发现比它自己找到的更连贯、更具临床相关性的主题 [@problem_id:5228466]。

### 门前的守护者：知识、安全与正义

知识表示的应用超出了发现和决策支持，延伸到安全和伦理的关键领域。在这里，这些系统的形式化、逻辑性提供了一种独特而强大的保障。

人工智能系统，像任何软件一样，可能受到攻击。攻击者可能会通过添加或更改代码来巧妙地操纵患者的记录，意图欺骗风险预测模型做出危险的错误决策。我们如何防御这种情况？一个纯粹的[统计模型](@entry_id:755400)可能会将一组代码标记为异常，因为它很罕见，但它无法区分“罕见但可能”和“逻辑上不可能”。而一个基于知识的系统可以。想象一下，一份电子健康记录中包含了一位82岁男性既患有良性前列腺增生又刚刚分娩的代码。一个[统计模型](@entry_id:755400)可能只会将其视为一个奇怪的、低概率事件。但是一个配备了医学本体和逻辑[推理机](@entry_id:154913)的系统会看到一个彻头彻尾的矛盾。该[本体](@entry_id:264049)包含公理：`BenignProstaticHyperplasia ⊑ Male`（良性前列腺增生 ⊑ 男性），`Delivery ⊑ Female`（分娩 ⊑ 女性），以及关键的不相交公理 $Male \sqcap Female \sqsubseteq \bot$ （任何事物都不能既是男性又是女性）。当患者的数据被断言到这个知识库中时，[推理机](@entry_id:154913)立即检测到它是不可满足的——一个逻辑上的不可能。这提供了一个确定性的、形式化的保证，即某些类型的无意义数据将被捕获，从而在我们临床系统的入口处充当了一个强大的逻辑盾牌 [@problem_id:4401482]。

这个最终的应用将我们带到了知识表示最深刻和最具挑战性的方面。我们倾向于认为我们的[本体](@entry_id:264049)和知识库是客观的、科学的产物。但它们是由人和机构创造的，不可避免地反映了构建它们的社会的价值观和盲点。这引出了**认知不公**（epistemic injustice）的概念，即知识领域本身的不公正。

再次考虑我们的数据管道。当一个说话者的可信度因偏见而被不公平地贬低时，就发生了**证言不公**（testimonial injustice）。在临床环境中，当临床医生低估来自边缘化群体患者的自我报告症状时，这种情况就可能发生。这种偏见可以被数据捕捉到，可能表现为分配给该患者证言的权重较低，或者更微妙地，表现为更高的“[标签噪声](@entry_id:636605)”率，因为他们的症状由于不被相信而被误解或错误编码。

更[隐蔽](@entry_id:196364)的是**诠释不公**（hermeneutical injustice）。这是一个结构性问题，即一个群体的经历由于集体诠释资源——我们共享的语言、我们的概念、我们本体本身——的缺乏而难以被理解。如果标准的医学词汇没有合适的词语来描述某个特定文化体验痛苦或疾病的方式，那么这些经历就会在系统中变得不可见。患者的叙述被迫塞入不合适的类别中，再次增加了[标签噪声](@entry_id:636605)，并降低了整个群体数据的质量。

这两种不公形式都可能固化在我们的AI系统中，形成一个恶性循环：质量差的数据导致针对特定群体的模型性能不佳，这反过来又导致更差的健康结果，进一步加固了最初的偏见。从这个角度看，知识表示并非一种中立的行为。我们绘制的意义地图可能有排斥性的边界，我们创建的类别可能成为牢笼。追求一个真正智能和公平的医学AI，不仅要求我们构建更好的推理器，还要求我们不断地、批判性地审视我们赋予它们的知识，并提出那个至关重要的问题：在我们的理解中，谁被遗漏了？ [@problem_id:4417426]。