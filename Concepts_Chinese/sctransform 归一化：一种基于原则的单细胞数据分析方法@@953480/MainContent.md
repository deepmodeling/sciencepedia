## 引言
单细胞 RNA 测序 (scRNA-seq) 彻底改变了生物学，使我们能够以前所未有的规模分析单个细胞的基因表达谱。然而，这项技术产生的原始数据本质上是嘈杂的，一层技术性伪影掩盖了真实的生物学变异。诸如每个细胞可变的测序深度和分子计数的统计特性等因素给分析带来了巨大挑战，混淆了结果并导致不正确的结论。尽管早期的归一化方法试图纠正这些问题，但它们往往力不从心，未能完全将生物信号从技术噪音中分离出来。

本文探讨 `sctransform`，这是一种强大且基于原则的方法，代表了单细胞[数据归一化](@entry_id:265081)的范式转变。`sctransform` 并非简单地应用一种变换，而是构建一个噪音的[统计模型](@entry_id:755400)，以数学方式将其减去，从而更清晰地揭示潜在的生物学结构。在接下来的章节中，我们将首先在 **原理与机制** 中剖析该方法背后的核心统计概念，理解它如何抑制方差和移除混杂因素。然后，我们将在 **应用与跨学科联系** 中探讨其在各个生物学科中的变革性影响，从绘制空间组织图谱到整合多组学数据集。

## 原理与机制

要真正欣赏像 `sctransform` 这样的现代归一化方法的精妙之处，我们必须首先深入探究单细胞数据的核心含义。这是一段将微弱的生物学低语从嘈杂的技术轰鸣中分离出来的旅程。

### 两种噪音的故事：分子计数的挑战

想象一下，你是一名音乐星探，任务是发现下一首热门歌曲。你派遣代理人到数千个城市，让他们收听广播并记录每首新歌被听到的次数。当报告返回时，你看到一首歌在 A 市被听了 50 次，但在 B 市只被听了 5 次。这首歌在 A 市的受欢迎程度是 B 市的十倍吗？不一定。如果你在 A 市的代理人听了 10 个小时，而在 B 市的代理人只听了 1 个小时呢？

这个简单的类比抓住了单细胞 RNA 测序 (scRNA-seq) 的核心挑战。“歌曲”是基因，“城市”是单个细胞，“收听时间”是**[测序深度](@entry_id:178191)**或**文库大小**——即我们从一个给定细胞中碰巧捕获和测序的转录本分子的总数。我们得到的原始数据，一个**[唯一分子标识符 (UMI)](@entry_id:265196)** 计数表，告诉我们我们“听到”每个基因的转录本在每个细胞中出现的次数。但是这个计数，$Y_{gi}$（基因 $g$ 在细胞 $i$ 中的计数），是两件事的混合体：基因真实的生物学丰度和纯粹的技术因素——我们对那个特定细胞测序的深度 [@problem_id:4397391]。

第一个障碍是文库大小的这种混杂效应。但第二个更微妙的挑战在于计数的本质。计数不是平滑、连续的数字；它们是离散的整数。和任何[计数过程](@entry_id:260664)一样，它们具有固有的随机性。如果一首歌的真实平均播放率为每小时 10 次，你不会每小时都恰好听到 10 次。你可能听到 8 次、12 次或 15 次。这种抽样过程的统计性质是变异的一个基本来源。

对于基因计数，这种变异有一个奇特而关键的特性：方差与均值相关。高丰度（高平均计数）的基因也比低丰度的基因具有大得多的方差。想想我们的音乐类比：一首排行榜冠军歌曲的播放次数变异在绝对值上将远大于一首不知名的独立音乐。在统计学上，这通常用**负二项 (NB) 分布**来建模，其中方差是均值的二次函数：$\mathrm{Var}(Y) = \mu + \mu^2/\theta$。这里，$\mu$ 是平均计数，$\theta$ 是一个基因特异性的**离散参数**，它捕捉了计数比简单的泊松过程 ($\mathrm{Var}(Y)=\mu$) 预测的变异性要大多少 [@problem_id:4373754]。

这种**均值-方差依赖性**是许多数据分析工具（如[主成分分析](@entry_id:145395) (PCA)）的一大难题，这些工具是为方差稳定的数据设计的。如果我们分析原始或简单缩放的计数，分析将完全被少数几个表达量最高的基因所主导，不是因为它们在生物学上最有趣，而仅仅是因为它们的高方差使它们“喊”得最响。因此，我们的任务是抑制这种方差，并将技术因素与生物因素分离开来。

### 早期尝试：拉伸和压缩数据

第一代归一化方法用直观但最终不完整的策略来处理这个问题。

最明显的想法是简单的缩放，通常称为**每百万计数 (CPM)** 归一化。逻辑很简单：如果细胞 A 的测序深度是细胞 B 的两倍，我们就将细胞 A 的所有计数除以二（或者更一般地，将每个细胞的计数除以其总文库大小，再乘以一个常数，如一百万）。虽然这看起来合理，但它从根本上未能解决均值-方差问题。它调整了均值，但方差结构仍然是扭曲的。结果是，像文库大小这样的技术因素继续污染“归一化”后的数据，这一点由诊断性测试证实，即使在 CPM 之后，基因表达和[测序深度](@entry_id:178191)之间仍存在持续的相关性 [@problem_id:2967167]。

一个更复杂的方法是**对数归一化**。通过应用像 $g(x) = \log(x+1)$ 这样的函数，我们可以“压缩”数据，对高表达基因的尺度压缩程度远大于低表达基因。这有帮助，但并非万能药。我们可以使用一个称为**delta 方法**的数学工具来看看原因。变换后变量 $g(Y)$ 的方差约等于 $(\mathrm{Var}(Y)) \cdot [g'(\mu)]^2$。对于对数函数，其导数 $g'(\mu)$ 是 $1/\mu$。对于一个负二项变量，这得到：

$$ \mathrm{Var}(\log(Y)) \approx (\mu + \mu^2/\theta) \cdot \left(\frac{1}{\mu}\right)^2 = \frac{1}{\mu} + \frac{1}{\theta} $$

[对数变换](@entry_id:267035)后数据的方差 $\frac{1}{\mu} + \frac{1}{\theta}$ *仍然依赖于均值 $\mu$*！这种依赖性较弱，但对于低计数基因尤其成问题，因为此时 $1/\mu$ 项很大 [@problem_id:4991035]。这种简单的变换虽然有帮助，但并不能真正在整个基因表达范围内稳定方差。此外，这些变换引入了它们自己的权衡；通过压缩表达范围的高端，它们可能无意中缩小了高表达基因在细胞类型之间真实但微小的生物学差异 [@problem_id:4373754]。

### 一种更具原则性的方法：建模噪音以减去它

这就是 `sctransform` 代表真正范式转变的地方。它不再是应用一种“一刀切”的变换，而是说：“让我们为每个基因的技术噪音建立一个[统计模型](@entry_id:755400)，然后用数学方法将其减去，只留下生物信号。”

完成这项工作的工具是**[广义线性模型 (GLM)](@entry_id:749787)**。你可以把它看作是你在统计学课上可能学过的线性回归的增强版，但它是专门为计数数据构建的。对于每个基因，`sctransform` 拟合一个**负二项 GLM** [@problem_id:4608298]。这个模型学习了基因的[期望计数](@entry_id:162854)与技术变量（最重要的是细胞的文库大小）之间的精确关系。模型的核心可以写成：

$$ \log(\mu_{gi}) = \beta_{g0} + \beta_{g1} \log(\text{depth}_i) $$

这里，$\mu_{gi}$ 是基因 $g$ 在细胞 $i$ 中的[期望计数](@entry_id:162854)，$\text{depth}_i$ 是文库大小。模型为每个基因拟合一个独立的截距 ($\beta_{g0}$，代表基因的基线丰度) 和斜率 ($\beta_{g1}$) [@problem_id:4991035]。这是一个关键特性。它允许模型学习到，某些基因的计数随测序深度的增加可能比其他基因更陡峭——这是简单缩放方法完全缺乏的灵活性 [@problem_id:4381636]。

当然，拟合数千个这样的模型（每个基因一个）可能很棘手。对于那些很少被检测到的基因，$\beta_{g0}$ 和 $\beta_{g1}$ 的估计值可能充满噪音且不可靠。为了解决这个问题，`sctransform` 采用了一种巧妙的技术，称为**正则化**。它通过假设具有相似总体丰度的基因应该具有相似的技术特性，从而在基因之间“借用信息”。它从所有基因中学习一个平滑的趋势，并轻轻地将每个基因的独立模型参数推向这个稳定的平均趋势。这使得最终的估计值更加稳健，尤其对于[稀疏数据](@entry_id:636194) [@problem_id:3349810]。

### 输出：剩下的（希望）是生物学

为每个基因拟合了这个复杂的模型后，我们得到了什么？输出不是传统意义上的“归一化计数”。相反，`sctransform` 提供 **Pearson 残差**。这个概念既简单又深刻：

$$ \text{残差} = \frac{\text{观测计数} - \text{预测计数}}{\text{期望标准差}} $$

让我们来解析一下。分子，即 $\text{观测值} - \text{预测值}$ ($y_{gi} - \hat{\mu}_{gi}$)，是在我们减去技术模型可以解释的那部分计数后剩下的东西。它是对真实生物学偏离基线的最佳估计。如果一个细胞表达某个基因的量超过了我们基于其文库大小的预期，这个项将是正的；如果表达量更少，它将是负的 [@problem_id:4991035]。

分母才是真正的魔力所在。我们不只是看原始差异；我们用在该表达水平下基因预期会有的变异量来缩放它。基于负[二项模型](@entry_id:275034)，这个“期望标准差”是 $\sqrt{\hat{\mu}_{gi} + \hat{\mu}_{gi}^2/\hat{\theta}_g}$。通过除以这个项，我们考虑了固有的均值-方差关系 [@problem_id:4608266]。对于一个几乎不表达的基因来说，偏离均值 10 个计数是高度显著的，但对于一个表达数千次的基因来说，这只是无意义的噪音。Pearson 残差捕捉了这种背景信息。

例如，考虑一个基因，我们的模型预测在某个特定细胞中的平均计数为 $\hat{\mu}_{gc} \approx 1.81$。如果我们观察到的计数是 $y_{gc}=0$，原始差异只是 $-1.81$。但在用期望标准差（比如说可能是 $1.57$）缩放后，最终的 Pearson 残差变为 $r_{gc} = -1.153$。这就是该基因在该细胞中的新的、“归一化”的值 [@problem_id:3349810]。

这些残差有两个优美的特性。首先，因为文库大小被用来计算预测的均值，它的影响已经被“回归掉”了，所以残差不再与[测序深度](@entry_id:178191)相关 [@problem_id:4991035]。其次，因为我们除以了标准差，*所有基因*的残差现在的方差都被稳定到大约为 1。一个高丰度的基因和一个低丰度的基因终于站在了同一起跑线上，为像 PCA 这样的下游分析做好了准备，这些分析现在可以“聆听”生物学模式，而不会被技术噪音所淹没 [@problem_id:4373754]。

### 统一的视角：从缩放到建模

我们的旅程从简单的缩放方法走向了一个成熟的[统计建模](@entry_id:272466)框架。`sctransform` 及类似方法代表了一种根本性的思想转变：从对数据的*ad hoc*（即席）操作，转变为一种有原则的努力，去理解并用数学方法移除产生数据的技术过程 [@problem_id:3348625]。

这里存在着深刻的统一性。导致问题的那个统计特性——负二项的均值-方差关系——也正是解决方案的关键。通过明确地对这种关系建模，我们可以用它来定义一个不受其影响的残差。这就是基于模型的方法的力量和精妙之处，它将一个统计上的麻烦变成了自我修正的基石。它让我们最终能够平息技术的轰鸣，开始聆听我们数据内部的生物学交响曲。

