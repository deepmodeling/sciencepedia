## 引言
几十年来，机器学习模型一直通过暴力学习的方式，需要无数的样本才能掌握人类凭直觉就能理解的概念，例如一个物体的身份不因其位置或朝向而改变。这种低效率源于一个根本性的脱节：模型本身并未意识到支配我们世界的物理对称性。本文旨在弥补这一差距，探讨将对称性直接[嵌入](@article_id:311541)[深度学习](@article_id:302462)模型架构的强大[范式](@article_id:329204)。通过内置“游戏规则”，我们不仅可以创造出数据效率更高、更鲁棒的[算法](@article_id:331821)，还能使其更符合现实的基本原理。

接下来的章节将引导您了解这一革命性的概念。首先，在 **原理与机制** 部分，我们将剖析不变性与[等变性](@article_id:640964)的核心思想，并审视用于构建对称网络的数学和架构工具，如[群卷积](@article_id:639745)。随后，**应用与跨学科联系** 部分将展示这些基于原理的模型如何改变从[材料科学](@article_id:312640)、药物发现到[计算机视觉](@article_id:298749)和[机器人学](@article_id:311041)等领域，证明将计算与宇宙规律对齐[能带](@article_id:306995)来更强大、更具洞察力的人工智能。

## 原理与机制

想象一下，您正试图教一台计算机理解世界。您可能会给它看一张猫的图片，告诉它：“这是一只猫。”然后，您可能会给它看同一张图片，但稍微向左移动了一点，然后说：“这也是一只猫。”您需要为无数的位置、大小和朝向重复这个过程。然而，一个人类小孩似乎能直观地掌握这一点。在见过一次猫之后，无论这只猫是近是远、是颠倒还是在镜子中反射，他们都能认出来。这个孩子不仅学会了猫的*模式*，还学会了世界的*对称性*——即物体的身份不会因为你移动它而改变的游戏规则。

在很长一段时间里，我们的机器学习模型就像前面提到的那个学生，通过暴力学习的方式处理无穷无尽的样本。但一个革命性的想法已经深入人心：如果我们能将这些[基本对称性](@article_id:321660)直接构建到模型的架构中呢？如果我们能设计出已经知道游戏规则的网络呢？这就是深度学习中对称性的核心思想，这一原则不仅使模型更智能、更高效，还揭示了宇宙结构与计算结构之间深刻的联系。

### 对称性的两种类型：不变性与[等变性](@article_id:640964)

在此背景下，对称性有两种基本类型。为了理解它们，让我们考虑一个物理系统，比如一个漂浮在太空中的水分子。它的势能——一个代表其内应力的单一数值——仅取决于其氢原子和氧原子的相对位置。如果你移动整个分子（**平移**）、旋转它（**旋转**）或者交换其两个相同的氢原子的标签（**[置换](@article_id:296886)**），这个能量值都保持完全不变。这种性质被称为**[不变性](@article_id:300612)**。如果一个输出在输入被变换时保持不变，那么它就是**不变的**。

但是，其他属性，比如作用在每个原子上的力呢？力是矢量，既有大小又有方向。如果你旋转水分子，作用在其原子上的力也必须随之旋转。输出不会保持不变，而是以与输入*相同的方式*进[行变换](@article_id:310184)。这种性质被称为**[等变性](@article_id:640964)**。如果一个输出在输入变换时做出可预测的响应变换，那么它就是**等变的** [@problem_id:2479779]。

这种区分不仅仅是学术上的，它是一个根本性的设计选择。想象一下，你试图构建一个模型来预测一个矢量，比如[分子偶极矩](@article_id:313069)（衡量[电荷](@article_id:339187)分离的指标），但你错误地将模型设计成纯粹不变的。假设你的模型 $g$ 是不变的，意味着 $g(\text{旋转后的输入}) = g(\text{原始输入})$。然而，你的训练数据正确地显示偶极矩矢量会随着分子的旋转而旋转。对于任何具有非零偶极矩 $\boldsymbol{\mu}$ 的分子，你的模型被要求满足两个相互矛盾的条件：
1.  来自数据：$g(\text{旋转后的输入}) \approx \text{旋转后的 } \boldsymbol{\mu}$
2.  来自其架构：$g(\text{旋转后的输入}) = g(\text{原始输入}) \approx \boldsymbol{\mu}$

这意味着 $\boldsymbol{\mu}$ 必须等于它自身旋转后的版本。除非该矢量为零或旋转是平凡的，否则这在数学上是不可能的。一个不变模型要解决在所有可能旋转下的这种冲突，唯一的办法就是对所有输入都预测一个零偶极矩！[@problem_id:2903793]。这绝佳地阐明了一个关键教训：你必须使模型的对称性与你试图学习的目标的对称性相匹配。不变模型用于不变目标（如能量），而等变模型用于等变目标（如力和其他矢量）[@problem_id:2479779] [@problem_id:2648604]。

### 具有内置对称性的架构

那么，我们如何将这些对称性构建到[神经网络](@article_id:305336)中呢？这些方法通常出人意料地优雅，并植根于简单的数学原理。

#### 处理集合：[置换对称性](@article_id:365034)

让我们从最简单的对称性开始：[置换](@article_id:296886)。我们如何设计一个网络，将一组项目视为一个*集合*，其中顺序无关紧要？这对于从分析点云到理解具有相同原子的分子等任务至关重要 [@problem_id:2760102]。如果我们的模型不是[置换](@article_id:296886)不变的，它可能会仅仅因为我们决定将原子‘A’标记为#1而不是#2，就计算出不同的能量或力，这在物理上是荒谬的 [@problem_id:2456264]。

解决方案非常简单，并且是像 **Deep Sets** 这类架构的基础。它涉及一个两步过程：
1.  对集合中的每个项目单独应用相同的变换（例如，一个小型的[神经网络](@article_id:305336)）以提取其特征。
2.  使用一个交换运算（如求和）来聚合这些特征。

因为加法是可交换的（$a+b = b+a$），所以无论你以何种顺序处理这些项目，特征的最终总和都是相同的。这个简单的配方——先变换后求和——是一种强大的方法，可以构造出一个对其输入顺序不变的函数 [@problem_id:3155388]。

这一原理也延伸到了现代人工智能中最强大的工具之一：Transformer 模型中的**[自注意力](@article_id:640256)**机制。其核心是，[自注意力](@article_id:640256)可以被看作是对一组输入标记（例如，句子中的单词或图像中的图块）的操作。每个标记通过对所有其他标记进行加权求和来计算其输出。由于这是对整个集合的求和，因此其底层操作天然是[置换](@article_id:296886)等变的。当你重新排序输入标记时，输出标记会以相同的方式重新排序 [@problem_id:3192582]。当然，对于像语言理解这样的任务，顺序*确实*很重要，这就是为什么 [Transformer](@article_id:334261) 会在输入中添加明确的“[位置编码](@article_id:639065)”——它们有意打破完美的对称性，以赋予模型序列感。

#### 处理空间：平移与旋转对称性

构建平移和旋转对称性——即欧几里得空间的对称性，通常表示为 $E(3)$——需要更复杂的工具。一个简单的起点是构造本身就是不变的输入特征。对于一个原子系统，我们可以不向网络输入原始的[笛卡尔坐标](@article_id:323143)，而是输入一个原子间距离矩阵。由于两点之间的距离不受整个系统的刚性平移或旋转的影响，任何仅使用这些距离作为输入的模型都将自动是 $E(3)$-不变的 [@problem_id:2760102]。

这是一个好的开始，但可能具有限制性。它将所有丰富的几何信息压缩成一组标量，使得学习像角度这样的方向性属性变得困难。真正的力量来自于构建其*内部操作*也尊重这些对称性的网络。

### 通用引擎：[群卷积](@article_id:639745)

图像识别中我们熟悉的[二维卷积](@article_id:338911)，也许在你没有意识到的时候，它就是一种等变操作。它对于平移**群**是等变的。**群**是数学家用来描述对称性的[形式语言](@article_id:328817)。当你在图像中滑动一个物体时，卷积滤波器检测到的特征也会随之滑动。网络不必在每个可能的位置上重新学习如何检测边缘；其[平移等变性](@article_id:640635)免费赋予了它这种能力。

**[群卷积](@article_id:639745)**将这一思想推广到其他类型的对称性。例如，我们可以设计对旋转和反射等变的卷积，比如正方形的对称性（即**二面体群**，$D_4$）。在一个 $D_4$-[等变网络](@article_id:304312)中，你可能只需要学习一个用于检测水平边缘的滤波器。然后，[网络架构](@article_id:332683)会自动创建该滤波器的其他七个旋转和反射版本（例如，用于垂直边缘、对角线边缘），而无需学习任何新参数 [@problem_id:3126226]。

这样做的实际好处是惊人的。想象一下，你有一个标准卷积层和一个 $D_4$-等变层，两者都设计用于产生64个输出特征通道。标准层必须学习64个独立的滤波器。然而，等变层可能只需要学习 $64 / 8 = 8$ 个基础滤波器，其他56个则由对称性免费生成。两个层在推理时执行相同量的计算，但等变层以**8倍更少的学习参数**达到了同样的效果 [@problem_id:3180091]。这种参数效率的巨大提升意味着模型更不容易[过拟合](@article_id:299541)，并且可以从少得多的数据中学习——这在数据生成成本高昂的科学应用中是一个巨大优势。

为了实现三维数据的完全 $E(3)$ [等变性](@article_id:640964)，我们使用了更强大的数学工具。特征不再是简单的矢量，而是表示为根据[旋转群](@article_id:383013)的**不可约表示**进[行变换](@article_id:310184)的对象，由整数 $l=0, 1, 2, \dots$ 索引（标量、矢量、[张量](@article_id:321604)等）。我们使用像**球谐函数**（球面的自然基函数）和 **Clebsch-Gordan 系数**这样的数学对象来组合这些特征，以保持它们明确定义的旋转行为 [@problem_id:2648604]。甚至网络内部的非线性[激活函数](@article_id:302225)也必须经过精心设计以保持[等变性](@article_id:640964)；例如，一个标准的 ReLU 会破坏这种精巧的对称性。事实证明，解决方案是那些与特征矢量的模长相互作用但保留其方向的函数 [@problem_id:3133446]。

### 伟大的统一

这引领我们走向一个最终的、美妙的综合。我们已经看到了构建物理系统模型的两条路径：
1.  **不变路径：** 设计一个 $E(3)$-不变网络来预测标量势能。
2.  **等变路径：** 设计一个 $E(3)$-[等变网络](@article_id:304312)来直接预测矢量力。

事实证明，这两条路径被物理定律深刻地联系在一起。在一个[保守系统](@article_id:323146)中，力是势能的负梯度（$\mathbf{F} = -\nabla E$）。恰好，一个非凡的数学定理指出，如果一个标量场 $E$ 在旋转下是不变的，那么它的梯度 $\nabla E$ 保证是等变的 [@problem_id:2479779] [@problem_id:2648604]。

这意味着，如果你成功地构建了一个完全不变的能量预测器（路径1），你只需对其进行[微分](@article_id:319122)，就能免费得到一个等变的力预测器！这个原理为构建物理一致的机器学习模型提供了一个强大而优雅的方案。它表明，通过拥抱我们世界的基本对称性，我们不仅仅是为模型添加一个理想的特性，而是将宇宙自身的逻辑片段[嵌入](@article_id:311541)到它们的设计中，创造出不仅更高效而且更真实的工具。

