## 引言
训练深度神经网络存在一个根本性挑战：当信息穿过许多层时，信号要么衰减至无，要么被无限放大。这个问题被称为[梯度消失](@article_id:642027)和[梯度爆炸问题](@article_id:641874)，它困扰了早期构建深度架构的尝试，使得在层数稍多的网络中进行学习几乎成为不可能。解决方案并不在于复杂的[算法](@article_id:331821)，而在于一个惊人地简单的起点：我们如何初始化网络的权重。一个合适的初始化方案能确保信号稳定传播，从而使梯度得以流动，即使在最深的模型中也能进行学习。

本文将探讨该领域最重要的突破之一：He 初始化。我们将剖析其理论，正是这一理论使得采用[修正线性单元](@article_id:641014)（ReLU）的[深度学习](@article_id:302462)变得实用而高效。首先，在“原理与机制”部分，我们将深入探讨 He 初始化背后优美的数学逻辑，从方差保持的核心原则出发推导其公式，并理解为何 ReLU 激活函数需要一种独特的处理方法。然后，在“应用与跨学科联系”部分，我们将看到这一理论洞见如何释放现代[深度学习](@article_id:302462)的潜力，其影响遍及从[网络架构](@article_id:332683)设计到稳定训练的实践艺术等方方面面。

## 原理与机制

想象一下，你正试图让一条长队里的人通过悄悄话传递一个秘密。第一个人悄声告诉第二个人，第二个人再告诉第三个人，依此类推。为了让信息完整地传到队尾，每个人都必须以大致相同的音量复述他们听到的内容。如果每个人说话的声音都太小，信息在到达终点前就会消失。如果每个人都说得太大声，悄悄话很快就会变成一片嘈杂的喊叫，原始信息将淹没在噪音中。

[深度神经网络](@article_id:640465)面临的正是这样的困境。“信息”是你数据中编码的信息，它通过网络层向前传播。“学习”是一个[反向传播](@article_id:302452)的误差信号，它告诉网络如何进行调整。每一层都像一个放大器。如果每一步的放大效果没有得到精细控制，这些信号要么会消失（**梯度/激活值消失问题**），要么会爆炸（**梯度/激活值爆炸问题**）。因此，训练一个深度网络，就好比要求我们那条长队里的人，在信息前向传递和反向回传的过程中，都能完美地保持信息的音量。我们如何实现这种微妙的平衡呢？

### 方差保持的黄金法则

关键在于一个简单而优美的原则。在统计学中，一个信号的“音量”或“能量”可以通过其**方差**来衡量。如果流经我们网络的信号——即激活值——要保持其强度，那么一个层的输出方差应约等于其输入方差。这就是黄金法则：**方差保持**。

让我们从单个[神经元](@article_id:324093)开始。其核心操作是计算其输入的加权和，称为预激活值，$z = \sum_{j=1}^{n} w_j x_j$。（我们暂时忽略偏置项，稍后会再讨论这一点。）假设我们的输入 $x_j$ 和权重 $w_j$ 都是从零均值分布中抽取的[独立随机变量](@article_id:337591)。那么输出 $z$ 的方差是多少？

方差有一个奇妙的性质：对于[独立随机变量之和](@article_id:339783)，和的方差等于方差之和。此外，对于两个独立的零均值变量 $A$ 和 $B$，它们乘积的方差就是它们方差的乘积：$\mathrm{Var}(AB) = \mathrm{Var}(A)\mathrm{Var}(B)$。应用这个性质，我们的预激活值 $z$ 的方差是：

$$
\mathrm{Var}(z) = \mathrm{Var}\left(\sum_{j=1}^{n} w_j x_j\right) = \sum_{j=1}^{n} \mathrm{Var}(w_j x_j) = \sum_{j=1}^{n} \mathrm{Var}(w_j) \mathrm{Var}(x_j)
$$

如果我们假设所有权重都具有相同的方差 $\sigma_w^2$，并且所有输入都具有相同的方差 $\sigma_x^2$，那么这个方程可以优美地简化为 [@problem_id:3123395]：

$$
\mathrm{Var}(z) = n \sigma_w^2 \sigma_x^2
$$

这里，$n$ 是[神经元](@article_id:324093)的输入数量，即其“[扇入](@article_id:344674)”（fan-in）。这个方程是我们的万能钥匙。它告诉我们信号在通过[神经元](@article_id:324093)的线性部分后方差如何变化。为了保持信号稳定，我们希望输出的方差与输入的方差相同，即 $\mathrm{Var}(z) = \sigma_x^2$。将此代入我们的主方程，得到稳定性的条件：

$$
\sigma_x^2 = n \sigma_w^2 \sigma_x^2 \quad \implies \quad n \sigma_w^2 = 1
$$

这导出了一个著名的初始化策略：我们应该从方差为 $\sigma_w^2 = \frac{1}{n}$ 的分布中抽取权重。这就是 **Xavier** 或 **Glorot 初始化**背后的核心思想。它专为像[双曲正切函数](@article_id:638603)（$\tanh$）这类在零点附近表现得像线性函数的[激活函数](@article_id:302225)而设计 [@problem_id:3094653]。对于这些较早的[激活函数](@article_id:302225)，设置 $\mathrm{Var}(w) = 1/n$ 是训练更深网络的秘诀。但[深度学习](@article_id:302462)的世界即将被颠覆。

### ReLU 革命与消失的另一半

接着是**[修正线性单元](@article_id:641014)**（**ReLU**）的登场：$f(z) = \max(0, z)$。它极其简洁——仅仅将所有负值截断为零——这使得它非常高效和强大。但这种截断对我们的方差计算产生了深远的影响。ReLU 不是线性的，在零点附近也远非线性。它粗暴地砍掉了信号定义域的一半。

这对​​方差有什么影响？假设我们的预激活值 $z$ 是一个均值为零的对称信号（例如高斯分布，这是对许多随机输入求和的一个合理假设）。ReLU 函数让信号的正半部分保持不变，但将负半部分置零。直观上，这必然会降低信号的能量。降低了多少呢？

输出的“能量”或二阶矩 $\mathbb{E}[\mathrm{ReLU}(z)^2]$，是通过对 $z$ 的分布进行积分来计算的。由于负半部分变为零，我们只需对正半部分进行积分。根据对称性，这个积分恰好是整个定义域上积分的一半。换句话说，输出的二阶矩是输入的二阶矩的一半 [@problem_id:3167810]：

$$
\mathbb{E}[(\max(0, z))^2] = \frac{1}{2} \mathbb{E}[z^2]
$$

由于均值接近于零，我们可以用二阶矩来近似方差。该层输出激活值的方差，我们称之为 $\mathrm{Var}(a)$，现在与输入方差 $\sigma_x^2$ 的关系如下：

$$
\mathrm{Var}(a) \approx \frac{1}{2} \mathrm{Var}(z) = \frac{1}{2} (n \sigma_w^2 \sigma_x^2)
$$

现在，我们再次应用黄金法则：我们希望输出激活值的方差等于输入方差，即 $\mathrm{Var}(a) = \sigma_x^2$。

$$
\sigma_x^2 = \frac{1}{2} n \sigma_w^2 \sigma_x^2 \quad \implies \quad \frac{1}{2} n \sigma_w^2 = 1
$$

求解权重方差，我们得到了新的方案：

$$
\sigma_w^2 = \frac{2}{n}
$$

这就是 **He 初始化**，以其发现者 Kaiming He 的名字命名。这个神秘的因子 2 根本不神秘！它正是对 ReLU [激活函数](@article_id:302225)丢弃的那一半信号的精确数学补偿。这是一个绝佳的例子，说明了激活函数的性质如何直接决定了正确的初始化策略 [@problem_id:3094653] [@problem_id:3123395]。

### 回声：在反向传播中保持梯度

保持前向信号的健康只是成功的一半。学习发生在**反向传播**期间，此时一个[误差信号](@article_id:335291)，即梯度，会反向穿过整个网络。这个“回声”也必须被保留下来。如果它消失了，网络的早期层将学不到任何东西。如果它爆炸了，训练过程将变得不稳定。

梯度的传播也涉及到与网络权重的相乘。类似的分析表明，梯度的方差在每一层都会被一个因子缩放，这个因子取决于权重和激活函数的*[导数](@article_id:318324)* $\phi'(z)$ [@problem_id:3125165]。梯度稳定的条件是这个[缩放因子](@article_id:337434)应该为 1。这个因子结果是：

$C = n \, \mathrm{Var}(w) \, \mathbb{E}[(\phi'(z))^2]$

对于 ReLU，其[导数](@article_id:318324)很简单：当 $z>0$ 时为 1，当 $z \le 0$ 时为 0。假设我们的预激活值是对称的，那么 $z>0$ 的情况有一半时间发生。所以，[期望](@article_id:311378)的[导数](@article_id:318324)平方是：

$$
\mathbb{E}[(\phi'(z))^2] = 1^2 \cdot P(z>0) + 0^2 \cdot P(z \le 0) = \frac{1}{2}
$$

现在让我们看看我们的两种初始化方案会发生什么：
- **Xavier 初始化与 ReLU 结合**：$\mathrm{Var}(w) = 1/n$。梯度[缩放因子](@article_id:337434)为 $C = n \cdot (\frac{1}{n}) \cdot (\frac{1}{2}) = \frac{1}{2}$。梯度信号在每一层都会被*减半*！在一个 10 层的网络中，它会缩小约 $2^{10} \approx 1000$ 倍。[梯度消失](@article_id:642027)了。
- **He 初始化与 ReLU 结合**：$\mathrm{Var}(w) = 2/n$。梯度缩放因子为 $C = n \cdot (\frac{2}{n}) \cdot (\frac{1}{2}) = 1$。梯度方差被完美地保留了下来！[@problem_id:3125165] [@problem_id:3180442]。

这证实了：为了保持[前向传播](@article_id:372045)信号所需的那个因子 2，*也*恰好是保持[反向传播](@article_id:302452)梯度信号所需要的。这个解决方案是优美对称的。

### 当理论遇上纷繁现实

这个优雅的理论建立在一些理想化的假设之上，比如输入具有单位方差。当纷繁复杂的现实世界介入时，会发生什么呢？

- **不守规矩的输入**：假设由于预处理错误，我们的输入数据没有被[归一化](@article_id:310343)，其方差为 $c^2$ 而不是 1，其中 $c$ 很大。He 初始化旨在将输入方差 $\sigma_x^2$ 转化为输出方差 $\sigma_x^2$。但现在，第一层激活值的方差变成了 $c^2$。下一层继承了这个巨大的方差并将其保持下去。网络中所有的激活值都会变得巨大。这会导致[梯度爆炸](@article_id:640121)，学习过程可能会失控。存在一个临界缩放因子 $c_{\mathrm{crit}}$，在该值下，第一次权重更新的大小就已与初始权重本身相当，导致完全的不稳定 [@problem_id:3111792]。这表明，正确的**[数据归一化](@article_id:328788)**不仅仅是一个有用的技巧；它是一个健全的初始化策略的必要伙伴。

- **恼人的偏置项**：我们一直忽略了偏置项 $b$。完整的预激活值是 $z = \mathbf{w}^{\top}\mathbf{x} + b$。如果我们从一个方差为 $\sigma_b^2$ 的分布中随机初始化偏置，这个方差将直接加到预激活值的方差上：$\mathrm{Var}(z) = n \sigma_w^2 \sigma_x^2 + \sigma_b^2$ [@problem_id:3199849]。这将打破我们精心校准的平衡。这就是为什么偏置几乎总是被初始化为一个常数，通常是 0，它对​​方差没有任何贡献。

- **一个原则，而非一个规则**：He 初始化中的因子 2 并不是一个神奇的数字。它是 ReLU 性质的直接结果。如果我们使用不同的激活函数，数学计算就会改变。考虑 **[PReLU](@article_id:640023)**（参数化 ReLU），其定义为 $f(z) = \max(z, \alpha z)$，其中 $\alpha$ 是负斜率的一个小的可学习参数。通过应用同样的方差保持原则，我们可以为其推导出正确的初始化方法 [@problem_id:3197644]：
    $$
    \sigma_w^2 = \frac{2}{n(1+\alpha^2)}
    $$
    注意，如果 $\alpha=0$，我们就恢复了标准的 He 初始化。如果 $\alpha=1$（一个线性函数），我们得到 $\sigma_w^2 = 1/n$，恢复了 Xavier 初始化。这展示了其基本概念的真正力量：它是一个可以适应新架构和激活函数的通用原则，而不仅仅是一个需要记住的固定规则。

我们甚至可以观察这些原则的实际作用。如果我们进行计算机模拟，我们可以凭经验测量深度网络每一层激活值和梯度的方差。正如预测的那样，当我们将 He 初始化与 ReLU 配对时，即使在非常深的网络中，方差也能优美地稳定在接近 1 的水平。而当我们使用不匹配的 Xavier 初始化时，我们可以亲眼看到方差逐层骤降至零，这是信号消失的清晰标志 [@problem_id:3194508] [@problem_id:3199598]。眼见为实，这些实验为这个优雅的理论提供了惊人的证实。

