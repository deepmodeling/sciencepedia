## 应用与跨学科联系

在前面的讨论中，我们揭示了 He 初始化背后的优雅原则：为了训练一个由[修正线性单元](@article_id:641014)（ReLU）构成的深度网络，我们必须仔细设置权重的初始方差，以保持通过它的信号的方差。这是一个简单而优美的规则，源于一个直接的计算。但一个科学原则的真正美妙之处不仅在于其简单性，还在于其力量——它所能解释的现象之广，以及它让我们能够构建的新世界之多。现在我们有了这把钥匙，让我们看看它能打开多少扇门。我们会发现，这一个简单的想法回响在历史和现代[网络架构](@article_id:332683)之中，为训练的实用艺术提供了信息，甚至与关于学习何以可能的最深层理论问题联系在一起。

### [信号传播](@article_id:344501)的艺术：征服深度

想象一条长队在玩“传话游戏”。第一个人把信息悄悄告诉第二个人，第二个人再告诉第三个人，依此类推。到最后，信息往往变得面目全非或消失殆尽。这正是困扰早期[深度神经网络](@article_id:640465)的问题。“信息”是误差梯度，一个至关重要的信号，它告诉网络最前面的层如何调整它们的权重。在一个初始权重选择不当的深度网络中，这个梯度信号在从输出层[反向传播](@article_id:302452)时会指数级缩小。当它到达输入层时，它会变得微弱到毫无用处。最开始的几层将永远学不到东西。这就是臭名昭著的*[梯度消失问题](@article_id:304528)*。

He 初始化是这个游戏的完美解决方案。它就像队伍中每个人（或每一层）旁边的一个小巧而精确调谐的放大器。通过将权重方差设置为 $\sigma_w^2 = 2/n_{\text{in}}$，我们确保平均而言，梯度在反向通过每个 ReLU 层时其方差得以保持。一项详细的分析，例如我们可以在一个经典的架构如 [LeNet-5](@article_id:641513)（如果用 ReLU 对其进行现代化改造）上进行的分析，表明这种初始化方案能够使梯度幅度一直保持健康，直到输入层 ([@problem_id:3118616])。信息完整地到达，整个网络可以协同学习。

[信号完整性](@article_id:323210)这一原则对于[前向传播](@article_id:372045)同样至关重要。考虑一个[生成对抗网络](@article_id:638564)（GAN），这是一种从随机噪声向量中学习创造新数据（如图像）的模型。生成器网络就像一个雕塑家，从一块无形的“数字黏土”（噪声）开始，通过多层逐步将其精炼成可识别的图像。如果激活值——即网络正在构建的特征——的方差在每一层都缩小，黏土实际上会“变硬”并失去细节。网络的[表达能力](@article_id:310282)将会崩溃，只能产生模糊不清的混乱图像。使用像 Xavier 这样的初始化方法（专为类线性激活函数设计）与 ReLU 单元结合，会导致正是这种[信号衰减](@article_id:326681)。然而，一种调整过的 He 初始化可以确保特征的方差在每一层都得以维持，保持黏土的可塑性和丰富的潜力，从而允许创造出复杂而精细的图像 ([@problem_id:3112706])。

这一原则的力量是许多现代、极深度架构的基石。例如，在 [DenseNet](@article_id:638454) 中，每一层的输出都与后续所有层的输入连接在一起。这种大规模的[特征重用](@article_id:638929)使得网络可以达到惊人的深度，但这也意味着网络在不断创造新的特征，这些特征被添加到不断增长的信息“河流”中。He 初始化保证了由卷积层生成的每一条新的特征支流都能以健康、[标准化](@article_id:310343)的方差汇入主河，从而防止整体信号被稀释或爆炸 ([@problem_id:3114068])。正是这个简单的规则使得如此复杂的信息高速公路成为可能。

### 超越理想：驾驭训练的现实

虽然确保稳定的信号方差是一个美好的理想，但实际的训练过程要混乱得多。在这方面，初始化也在应对挑战方面扮演着惊人地实用的角色。

ReLU [神经元](@article_id:324093)的一个特有病症是“ReLU 单元死亡”问题。一个 ReLU 单元只有在其输入为正时才会产生非零输出（并因此产生非零梯度）。如果一个[神经元](@article_id:324093)的权重发生变化，导致其对于所有典型的训练样本输入都持续为负，它将停止输出任何东西，其梯度将为零，并且它将不再学习。它实际上死亡了。虽然 He 初始化并非完美的治疗方法，但它是一项至关重要的预防措施。通过将初始预激活值围绕零点以健康的方[差分](@article_id:301764)布，它给了每个[神经元](@article_id:324093)一个“战斗的机会”——大约 50% 的概率——从一开始就处于活跃状态。这使得[神经元](@article_id:324093)“出生即死亡”或立即陷入无法恢复的非活动状态的可能性大大降低 ([@problem_id:3099772])。

此外，初始化对学习问题的几何形状本身有着深远的影响。想象一下，[损失函数](@article_id:638865)是一个广阔、多山的地形。训练的目标是找到最低的山谷。在训练开始时，权重是随机的，这个地形通常极其混乱和崎岖，充满了悬崖和峡谷。如果我们试图下降得太快（通过使用大的学习率），我们模型的参数可能会被甩出悬崖，导致数值爆炸。这个地形的初始“崎岖程度”，在数学上由海森矩阵的[特征值](@article_id:315305)捕捉，直接受到初始权重尺度的影响。与其它初始化方法相比，He 初始化可能导致更大的初始权重尺度，从而可能导致更崎岖的初始地形。这一理解告诉我们*为什么*一个常见的实用技巧——*[学习率预热](@article_id:640738)*——如此有效。我们必须从非常小的步长（小的学习率）开始，以感受局部地形，然后才能安全地加速我们的下降 ([@problem_id:3143326])。初始化理论使我们能够将单个权重的微观统计数据与[损失景观](@article_id:639867)的宏观几何联系起来，将一个黑箱技巧转变为一个可理解的工程原理。

### 架构的宇宙：科学定律的特异性

人们很容易将 He 初始化视为适用于任何非饱和[激活函数](@article_id:302225)的普适定律。但科学往往比这更微妙、更美丽。He/ReLU 的配对是一个具体而优雅的解决方案，通过研究它*不*适用的情况，我们揭示了一个更深层次的原则：网络组件之间亲密的、舞蹈般的耦合关系。

考虑一下 Transformer，这个已经彻底改变了[自然语言处理](@article_id:333975)及更广泛领域的架构。它的核心是[缩放点积注意力](@article_id:641107)机制，它允许模型权衡不同输入元素的重要性。即使在这个截然不同的背景下，初始化仍然很重要。如果使用像 He 这样的初始化方案来初始化将输入投影为查询和键的矩阵，那么注意力“logits”的初始方差将会更大。这导致从一开始就有一个熵更低、更“尖锐”的注意力分布——模型在生命之初就已经对应该关注哪里有了“主见”。而像 Xavier 初始化这样的替代方案会产生方差更低的 logits 和一个更均匀、“不可知”的初始注意力。两者并非普遍优劣之分，但这一选择对训练的稳定性和性能有着深远的影响，实际上是设定了模型的初始“好奇心” ([@problem-id:3172410])。

对这种特异性最精彩的阐释来自[自归一化](@article_id:640888)神经网络（SNNs）的理论。研究人员设计了一种新的[激活函数](@article_id:302225)，即缩放[指数线性单元](@article_id:638802)（SELU），它具有近乎神奇的特性：如果满足某些条件，网络在激活值传播时会主动将其均值和方差引导回一个稳定的不动点（均值 0，方差 1）。这种“[自归一化](@article_id:640888)”行为省去了显式[归一化层](@article_id:641143)的需要。但这种魔力只有在一组非常特定的条件下才能起作用。激活函数必须有精确计算的常数，而且至关重要的是，权重*不能*使用 He 初始化。相反，它们需要一种不同的方案（LeCun 初始化，权重方差为 $\sigma_w^2 = 1/n_{\text{in}}$），这种方案与 SELU 的数学特性完美协调。使用 He 或 Xavier 初始化会打破这个魔咒 ([@problem_id:3200110])。这是一个美丽的教训。没有单一的“最佳”初始化方法，只有初始化和[激活函数](@article_id:302225)和谐共处的配对。He 和 ReLU 就是这样一对完美的搭档；LeCun 和 SELU 则是另一对。

### 现代前沿：[神经正切核](@article_id:638783)

最后，让我们放大到最现代、最抽象的视角。初始化如何影响网络能够学习的函数的本质？一个强大的理论工具，称为[神经正切核](@article_id:638783)（NTK），提供了答案。在某个特定范畴（无限宽度）下，神经网络的训练动态可以被一个固定的核完美描述，这个核完全在初始化时就已确定。这个核 $K(x, x')$，衡量了通过网络梯度结构这个镜头看到的两个输入 $x$ 和 $x'$ 之间的某种“相似性”。

至关重要的是，[权重初始化](@article_id:641245)的选择直接塑造了这个核。从 Xavier 初始化切换到 He 初始化会改变 NTK 的值 ([@problem_id:3199592])。由于核的[特征值](@article_id:315305)控制着学习的速度，其结构决定了网络最终收敛到的函数，因此初始化的作用远不止是防止[梯度爆炸](@article_id:640121)那么简单。它本质上是在网络诞生时就选择了它的“学习 DNA”。它定义了网络认为容易学习的[函数空间](@article_id:303911)，并影响了它将通过该空间所走的路径。

从一个防止悄悄话消失的简单规则出发，我们经历了一段非凡的旅程。我们看到了 He 初始化如何使构建深度学习的宏伟大厦成为可能，它如何为驾驭险恶训练地形的实用艺术提供信息，以及它如何揭示网络各部分之间微妙而具体的和谐关系。最终，我们发现它与网络是什么以及它如何学习的最深层问题联系在一起。这个设定随机数方差的简单规则，原来是一根线，只要一拉，就能帮助我们解开现代人工智能这幅美丽而错综复杂的织锦。