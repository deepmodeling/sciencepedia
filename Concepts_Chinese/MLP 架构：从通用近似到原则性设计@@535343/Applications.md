## 应用与跨学科联系

在上一章中，我们探讨了多层感知机的原理和机制，惊叹于其作为通用近似器的强大能力。这是一个了不起的结论，表明一个足够大的网络原则上可以学习几乎任何函数。但任何物理学家或工程师都知道，“原则上”与“实践中”相去甚远。[深度学习](@article_id:302462)的真正艺术和科学不在于仅仅知道解决方案*存在*，而在于高效、可靠地找到它，并以一种有意义的方式实现。

正是在这里，*架构*的概念走到了舞台中央。网络的架构不仅仅是可学习参数的容器；它是一种先验知识的形式，是对当前问题的一种信念陈述。一个精心设计的架构不仅仅使学习成为可能，它使学习变得*容易*。它引导网络走向合理的解决方案，远离荒谬的结果。在本章中，我们将踏上一段应用巡礼，探索深思熟虑的架构设计如何在神经网络的抽象世界与科学和工程的具体、结构化世界之间搭建桥梁。

### 架构作为通往物理世界的桥梁：拥抱对称性

物理学中最深刻的原则之一是对称性。如果我们平移实验室、旋转实验，或者更神秘地，交换两个像电子一样的相同粒子，自然法则并不会改变。一个不尊重这些对称性的物理模型，坦率地说，是错误的。

那么，当我们将一个朴素的多层感知机应用于一个物理系统时，会发生什么呢？让我们考虑一个简单而美丽的分子：苯，它有六个碳原子和六个氢原子[排列](@article_id:296886)成一个完美的六边形。我们可以尝试通过将所有 12 个原子的 3D 坐标输入一个标准 MLP 来预测其能量。但是我们该如何排序这些原子呢？我们可能会先列出第一个碳原子，然后是第二个，以此类推。网络将学会输入槽 1-3 中的坐标属于“原子 1”，4-6 中的坐标属于“原子 2”，等等。

但这简直是物理学家的噩梦！自然界不会给它的原子贴标签。如果我们交换两个相同的碳原子，分子在物理上是完全相同的，但我们输入给 MLP 的向量却截然不同。一个对不同输入槽应用不同权重的标准 MLP，几乎肯定会预测出不同的能量。它未能掌握问题的一个[基本对称性](@article_id:321660)：[置换](@article_id:296886)不变性 ([@problem_id:2457453], [@problem_id:1426741])。

这个失败不是一个小细节，而是一个灾难性的缺陷。它告诉我们，简单的、全连接的 MLP 架构与[多粒子系统](@article_id:371671)的物理特性从根本上不匹配。我们需要一个更好的想法。

而这个想法是多么美妙！与其对抗对称性，我们可以将其直接构建到网络中。想象一下，我们想要处理一个“袋子”或点的集合，其中顺序无关紧要。我们可以分三个阶段设计一个网络：
1.  首先，我们将每个点*独立地*通过一个相同的 MLP（我们称之为 $\phi$）来提取其特征表示。
2.  接下来，我们使用一个可交换的（即不关心顺序的）操作来聚合这些表示。最简单的此类操作是求和。
3.  最后，我们将这个单一的、聚合后的表示通过最后一个 MLP（$\rho$）来得到我们[期望](@article_id:311378)的输出。

完整的函数看起来像 $f(\{x_i\}) = \rho(\sum_i \phi(x_i))$。因为求和不关心其项的顺序，整个架构在构造上就是[置换](@article_id:296886)不变的！这种“Deep Sets”架构是一种处理无序数据的极其简单而强大的方法，从 3D 视觉中的点云到物理学中的粒子集都适用 ([@problem_id:3155388])。

这种内建对称性的原则具有令人难以置信的普适性。考虑一个工程问题，比如预测对称载荷下一个方形板的应力和应变。物理解决方案也必须是对称的。如果我们将板旋转 $90^\circ$，[位移场](@article_id:301917)应该也随之旋转。这个属性被称为*[等变性](@article_id:640964)（equivariance）*。我们可以设计出在数学上强制执行此属性的网络层。通过使用群论的工具，我们可以构建线性层和激活函数，以保证当输入变换时，输出也能正确地变换。一个由这些等变层构建的网络将*只能*表达尊重问题几何形状的函数，从而极大地缩小搜索空间，并带来更准确、更符合物理直觉的解决方案 ([@problem_id:2668946])。

### 架构作为领域约束的语言

除了基本对称性，许多现实世界的问题还带有一套“常识性”规则。在金融领域，我们[期望](@article_id:311378)更高的债务收入比应该增加一个人的[信用风险](@article_id:306433)，而不是降低它。在医学上，更高剂量的毒素不应导致更低的伤害概率。然而，一个标准的 MLP 没有这种常识。在训练过程中，它可能会找到一个奇怪的、非单调的关系，虽然拟合了训练数据，但违反了这些基本原则，使得模型不可信赖。

架构再次提供了答案。如果我们希望一个函数相对于某个输入是单调不减的，我们可以通过约束网络来强制实现这一点。一个关键的洞见是，非递减激活函数（如 ReLU）和具有非负权重的[仿射变换](@article_id:305310)的组合，其结果是一个[非递减函数](@article_id:381177)。因此，我们可以设计一种特殊的 MLP 架构，其中从单调输入到输出的所有路径上的权重都被强制为非负。这可以通过在每次训练更新后简单地将任何负权重钳位到零来实现。通过将模型分为一个用于单调特征的受约束分支和一个用于其他特征的无约束分支，我们可以构建出既灵活强大又遵守基本领域知识的模型 ([@problem_id:3155469])。

这种将灵活学习与硬性约束相结合的思想，在混合的、[物理信息](@article_id:312969)模型中得到了终[极体](@article_id:337878)现。想象一下试图为一个复杂的化学[系统建模](@article_id:376040)，比如盐水。离子间的相互作用主要由长程静电力主导，它遵循简单而优雅的[平方反比定律](@article_id:323218)，$q_i q_j / r_{ij}$。然而，在短程范围内，量子力学和水分子的复杂形状创造了一个混乱、复杂的[力场](@article_id:307740)。

让一个单一的 MLP 去学习这两种现象是一项艰巨的任务。网络可能难以完美捕捉简单的 $1/r$ 尾部，同时又要拟合复杂的短程数据。一种更为优雅的方法是合作。我们让一个专门的、基于物理的[算法](@article_id:331821)（如[粒子网格埃瓦尔德方法](@article_id:305795)）来处理它所擅长的、清晰的[长程静电学](@article_id:300301)。然后，MLP 的任务就只是学习相互作用中剩余的短程部分，这正是它擅长的那种复杂的、局部化的函数。总能量是物理模型的输出和神经网络输出的总和。这种混合架构不仅效果更好；它代表了对问题更深刻的理解，将正确的工作分配给了正确的工具 ([@problem_id:2457456])。

### 组合的艺术：专门化架构

简单的 MLP 不仅是最终产品；它还是一个基本的构建块，一个用于构建更复杂、更专业化机器的“乐高积木”。

一个绝佳的例子是**孪生网络（Siamese network）**。假设你想确定两个蛋白质序列是否具有进化上的[亲缘关系](@article_id:351626)（同源）。你可以尝试构建一个巨大的网络，将两个序列作为一个庞大的输入，但这很笨拙。一个更优雅的想法是使用两个具有共享权重的相同编码器（即“孪生”）。每个编码器处理其中一个序列，将其可变长度的氨基酸字符串映射到某个抽象空间中的一个固定大小的[嵌入](@article_id:311541)向量。因为编码器共享权重，它们学会了将相似的蛋白质映射到这个空间中的邻近点。然后我们可以测量这两个向量之间的距离或角度来计算相似性得分。这种架构非常适合学习相似性，这是科学技术中无处不在的任务 ([@problem_id:2373375])。

现代研究不断发现创造性地组合类 MLP 结构的方法。例如，**MLP-Mixer** 架构提出了一种有趣的分工，用于处理具有多个特征的数据（如电子表格中的一行或图像中的图块）。它在两种类型的 MLP 层之间交替：“令牌混合（token-mixing）”层允许信息在不同特征*之间*流动，“通道混合（channel-mixing）”层则处理每个特征表示*内部*的信息。这种明确的关注点分离为网络如何建立对输入的复杂理解提供了不同的视角，激发了关于网络中深度和宽度作用的新思考方式 ([@problem_id:3098873])。

### 隐藏的美：架构学到了什么

也许架构最迷人的方面不在于我们输入了什么，而在于涌现出了什么。有时，一个在简单任务上训练的网络会自行发现问题的深层、潜在结构。

考虑数字通信中的[纠错](@article_id:337457)问题。一条被编码为比特串的消息通过一个嘈杂的[信道](@article_id:330097)发送，可能会在到达时有一些比特被翻转。像著名的[汉明码](@article_id:331090)这样的纠错码会添加精心设计的冗余比特，以便可以恢复原始消息。经典的解码[算法](@article_id:331821)涉及通过将接收到的向量与一个特定的[奇偶校验矩阵](@article_id:340500) $H$ 相乘来计算一个“[伴随式](@article_id:300028)”。得到的[伴随式](@article_id:300028)唯一地标识了错误的位置。

现在，让我们尝试一些不同的东西。我们训练一个非常简单的、单隐藏层的 MLP 来做一个解码器。我们向它展示数百万个损坏的码字和正确的原始消息的例子。我们不教它任何关于[奇偶校验矩阵](@article_id:340500)或伴随式的知识。我们只是要求它最小化错误。

训练结束后，我们可以窥探一下这个“黑箱”的内部。我们发现的结果是惊人的。MLP 的隐藏层[实质](@article_id:309825)上已经学会了计算[伴随式](@article_id:300028)。连接输入与隐藏[神经元](@article_id:324093)的权重向量已经自行组织起来，模仿了[奇偶校验矩阵](@article_id:340500) $H$ 的行。网络不仅仅是记住了输入-输出对。它发现了用于解码该码的[基本数](@article_id:367165)学[算法](@article_id:331821) ([@problem_id:3155518])。这是一个深刻的结果。它表明，当学习由正确的架构和正确的数据引导时，它不仅仅是拟合一条曲线；它是一个发现的过程。

### 作为物理学家和工程师的架构师

我们已经看到，设计一个 MLP 架构是一项丰富而富有创造性的工作，融合了来自物理学、数学和计算机科学的原理。但我们的旅程结束于任何真实世界项目开始的地方：实际的约束。

一个理论上很美的架构在计算上可能是不可行的。一个更深或更宽的网络可能会带来更高的准确性，但代价是延迟增加，使其不适用于自动驾驶汽车的实时需求。“最佳”架构不是绝对的；它是在[帕累托前沿](@article_id:638419)上的一个点，是像准确性和速度这样相互竞争的目标之间的一种权衡。最终的选择取决于具体情境：用于数据中心大型服务器的架构将与为低[功耗](@article_id:356275)手机或微型传感器设计的架构大相径庭 ([@problem_id:3157506])。

因此，现代架构师必须身兼两职：既是寻求捕捉问题基本结构的优雅模型的物理学家，又是在成本、[功耗](@article_id:356275)和时间的无情约束下必须构建一个可行系统的工程师。事实证明，简单的多层感知机是一个深刻且无穷迷人的对话的起点，这个对话关乎什么是可能的与什么是实际的。