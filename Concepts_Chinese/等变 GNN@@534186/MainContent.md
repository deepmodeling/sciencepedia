## 引言
要构建物理世界的智能模型，我们必须教会它们物理世界的基本规则。这些规则中最基础的就是对称性——无论你的观察视角或位置如何，物理定律都是相同的。标准的机器学习模型通常难以掌握这一概念，需要海量数据来学习本应是其固有属性的东西。本文旨在弥合这一差距，探讨[等变图神经网络](@article_id:641098)（Equivariant Graph Neural Networks, GNNs），这是一类从头开始设计，旨在理解和尊重对称性语言的模型。

本文将引导您了解这一强大框架的核心概念。在第一部分 **原理与机制** 中，我们将解析[等变性](@article_id:640964)的概念，从熟悉的[卷积神经网络](@article_id:357845)（CNN）例子入手，逐步过渡到对 GNN 至关重要的[置换对称性](@article_id:365034)和几何对称性。您将学习到这些模型是如何被构建来处理标量、向量及其他几何对象的。随后，在 **应用与跨学科联系** 部分，我们将展示这些原理如何彻底改变科学发现，从预测分子模拟中的力、区分手性分子，到分析材料和实现更鲁棒的机器人技术。读完本文，您将理解，将对称性[嵌入](@article_id:311541)人工智能不仅仅是一种架构选择，更是创造能够对物理世界进行推理的模型的根本一步。

## 原理与机制

想一想你玩过的任何游戏——国际象棋、篮球，不胜枚举。要想玩得好，你不能只对当前局势做出反应，你必须理解规则。你知道主教总是沿对角线移动，球必须保持在场内。一个试图预测世界却不理解其基本规则的模型，就像一个从未听说过三分线的球员，注定会犯下愚蠢且可避免的错误。在物理世界中，最基本的规则就是对称定律。一个等变 GNN，本质上就是一个从头开始就被教会了这些规则的模型。

### 熟悉的旋律：图像中的对称性

在我们深入探讨图和分子的复杂性之前，让我们先用一个更简单、更熟悉的概念热身：在图片中寻找物体。想象一下，你正在构建一个计算机程序，用于在一段长文本（比如 DNA 序列）中寻找一个特定的字母序列——一个“基序”（motif）。这个基序可能出现在任何地方。为一个在字符串开头寻找该基序的探测器、一个用于中间位置的探测器和另一个用于末尾的探测器分别进行训练，这合理吗？当然不合理。它是同一个基序，只是位置不同。你想要的是一个单一、高效的探测器，可以沿着整个字符串滑动。

这就是**[平移等变性](@article_id:640635)**（translational equivariance）的核心思想，这一特性使得[卷积神经网络](@article_id:357845)（CNNs）如此强大。如果你移动输入（DNA 序列），网络对该序列的内部表示也会移动完全相同的量。一个学会在某个位置识别基序的滤波器，会自动识别它在任何其他位置的出现。这是通过一个极其简单的机制实现的：**[权重共享](@article_id:638181)**（weight sharing）。网络不是为每个位置学习数百万个独立的参数，而是学习一个单一、紧凑的滤波器（一组权重），并将其应用于整个输入。这不仅计算效率高，而且数据效率极高。模型能够从一个基序的例子泛化到其所有可能的位置，这在训练数据稀缺时是一个巨大的优势 [@problem_id:2373385]。

这揭示了一种强大的设计模式。卷积层是**等变的**（equivariant）——它们追踪特征的*位置*。但通常，我们想问的最终问题是*不变的*（invariant），意味着它不依赖于位置。对于我们的 DNA 问题，问题不是“基序是否在位置 42？”，而是“基序是否*存在于任何地方*？”为了实现这一点，我们可以取来自 CNN 的等变特征图，并应用一个**全局池化**（global pooling）操作，比如取所有位置上的最大激活值。这最后一步会折叠位置信息，给我们一个单一的、**不变的**（invariant）答案。这种从等变表示到不变预测的两步舞，是构建对称性感知模型中反复出现的主题 [@problem_id:2373385]。

### 图的[重排](@article_id:369331)：从网格到通用连接

现在，让我们离开图像或文本字符串那种整齐有序的网格，进入更为自由的图世界。一个图，比如社交网络或分子，是由其连接定义的，而不是由某种预先设定的顺序。当化学家将一个分子保存到文件中时，原子会被赋予任意的索引——原子 1、原子 2，等等。如果你打开那个文件并重新标记所有原子，你丝毫不会改变这个分子。它仍然是同一个物理实体。

这种对称性——即在不改变图身份的情况下自由[重排](@article_id:369331)节点标签——被称为**[置换对称性](@article_id:365034)**（permutation symmetry）。一个旨在理解图的[图神经网络](@article_id:297304)（GNN）必须尊重这一点。如果我们在输入中[重排](@article_id:369331)节点，那么这些节点的输出特征也必须以完全相同的方式被[重排](@article_id:369331)。这就是**[置换](@article_id:296886)[等变性](@article_id:640964)**（permutation equivariance）。未能强制执行这一点意味着模型可能会认为 1 号原子有某种特殊性，这对于任何科学模型来说都是一个致命的缺陷。

我们如何构建一个天生就能理解这种“[重排](@article_id:369331)”对称性的 GNN 呢？答案在于[消息传递](@article_id:340415)机制。在每一层中，一个节点从其邻居接收“消息”，将它们聚合起来，并用结果更新自己的特征。为了实现[置换](@article_id:296886)[等变性](@article_id:640964)，聚合函数必须是**可交换的**（commutative）——其结果不能依赖于输入的顺序 [@problem_id:2395438]。例如，一组数字相加，无论你按什么顺序加，结果都一样。因此，使用像**求和**（sum）、**均值**（mean）或**最大值**（max）这样的聚合器，可以确保 GNN 不关心一个节点邻居的任意排序。

此外，创建和处理这些消息的函数在所有节点之间是共享的。就像 CNN 的单一滤波器在图像上滑动一样，GNN 的[更新函数](@article_id:339085)对每个节点都是相同的。这同样是一种[权重共享](@article_id:638181)的形式。用群论的语言来说，这意味着 GNN 的操作在图的[对称群](@article_id:306504)的“轨道”（orbits）上是恒定的——对称等价的节点和边被同等对待 [@problem_id:3133466]。这就是将 CNN 中的卷积与 GNN 中的[消息传递](@article_id:340415)联系起来的美妙、统一的原则。两者都只是一个更深层次思想的具体实例：对[群作用](@article_id:332514)的卷积。

### 不仅仅是连接：真实世界的几何学

到目前为止，我们讨论了连通性的对称性。但许多最深刻的科学问题都与几何学有关——即物体的实际三维形状。想象一个水分子。它的性质很大程度上取决于两个氢原子之间的夹角以及其[化学键](@article_id:305517)的长度。如果我们在空间中旋转整个分子，它的能量不会改变；能量是一个标量，对旋转是**不变的**（invariant）。然而，其他性质，比如作用在每个原子上的力或分子的偶极矩，都是向量。它们有方向，如果我们旋转分子，这些向量必须随之旋转。它们是**等变的**（equivariant）。

这提出了一个新的挑战。如果我们想预测一个本身就是几何对象的属性，比如原子上的力，或者描述分子电子云在电场中如何变形的[分子极化率](@article_id:303800)[张量](@article_id:321604)，该怎么办？[@problem_id:2395448]。我们能用一个只知道哪些原子相连以及它们相距多远的标准 GNN 吗？

答案是响亮的“不”。原子间距离是标量；它们在旋转下是不变的。如果你构建一个模型的唯一几何输入是距离，你实际上是在告诉它：“这个物体在空间中的朝向无关紧要。”一个只接收抗旋转输入的模型只能产生抗旋转的输出。它可以预测分子的能量（一个不变的标量），但它从根本上无法预测一个向量或[张量](@article_id:321604)，因为这些量的定义本身就与它们在旋转下的变换方式紧密相连 [@problem_id:2395448]。这就像试图只用风速来描述风向一样。

### 几何[等变性](@article_id:640964)的构建方法

要构建一个理解[三维几何](@article_id:355311)的 GNN，我们必须为其提供正确的构建模块——即懂得方向的特征。这就引出了 **E(3) 等变 GNN**，它们被设计用来尊重三维欧几里得空间的对称性：旋转、反射和平移（即**欧几里得群 E(3)**）。

构建这些模型的方法出人意料地优雅 [@problem_id:3131946]。它首先根据特征在旋转下的行为，小心地将信息分离成不同的“类型”。

*   **0 型特征（标量）：** 这些是旋转不变的量。例子包括[原子序数](@article_id:299848)、质量，以及我们已经看到的，两个原子 $i$ 和 $j$ 之间的距离平方 $\| \mathbf{r}_i - \mathbf{r}_j \|^2$。
*   **1 型特征（向量）：** 这些是随系统一起旋转的量。最基本的例子是相对位置向量 $\mathbf{r}_j - \mathbf{r}_i$。

一个 E(3) 等变 GNN 层然后根据严格的规则组合这些特征，以保持它们的几何特性：

1.  **新的标量**通过组合旧的标量（使用任何标准函数）和取向量的[点积](@article_id:309438)来形成。例如，键角的余弦值只是两个相对位置向量的[点积](@article_id:309438)，使其成为一个完全有效的标量特征 [@problem_id:2395405]。
2.  **新的向量**通过取旧向量的[线性组合](@article_id:315155)来形成，其中组合的系数必须是标量。一条消息可能看起来像 $\phi(d_{ij}) (\mathbf{r}_j - \mathbf{r}_i)$，其中 $\phi$ 是不变距离 $d_{ij}$ 的一个可学习函数。

这个方法有一个至关重要的结果。请注意，像 `ReLU` 或 `tanh` 这样的非线性函数只应用于标量。将这样的函数直接应用于向量的分量（例如，`[ReLU(v_x), ReLU(v_y), ReLU(v_z)]`）会破坏其几何完整性，从而破坏[等变性](@article_id:640964)。旋转将不再与该操作交换 [@problem_id:2760146]。方向性将被打乱。[等变网络](@article_id:304312)通过将非线性操作限制在没有方向可打乱的标量世界中，避免了这一陷阱。

### 对称性的交响乐

这个框架不仅限于标量和向量。它可以扩展到处理任何复杂度的几何对象——**[张量](@article_id:321604)**——这在物理和化学中至关重要。用群论的正式语言来说，这些不同的特征类型对应于旋转群的**[不可约表示](@article_id:298633)（irreps）** [@problem_id:2760146]。标量是“零阶”不可约表示（$\ell=0$），向量是“一阶”不可约表示（$\ell=1$），以此类推。

E(3) GNN 的各层充当“[缠结算子](@article_id:303155)”（intertwiners），这些操作保证尊重这些几何类型。它们通过诸如**[张量积](@article_id:301137)**（tensor product）之类的操作来组合特征，[张量积](@article_id:301137)是[点积](@article_id:309438)和[叉积](@article_id:317155)的推广。这种数学机制确保了网络每个阶段的每个特征在[三维旋转](@article_id:308952)下的变换都完全符合其应有的方式。

其结果是一个具有深刻优雅性和强大功能的模型。试想一下：你可以构建一个 E(3) GNN，其最终输出是一个单一的、**不变的**标量——分子的势能。因为整个架构都是为了尊重几何微分而构建的，如果你随后对这个预测的能量关于输入原子位置取解析梯度，得到的力是自动且完美**等变的** [@problem_d:2760146]。模型不仅仅是预测一个数字；它学习了物理[力场](@article_id:307740)的一个片段，这个[力场](@article_id:307740)遵循[能量守恒](@article_id:300957)定律。这就是等变[深度学习](@article_id:302462)的终极承诺：不仅仅是创造黑箱预测器，而是发现能够流利运用宇宙基本语言——对称性语言的模型。

