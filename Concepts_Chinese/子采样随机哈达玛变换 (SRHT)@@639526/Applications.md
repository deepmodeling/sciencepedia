## 应用与跨学科联系

在了解了子采样随机哈达玛变换的原理与机制之后，我们可能感觉自己像一个刚学会国际象棋规则的学生。我们知道棋子的走法、棋盘的结构以及游戏的目标。但我们尚未见证特级大师对局中令人叹为观止的美妙之处，在那里，这些简单的规则绽放出深刻的策略和意想不到的创造力。在本章中，我们将观看 SRHT 的实战表现。我们将看到这个优雅的数学工具不仅是一种奇珍，更是一把强大的钥匙，为从超级计算机的硬盘到机器学习的前沿，甚至到算法本身的能耗等各种各样的现实问题开启了解决方案。

### 驯服数据洪流

在现代世界，我们正被数据淹没。科学模拟、卫星图像、金融交易和互联网流量生成了难以想象大小的矩阵。通常，这些矩阵是如此巨大，以至于即使是最强大的计算机的快速内存（[RAM](@entry_id:173159)）也无法容纳它们。它们驻留在较慢的、“核外（out-of-core）”存储设备上，如[固态硬盘](@entry_id:755039)或跨越机器网络。

想象你是一位大厨，但你的食品储藏室是一个位于数英里之外的巨型仓库。每次你需要一种食材，都必须进行一次漫长而昂贵的行程。你烹饪中最昂贵的部分不是切菜和混合，而是路途时间。这就是现代高性能计算的现实。移动数据——即通信——通常是比对其进行计算更为严重的瓶颈。一个用于解决大规模问题（如[最小二乘回归](@entry_id:262382)）的经典算法，可能就像一个健忘的厨师，为每一种食材都单独跑一趟。它必须一次又一次地遍历数据矩阵，导致了随着问题规模增长而急剧恶化的通信成本 [@problem_id:3537901]。

这正是 SRHT 施展其第一个伟大功绩的地方。它提供了一种只需一两次去仓库的行程就能创建一份极其精确的“购物清单”的方法。通过应用 SRHT，我们可以将巨大的 $m \times n$ 矩阵 $A$ “素描”成一个微小且易于管理的 $\tilde{m} \times n$ 矩阵 $SA$，该矩阵可以舒适地放入我们的快速内存中。其神奇之处在于，这个素描保留了原始矩阵的基本几何结构。在这个小素描上解决问题得到的答案，以极高的概率，几乎与处理完整、笨重矩阵得到的答案一样好。

但为什么是 SRHT？为什么不只是随机挑选一些行？或者使用其他类型的[随机投影](@entry_id:274693)，比如填充了高斯随机数的投影？答案在于厨师问题的第二部分：速度。高斯素描就像一个杂乱无章的仓库；为了制作我们的购物清单，我们可能不得不在每一条过道里徘徊，随机挑选物品。这是一个稠密、无结构的过程。而 SRHT，凭借其与[快速沃尔什-哈达玛变换](@entry_id:194514)的联系，则有所不同。它是一种*结构化*变换。它就像一个索引完美、组织有序的仓库。应用它的计算成本很低，几乎与数据大小成线性关系。这意味着，当数据从我们的慢速存储中流出时，我们执行素描操作的速度几乎和读取数据的速度一样快。

在一场直接的竞赛中，使用 SRHT 素描的算法可以显著超越使用高斯素描的算法，这正是因为“思考”（计算）的时间与“奔波”（I/O）的时间相比可以忽略不计。SRHT 的胜利在于它既是通信高效的，最大限度地减少了数据遍历的次数，又是计算高效的，这得益于其快速变换结构 [@problem_id:3416535] [@problem_id:3416548]。

### 以少见多之术：[压缩感知](@entry_id:197903)

SRHT 的用途远不止加速对现有数据的计算。它从根本上改变了我们获取数据的方式。这就是**压缩感知**的领域，一个挑战了信号处理百年教条的革命性思想。旧的思维方式是，要完美地捕捉一个信号，你必须以至少其最高频率两倍的速率进行采样。但如果信号具有简单的结构呢？

想象一下试图描述一个晴朗的夜空。你不需要描述每一小块黑暗的颜色。你只需列出可见的少数几颗星星的位置和亮度。这种描述是稀疏的，因此是可压缩的。[压缩感知](@entry_id:197903)告诉我们，如果一个信号在某个域中是稀疏的（意味着它可以用少数几个非零系数表示），我们可以用少量看似随机、无结构的测量来度量它，并且仍然能完美地重建它。

SRHT 为这一过程提供了一个近乎理想的“相机”。压缩感知中的测量矩阵必须满足一个称为受限等距性质 (RIP) 的微妙属性，该属性[实质](@entry_id:149406)上保证了它能保持所有稀疏向量的长度。虽然稠密高斯矩阵是教科书中的例子，但 SRHT 也是一个满足 RIP 的结构化矩阵，其测量次数 $m$ 随着稀疏度 $s$ 优雅地扩展 [@problem_id:3464445]。其快速变换的特性使其成为构建现实世界[压缩感知](@entry_id:197903)设备的实用且高效的选择。

然而，当我们将这一想法推向其绝对极限时，真正的魔力就显现出来了：**一位[压缩感知](@entry_id:197903)**。如果我们的测量设备极其粗糙呢？粗糙到每次测量只报告一个比特的信息：“是”或“否”，$+1$ 或 $-1$。想象一下，你试图通过只问一系列形式为“雕像是否在这个随机选择的平面的这一侧？”的问题来重建一个三维雕像。这似乎是一项不可能完成的任务。然而，理论表明，如果我们使用 SRHT 矩阵来定义这些随机的“平面”，我们可以通过求解一个优美的凸[优化问题](@entry_id:266749)，以惊人的准确度恢复原始[稀疏信号](@entry_id:755125)的*方向*。这个想法的大胆之处——从少数二[进制](@entry_id:634389)答案中恢复一个高维向量——证明了将结构化随机性与稀疏性原则相结合的力量 [@problem_id:3482557]。

### 磨利我们的工具：加速数值算法

到目前为止，我们已经将 SRHT 视为一种用于压缩和数据缩减的工具。但它的作用可以远不止于此，它可以作为一种“元工具”，使其他[数值算法](@entry_id:752770)更快、更鲁棒、更有效。

科学和工程中的许多问题都归结为求解大规模线性方程组。通常，这些系统是**病态的**。理解病态的一个直观方法是想象试图找到两条几乎平行的线的交点。其中一条线的方向发生微小的摆动，就可能导致交点飞到完全不同的位置。解对输入的微小扰动极其敏感。试图逐步逼近正确解的[迭代算法](@entry_id:160288)，在处理此类问题时可能会慢如蜗牛，甚至完全失败。

SRHT 提供了一种通过称为**[预处理](@entry_id:141204)**的技术来“治愈”这种疾病的方法。我们可以使用一个素描 $SA$ 来构建一个称为[预处理器](@entry_id:753679)的数学“透镜”。这个预处理器不会改变问题的答案，但它将问题本身转化为一个行为更好、条件良好的版本。将这个透镜应用于我们近乎平行的线，就像旋转我们的视角，直到它们看起来以一个健康、稳健的角度相交。SRHT 素描充当一个快速诊断工具，识别导致病态的“坏”方向，从而使我们能够构建矫正透镜 [@problem_id:3216425]。

此外，许多强大的迭代求解器，如[广义最小残差](@entry_id:637119)方法 (GMRES)，在一个不断扩展的“Krylov 子空间”内逐步构建其解。这个过程的速度取决于该[子空间](@entry_id:150286)能多快地捕捉到问题最重要的“方向”，这些方向与[系统矩阵](@entry_id:172230)的主导[奇异向量](@entry_id:143538)有关。SRHT 素描可以给求解器一个巨大的领先优势。通过对矩阵进行快速素描，我们可以得到这些主导方向的一个非常好的近似，并将它们作为初始猜测提供给求解器。求解器不是从头开始，而是以一个高质量的初始[子空间](@entry_id:150286)“启动”，使其能够在更少的迭代次数内收敛到解 [@problem_id:3416436]。

### 通往[现代机器学习](@entry_id:637169)的桥梁

也许 SRHT 最具前瞻性的应用位于数值计算与现代机器学习的交叉点。驱动[深度神经网络训练](@entry_id:633962)的引擎是一种称为反向模式[自动微分 (AD)](@entry_id:746586) 的算法，其更著名的名称是反向传播。这是一种计算单个输出（如“损失”函数）相对于数百万或数十亿输入参数的梯度的极其高效的方法。

然而，在许多复杂的科学模型中，我们需要的不仅仅是梯度。我们需要完整的雅可比矩阵——所有输出相对于所有输入的全部偏导数矩阵。对于一个拥有数百万输出和数百万参数的模型来说，这个雅可比矩阵是一个庞大到不可能存储在内存中的对象。

在这里，素描与[自动微分](@entry_id:144512)之间出现了一种美妙的协同作用。我们可以通过将原始函数 $f(x)$ 与素描矩阵 $S$ 复合，来定义一个新的、“素描后”的前向模型。这个新模型的雅可比矩阵恰好是素描后的雅可比矩阵 $SJ(x)$。我们现在可以使用反向模式 AD 来计算这个小得多的矩阵。但真正的优雅之处在于“免矩阵”方法。在一个问题的[迭代求解器](@entry_id:136910)中，我们可能只需要计算形如 $(SJ(x))^T v$ 或 $SJ(x)u$ 的乘积。

像 $J(x)u$ 这样的乘积可以通过单次*正向模式* AD 计算。像 $J(x)^T v$ 这样的乘积可以通过单次*反向模式* AD 计算。通过巧妙地结合这些工具，我们可以在不显式形成完整[雅可比矩阵](@entry_id:264467)*或*素描后雅可比矩阵的情况下，执行所有涉及素描后雅可比矩阵的必要操作。这是一场精彩的算法之舞：一次[前向传播](@entry_id:193086)计算一个[雅可比](@entry_id:264467)-向量乘积，结果与 $S$ 相乘，其转置与 $S^T$ 相乘，然后将结果输入到一次[反向传播](@entry_id:199535)中以计算最终的向量-雅可比乘积。这种素描与 AD 的“免矩阵”集成的思想，是现代[大规模优化](@entry_id:168142)和[逆问题](@entry_id:143129)的基石 [@problem_id:3416440]。

### 看不见的成本：算法与可持续性

最后，我们来到了一个既令人惊讶又至关重要的联系。算法的选择不仅仅是一个抽象的数学决策；它具有切实的物理影响。每一次[浮点运算](@entry_id:749454)，每一个生成的随机数，以及从磁盘移动的每一个字节，都消耗少量但非零的能量。当处理我们一直在讨论的规模的计算时，这些能量会累积起来。

通过为我们的算法的能耗建模，我们可以为一个计算附加一个[碳足迹](@entry_id:160723)。当我们为一个庞大的、核外问题比较稠密高斯素描和 SRHT 素描时，一幅鲜明的画面出现了。高斯素描，虽然理论上简洁优美，却需要一遍又一遍地从慢速存储中重读庞大的数据矩阵。而 SRHT，凭借其单次遍历的特性和快速的计算核心，避免了这种昂贵的数据移动。结果不仅是一个更快的算法，也是一个更“绿色”的算法。SRHT 的优雅和效率直接转化为更低的能耗和更小的环境影响 [@problem_id:3416506]。这是一个强有力的提醒：对算法之美的追求与对效率——时间、内存和能量效率——的追求，往往是同一件事。

从驯服大数据洪流到用少数测量看见无形之物，从磨利我们最古老的数值工具到驱动[现代机器学习](@entry_id:637169)的引擎，子采样随机哈达玛变换展现了其作为一条普适线索的本质。它深刻地证明了，当富有洞察力地运用结构化随机性时，它为我们理解和操纵我们周围复杂、高维的世界提供了一个优雅且用途惊人广泛的工具。