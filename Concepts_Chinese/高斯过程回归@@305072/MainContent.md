## 引言
在机器学习领域，许多模型擅长进行预测，但很少有模型能如实地报告它们对这些预测的[置信度](@article_id:361655)有多高。在科学和工程等决策风险高、理解“已知的未知”与预测本身同等重要的领域，这一差距尤为关键。[高斯过程回归](@article_id:339718)（Gaussian Process Regression, GPR）为应对这一挑战提供了一种强大而优雅的解决方案。它是一种非参数的贝叶斯方法，超越了单点预测，转而对与数据一致的所有可能函数的完整分布进行建模。本文旨在为理解这一卓越方法提供指导。在第一章“原理与机制”中，我们将剖析GPR的核心理论，探索它如何使用[核函数](@article_id:305748)定义对函数的信念，并利用数据更新这些信念。随后，在“应用与跨学科联系”一章中，我们将见证GPR的实际应用，了解其在[材料科学](@article_id:312640)、自动化科学发现等领域的变革性影响。

## 原理与机制

现在我们对[高斯过程回归](@article_id:339718)的功能有了宏观的了解，让我们揭开其层层面纱，欣赏其内部精美的机制。我们究竟如何处理“所有函数上的分布”？模型实际上是如何从数据中学习并量化自身不确定性的？答案在于几个优雅的原则，它们将概率论和线性代数融合成一个异常强大的工具。

### 函数上的分布

第一个重要思想，也是一个真正令人脑洞大开的想法，是[高斯过程](@article_id:323592)（GP）不是关于一个数或一个向量的分布，而是关于一个完整**函数**的分布。在看到任何数据点之前，GP代表了我们对于试图建模的函数可能是什么样子的信念。想象一下你可以在一张纸上画出的所有可能的平滑曲线的无限集合。GP为我们提供了一种为每条曲线分配概率的方法，根据一些简单的规则告诉我们哪些是合理的，哪些不是。

这种“先验”信念仅由两个部分定义：

1.  **[均值函数](@article_id:328567)**，$m(x)$：这是我们在看到任何数据*之前*，对函数在任意点 $x$ 处值的最佳猜测。在缺乏特定知识的情况下，我们通常将[均值函数](@article_id:328567)在任何地方都设为零，以此来表达我们最初的无知。这是最谦逊的起点。

2.  **[协方差函数](@article_id:328738)**，或称**[核函数](@article_id:305748)**，$k(x, x')$：这是GP真正的核心。它是我们函数的规则手册和DNA。[核函数](@article_id:305748)不关心函数本身的值，而是关心其在两个不同点 $x$ 和 $x'$ 处值之间的关系。它回答了这样一个问题：“如果我知道函数在 $x$ 处的值，这能告诉我多少关于它在 $x'$ 处的值？”如果 $x$ 和 $x'$ 很近，我们通常[期望](@article_id:311378)它们的函数值[强相关](@article_id:303632)。如果它们相距很远，它们可能就是独立的。

一个非常常见的规则手册是平方指数核，您可能也听过它被称为[RBF核](@article_id:346169) [@problem_id:2425194]：
$$
k(x, x') = \sigma_f^2 \exp\left(-\frac{|x - x'|^2}{2\ell^2}\right)
$$
这个公式看起来有点技术性，但其含义非常直观。$|x - x'|^2$ 这一项只是两点之间的平方距离。参数 $\ell$ 是**长度尺度**。它定义了“近”的含义。一个小的 $\ell$ 意味着相关性迅速下降，允许函数非常“曲折”。一个大的 $\ell$ 意味着相关性在长距离上持续存在，从而产生非常平滑的函数。另一个参数 $\sigma_f^2$ 是**信号方差**，它[控制函数](@article_id:362452)的整体振幅，即函数值预计会偏离均值的程度。

均值和[核函数](@article_id:305748)共同定义了我们的先验：一个GP是[随机变量](@article_id:324024)的集合，每个点 $x$ 对应一个，使得其中任何有限个变量都服从[联合高斯](@article_id:640747)（或正态）分布。

### 从数据中学习：[贝叶斯更新](@article_id:323533)

那么，我们有了这个优美的[先验信念](@article_id:328272)，这片无限的可能函数之云。但它有什么用呢？我们如何将其与真实世界和我们实际拥有的数据联系起来？这就是[贝叶斯推断](@article_id:307374)的魔力所在。我们根据证据更新我们的信念。

标准的GPR模型假设我们的观测值 $y_i$ 并非对真实潜在函数（我们称之为**潜函数** $f(x)$）的完美测量。相反，它们被噪声所污染 [@problem_id:2837958]。我们通常将其建模为：
$$
y_i = f(x_i) + \epsilon_i
$$
其中 $\epsilon_i$ 是一个[随机噪声](@article_id:382845)项，通常假设服从均值为零、噪声方差为 $\sigma_n^2$ 的高斯分布。这个方程构成了我们的**似然模型**——它告诉我们，在给定潜函数 $f(x_i)$ 的特定值的情况下，观测到数据 $y_i$ 的概率。

因此，完整的[生成模型](@article_id:356498)是一个两步过程：首先，自然从我们的GP先验中抽取一个潜函数 $f$；其次，自然根据该函数，从我们的[似然](@article_id:323123)模型中生成带噪声的观测值 $y$ [@problem_id:2837958]。
$$
\text{Prior: } f \sim \mathcal{GP}(m(x), k(x, x'))
$$
$$
\text{Likelihood: } y_i \mid f(x_i) \sim \mathcal{N}(f(x_i), \sigma_n^2)
$$
利用贝叶斯定理，我们将关于函数的先验信念与来自数据的证据相结合，形成一个**后验分布**。GPs的美妙之处在于，这个后验同样是一个GP！我们从一片充满可能性的云开始，观测数据只是“收窄”了这片云，约束它通过我们观测到的数据点附近。那些与我们数据不符的函数被赋予接近零的概率，而那些相符的函数则被认为可能性大得多。

### 进行预测：均值与不确定性

现在我们已经“训练”了我们的模型——也就是说，我们已经找到了后验分布——我们可以让它对一个新的、未见过的点 $x_*$ 进行预测。因为后验是一个GP，所以预测不仅仅是一个单一的数字；它是关于 $f(x_*)$ 值的完整[概率分布](@article_id:306824)，这个分布同样是一个高斯分布。该分布由其均值和方差来表征。

**[后验均值](@article_id:352899)**是我们对函数值的最佳单点猜测。它最终表现为我们已见的训练观测值的[加权平均](@article_id:304268)。正如 [@problem_id:1031932] 中所暗示的，预测看起来像这样：
$$
\bar{f}(x_*) = \sum_{i=1}^N \alpha_i k(x_i, x_*)
$$
在这里，权重 $\alpha_i$ 依赖于所有的训练数据，而预测是基于核函数对新点 $x_*$ 与所有旧点 $x_i$ 之间相似性的度量构建的。本质上，模型在说：“要预测这个新位置的值，我将查看我在相似位置看到的值，并智能地将它们组合起来。”

**后验方差**也许是GPR最受赞誉的特性。它为模型自身的不确定性提供了一种原则性的度量。这种不确定性的行为非常直观 [@problem_id:2425194]：
-   在训练数据密集的区域，方差很小。模型很有信心，因为它在许多已知点之间进行[插值](@article_id:339740)。
-   当我们远离数据时，方差会增大。模型对其预测变得不那么确定。
-   在远离任何训练数据的[外推](@article_id:354951)区域，后验方差接近先验方差 $\sigma_f^2$。模型实质上在报告：“我在这里没有任何相关信息，所以我像看到任何数据之前一样不确定。”这是一种深刻而诚实的对无知的承认。与此形成对比的是像[多项式回归](@article_id:355094)这样的方法，它在远离其训练数据的区域可能做出极其自信——且极其错误——的预测。

计算这些预测似乎需要对大[矩阵求逆](@article_id:640301)，这是一项以缓慢和数值不稳定而臭名昭著的任务。然而，实践者们使用了线性代数中的巧妙技巧。由于协方差矩阵是**对称正定**的，我们可以使用一种称为**[Cholesky分解](@article_id:307481)**的方法来高效且稳健地求解必要的方程，而无需直接计算逆矩阵 [@problem_id:2376451]。

### 不确定性的两面：[认知不确定性](@article_id:310285)与[偶然不确定性](@article_id:314423)

“不确定性”这个词可以有不同的含义，而GPR帮助我们精确区分。我们刚刚讨论的后验方差主要代表一种特定类型的不确定性 [@problem_id:2784631]。

**[认知不确定性](@article_id:310285)**（Epistemic uncertainty）是由于*知识的缺乏*而产生的不确定性。它反映了我们对真实潜在函数的无知。在我们没有数据的区域，它很高；在数据密集的区域，它很低。至关重要的是，这种不确定性是*可减少的*：如果我们对某个区域不确定，我们可以去那里收集更多的数据来减少我们的无知，从而减少模型的[认知不确定性](@article_id:310285)。潜函数的后验方差 $\mathrm{Var}[f(x_*) \mid \text{data}]$ 正是对此的直接度量。

**[偶然不确定性](@article_id:314423)**（Aleatoric uncertainty）是由于数据生成过程中*内在的随机性*或噪声而产生的不确定性。它就是我们[似然](@article_id:323123)模型中的 $\sigma_n^2$。即使我们完美地知道了真实函数 $f(x)$，我们的观测值仍然会围绕它[散布](@article_id:327616)。除非我们能找到更精确的测量世界的方法，否则这种不确定性是*不可减少的*。一个标准的GP模型通过将噪声方差 $\sigma_n^2$ 加到[认知不确定性](@article_id:310285)上来解释这一点，当预测一个新的带噪声的观测值 $y_*$ 时。更高级的模型甚至可以学习随输入 $x$ 变化的[偶然不确定性](@article_id:314423)，以捕捉某些配置本质上比其他配置更嘈杂的场景 [@problem_id:2784631]。

### 教机器学习：揭示核函数的秘密

我们的拼图还有最后一块。我们如何选择核参数，或称**超参数**，比如长度尺度 $\ell$ 和信号方差 $\sigma_f^2$？难道我们只是猜测吗？

当然不是！我们让数据告诉我们。这是通过一个称为**[超参数优化](@article_id:347726)**的过程来完成的。指导原则是找到使观测到的训练数据最可能出现的那组超参数。用统计学术语来说，我们调整 $\ell$ 和 $\sigma_f^2$ 来最大化**对数边缘[似然](@article_id:323123)**。这是数据的概率，是在我们的GP先验可能生成的所有函数上进行平均后得到的。

通过最大化这个量，模型学习了数据的特征属性。如果观测到的数据点表明一个非常平滑的潜在函数，优化过程将偏爱一个大的长度尺度 $\ell$。如果数据非常复杂和曲折，它将收敛到一个较小的 $\ell$ [@problem_id:320799]。这是一个美妙的机制，数据本身校准了模型的“规则”，确保我们的先验假设与我们观测到的现实相一致。