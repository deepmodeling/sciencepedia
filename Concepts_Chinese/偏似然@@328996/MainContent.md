## 引言
我们如何衡量影响事件发生时间的因素——无论是病人的康复、机器的故障，还是股票的交易？这是[生存分析](@article_id:314403)的核心问题，该领域因不完整数据（[删失](@article_id:343854)）和随时间变化的未知潜在风险剖面而变得复杂。其挑战在于如何从这种神秘的基准风险中，厘清特定变量（如新药或[遗传标记](@article_id:381124)）的影响。当我们的模型包含一个我们甚至不知道的函数时，我们如何估计其效应？

本文将探讨解决这一问题的巧妙方法：[偏似然](@article_id:344587)法。这一由 Sir David Cox 提出的半参数方法，从根本上改变了我们分析事件时间数据的方式。我们将深入探讨 Cox 模型的核心逻辑，从其基本原理到现实世界中的影响。第一章“原理与机制”将剖析[偏似然](@article_id:344587)如何通过关注每个事件发生瞬间的相对风险，巧妙地回避了未知的基准风险。随后的“应用与跨学科联系”一章将展示该方法非凡的通用性，阐述其在医学、[基因组学](@article_id:298572)、生态学和金融学等领域的应用。我们首先从揭示使这一强大工具成为可能的精妙推理开始。

## 原理与机制

想象你是一名侦探，正在调查一系列不幸事件，比如某种特定型号灯泡的故障。你有一批来自不同生产工艺（你的协变量）的灯泡，并逐一观察它们烧坏。有些灯泡可能在失效前就停止使用了——也许你搬家时把台灯带走了。这就是统计学家所说的**[删失](@article_id:343854)**（censoring）。你的目标是弄清楚某个特定的生产工艺是否会使灯泡更容易或更不容易失效，但你面临两个主要问题。首先，你不知道这些灯泡随时间变化的内在、潜在的失效率。随着灯泡老化，[失效率](@article_id:330092)会增加吗？还是保持不变？你一无所知。其次，删失意味着你的数据不完整；你不知道每个灯泡的真实寿命。你究竟如何才能推断出任何有意义的结论呢？

这正是[生存分析](@article_id:314403)所要解决的挑战，而 Sir David Cox 在 1972 年提出的解决方案是如此优雅，以至于从根本上改变了该领域。让我们来梳理一下他的思路。

### 一个大胆的假设：[比例风险](@article_id:346084)

第一步是区分我们知道的与我们不知道的。在任何给定时间 $t$ 未知的、潜在的[失效率](@article_id:330092)被称为**基准风险**（baseline hazard），我们可以将其写作 $h_0(t)$。你可以把它想象成一个神秘、崎岖的风险地貌，所有灯泡都必须穿越它。我们对这个地貌的形状不做任何假设；它可能上升、下降或任意波动。这是我们模型的**非参数**部分——我们没有强行将其套入一个整洁的数学框架中。

现在，让我们思考一下我们*确实*知道的因素，比如生产工艺，我们可以用一个变量（一个协变量）$X$ 来表示。Cox 的绝妙想法是假设这些因素不会改变风险地貌的基本*形状*，而是将其向上或向下缩放。一个更好的生产工艺可能会降低整个地貌，而一个较差的工艺可能会处处抬高它。这种缩放效应被假设为随时间恒定——如果一个灯泡今天失效的可能性是标准灯泡的两倍，那么一周后它失效的可能性也是标准灯泡的两倍。这就是**[比例风险假设](@article_id:343009)**（proportional hazards assumption）。

在数学上，我们将具有协变量 $X$ 的灯泡的总风险写为：
$$
h(t | X) = h_0(t) \exp(\beta X)
$$
在这里，$\exp(\beta X)$ 是我们的风险乘数。参数 $\beta$ 是我们想要寻找的目标。它告诉我们协变量效应的强度和方向。如果 $\beta$ 为正，则较高的 $X$ 意味着较高的风险。如果 $\beta$ 为负，则较高的 $X$ 意味着较低的风险。模型的这一部分，$\exp(\beta X)$，具有由参数 $\beta$ 控制的特定数学形式，使其成为**参数**部分。因为该模型是未指定函数 $h_0(t)$ 和指定函数 $\exp(\beta X)$ 的结合，所以它被称为**[半参数模型](@article_id:378771)**（semi-parametric model）[@problem_id:1911752]。

但这仍然给我们留下了未知 $h_0(t)$ 的问题。如果我们的方程包含一个我们不知道的函数，我们怎么可能估计 $\beta$ 呢？

### 关键洞见：每一事件都是一场竞赛

这正是其天才之处。Cox 意识到，你根本不需要知道风险地貌在任何时刻的高度。你只需要关注在事件发生的那一刻发生了什么。

让我们回到灯泡的例子。想象一下，第一个灯泡在 $t=500$ 小时时闪烁并熄灭了。在那一刻，让我们将时间定格。我们看看就在那一刻之前所有仍在发光的灯泡。这组幸存者就是我们的**风险集**（risk set）。它包括刚刚失效的灯泡，以及所有其他仍在观察中的灯泡，即使是那些后来会从研究中移除（删失）的灯泡 [@problem_id:1911718] [@problem_id:1911749]。如果一个灯泡在第 400 小时时已经坏掉或被移除，那么它在第 500 小时时就不在风险集中。类似地，在有延迟进入的研究中，一个受试者只有在进入研究后才被加入风险集 [@problem_id:1911738]。

现在，问自己一个简单的问题：鉴于风险集中的*一个*灯泡注定在这一刻失效，它恰好是那个特定灯泡的概率是多少？这就像一场比赛，在终点线上有一群竞争者。任何一个竞争者获胜的概率是其“速度”（其风险）除以所有竞争者“速度”的总和。

在时间 $t=500$ 时，风险集中任何灯泡 $k$ 的风险是 $h_k = h_0(500) \exp(\beta X_k)$。所以，如果灯泡 $j$ 是那个失效的灯泡，那么它是灯泡 $j$ 的概率是：
$$
P(\text{灯泡 } j \text{ 失效}) = \frac{\text{灯泡 } j \text{ 的风险}}{\sum_{\text{风险集中所有灯泡 } k} \text{灯泡 } k \text{ 的风险}}
$$

### 抵消的魔力

让我们将我们的模型代入这个表达式。时间是 $t_{(1)}$，我们的第一个事件时间。失效的灯泡是 $j$，风险集是 $R(t_{(1)})$。
$$
P(\text{灯泡 } j \text{ 在 } t_{(1)} \text{ 失效}) = \frac{h_0(t_{(1)}) \exp(\beta X_j)}{\sum_{k \in R(t_{(1)})} h_0(t_{(1)}) \exp(\beta X_k)}
$$
仔细看。神秘的基准风险 $h_0(t_{(1)})$ 出现在分子中。它也出现在分母求和的*每一项*中。我们可以将它从求和中提取出来：
$$
P(\text{灯泡 } j \text{ 在 } t_{(1)} \text{ 失效}) = \frac{h_0(t_{(1)}) \exp(\beta X_j)}{h_0(t_{(1)}) \sum_{k \in R(t_{(1)})} \exp(\beta X_k)}
$$
就这样，它被抵消掉了！我们剩下的表达式只依赖于协变量 $X$ 和我们想要估计的参数 $\beta$：
$$
\frac{\exp(\beta X_j)}{\sum_{k \in R(t_{(1)})} \exp(\beta X_k)}
$$
这是一个充满深刻美感的时刻。通过仅仅关注事件发生瞬间的*相对*风险，我们构建了一个完全独立于潜在的、未知的基准风险的概率 [@problem_id:1911762]。我们成功地将协变量的影响与时间的混杂效应分离开来。该方法巧妙地只使用了事件的*顺序*，而不是它们的确切时间。

如果多个事件在完全相同的时间发生，即出现**时间结**（tied events），这个逻辑就会失效。简单的“是*哪一个*失效了？”的问题不再有明确的答案。这种排序上的模糊性是处理时间结的主要挑战，统计学家已经开发了几种巧妙的近似方法来处理这种情况 [@problem_id:1911707]。

### 整合一切：[偏似然](@article_id:344587)

我们刚才推导出的表达式是第一个失效事件为我们提供的证据。为了从整个研究中获得全部证据，我们只需为每一个失效事件重复这个过程。我们移动到第二个失效时间，确定新的、更小的风险集（因为第一个灯泡已经不在了），并为第二个失效的灯泡计算相同的条件概率。我们对第三个、第四个，直到最后一个失效事件都这样做。

我们观察到的特定失效序列的总概率是所有这些单个概率的乘积。这个最终的乘积就是著名的**Cox [偏似然](@article_id:344587)**（Cox partial likelihood）：
$$
L(\beta) = \prod_{\text{所有失效事件 } i} \frac{\exp(\beta X_i)}{\sum_{j \in R(t_i)} \exp(\beta X_j)}
$$
这里的乘积是针对所有经历事件的个体 $i$，$t_i$ 是他们的事件时间，$R(t_i)$ 是那个时间的风险集 [@problem_id:1961962] [@problem_id:1911734]。它之所以被称为“偏”似然，是因为它巧妙地丢弃了与基准风险 $h_0(t)$ 相关的信息，而仅仅依赖于失效的顺序。虽然这看起来像一个聪明的技巧，但这个公式可以被更正式地推导为所谓的**[剖面似然](@article_id:333402)**（profile likelihood），从而在统计理论中获得了坚实的基础 [@problem_id:1911739]。

### 这意味着什么？寻求平衡

那么我们有了这个函数 $L(\beta)$。我们用它做什么呢？我们找到使这个函数最大化的 $\beta$ 值。这就是最大似然原理：对我们参数的最佳估计，是那个使我们实际观察到的数据最可能出现的参数值。

这种最大化背后有一个美妙的直觉。当我们找到最大化[偏似然](@article_id:344587)的 $\beta$ 时，我们本质上是在寻找一个能达到完美平衡的值。在每个事件时间，模型将失效个体的协变量 $X_i$ 与当时风险集中所有人的协变量的[加权平均](@article_id:304268)值进行比较。我们求解以找到最优 $\beta$ 的得分方程可以写为：
$$
U(\beta) = \sum_{\text{失效事件 } i} \left[ X_i - E_{\beta}[X | R(t_i)] \right] = 0
$$
其中 $E_{\beta}[X | R(t_i)]$ 是协变量 $X$ 在风险集 $R(t_i)$ 上的[期望值](@article_id:313620)或[加权平均](@article_id:304268)值，权重由当前对 $\beta$ 的猜测值决定 [@problem_id:1911737]。

这个方程讲述了一个简单的故事。为了让数据在我们的模型下处于“平衡”状态，失效个体的协变量与其竞争者“预期”协变量之间的差异之和必须为零。如果具有高 $X$ 值的个体比模型预期的更频繁地失效，这个和将为正，[算法](@article_id:331821)将增加 $\beta$ 以给予 $X$ 更大的权重。如果他们失效的频率较低，这个和将为负，$\beta$ 将被减小。最终的估计值 $\hat{\beta}$ 是这样一个值，它使得观察到的失效事件，在考虑了每一步竞赛中所有参与者的协变量后，变得完全合理。它证明了一个复杂问题——在面临不完整数据时，如何从未知的时间进程中厘清相对风险——可以通过一系列简单、深刻直观的逻辑步骤来解决。