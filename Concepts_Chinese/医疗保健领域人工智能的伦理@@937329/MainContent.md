## 引言
随着人工智能（AI）日益融入医疗保健领域，其在诊断疾病和优化治疗方面的潜力毋庸置疑。然而，衡量人工智能成功的传统标尺——其预测的准确性——却存在危险的缺陷。在医学这个复杂且以人为中心的领域，一个技术上正确的算法仍然可能造成重大伤害、加剧不公或损害患者尊严。这就提出了一个更深层次的问题：除了准确，我们如何确保医疗人工智能真正是*善*的？

本文旨在弥合这一关键差距，将讨论从纯粹的计算机科学转向应用伦理学。它为设计、部署和治理与医学核心价值观相符的人工智能系统提供了一个全面的框架。第一章“原则与机制”解构了“善”的人工智能这一概念，介绍了伦理效用、分配正义原则以及公平的数学表述。接下来的章节“应用与跨学科联系”则探讨了这些原则如何在现实世界场景中体现，从临床部署和数据治理到这些强大技术带来的系统性和全球性影响。

## 原则与机制

当我们考虑制造一个工具时，我们的第一个问题通常是：“它能用吗？”对于锤子来说，这意味着钉钉子。对于汽车来说，这意味着从A地到B地。对于一个旨在诊断疾病的人工智能（AI），我们可能会认为答案同样简单：“它准确吗？”一个更好的人工智能能更频繁地得出正确答案，这似乎是显而易见的。但在人类健康这个复杂且高风险的世界里，这种简单的直觉不仅不完整，甚至可能具有危险的误导性。真正的探索始于我们提出一个更深层次的问题：不仅是“这个人工智能正确吗？”，而是“这个人工智能是*善*的吗？”

### 超越准确性：人工智能之“善”意为何？

想象一下，我们有两个人工智能模型在医院里竞争一个职位。它们的任务是检测败血症的早期迹象，这是一种危及生命的疾病。从纸面上看，模型 $M_2$ 是明星学生。它的曲线下面积（AUC）为 $0.90$，这是一个衡量预测能力的标准备，超过了模型 $M_1$ 虽然可观但较低的 $0.80$ 分。传统智慧会告诉我们立即聘用 $M_2$。

但如果我们看得更仔细呢？医疗保健领域的人工智能不仅仅是做出预测；它会触发影响真人的行动。一个“阳性”警报可能带来积极的、拯救生命的治疗，但如果警报是假警报，它也可能导致不必要、昂贵甚至有害的干预。它可能在没有获得适当同意的情况下发生，或者可能不成比例地标记来自某个社区的人群。我们如何权衡这些不同的结果？

在这里，我们必须从计算机科学的语言转向伦理学的语言。我们可以将一个真正“善”的人工智能视为与医学本身的核心原则相一致的系统：**行善**（do good）、**不伤害**（do no harm）、**自主**（respect people's choices）和**公正**（be fair）。我们甚至可以想象，作为一个思想实验，创建一个结合了这些原则的单一“伦理效用”评分。我们为每一次正确且拯救生命的干预赋予一个正值 $w_B$，但对每一次导致伤害的假警报（罚分 $w_M$）、每一次绕过正当同意的干预（罚分 $w_A$）以及每一次系统在不同人群间造成不公平差异的情况（罚分 $w_J$）都进行扣分 [@problem_id:4438917]。

现在，当我们用这个更丰富、更以人为中心的记分卡重新评估我们的两个模型时，一个令人惊讶的画面可能会出现。模型 $M_2$ 在追求高准确性的过程中可能会变得过于激进。是的，它能捕捉到更多真实的败血症病例，但代价是产生大量的假警报，尤其是在人口中某个脆弱的亚群中。它的运作方式也可能更容易导致在同一群体中未经充分同意就进行治疗。当我们计算总分时，我们可能会发现，其增加的错误和不公平性所带来的伤害完全超过了其更高准确性带来的益处。纸面上“更好”的模型 $M_2$，实际上其伦理效用可能是*负*的，而那个“准确性”较低的模型 $M_1$，却因其更为平衡和公平而保持了正分。

这个有力的想法揭示了一个深刻的真理：**医疗保健领域的人工智能对齐，不等同于最大化预测准确性**。它是一项更具挑战性、也更重要的任务，即优化我们的系统，以坚守我们最深刻的伦理承诺。其中最复杂的承诺之一就是公正。

### 公正的天平：公平分配关怀与负担

当资源稀缺时——无论是ICU床位、捐赠器官还是专家的诊疗时间——我们都不得不做出艰难的选择。谁能得到它？一个负责推荐分配的人工智能，必须被编程注入一种正义理论。但“正义”并非一个单一、简单的概念。它是一个由相互竞争、有时甚至相互冲突的原则构成的图景[@problem_id:4417382]。

想象一下，一家医院只剩下最后一部呼吸机。谁应该得到它？

-   一种方法是**平等**：我们可以在所有临床符合条件的患者中进行抽签。每个人都有平等的机会。这很简单，避免了对人的评判，但把呼吸机给一个轻症患者而不是一个危重病人，感觉对吗？

-   这引出了一个**基于需求**的原则：把它给最病重的人。这与我们帮助最危难者的直觉相呼应。但如果最病重的人病得太重，以至于呼吸机不太可能救活他们呢？把呼吸机给他们可能是一种徒劳的姿态，而一个病情稍轻的人本可以得救。因此，一个稳健的基于需求的原则，不仅仅是优先考虑处境最差的人；它还必须受到合理受益预期的约束。

-   然后是**公平**。这个原则承认并非每个人都从同一起点出发。有些人面临结构性劣势——贫困、歧视、缺乏预防性医疗——这使得他们从一开始就更容易生病。一个基于公平的方法可能会对这些人给予“倾斜”，旨在纠正非应得的劣势，创造公平的竞争环境。

医学伦理学断然拒绝的是基于**应得**或**功绩**的分配：即我们应根据某人被感知的社会价值、过去的贡献或“良好”行为来优先考虑他们。医院是治愈之地，不是审判法庭。唯一狭窄的例外是一种前瞻性的、工具性的论证——例如，在疫情大流行期间优先考虑医护人员，不是因为他们天生“更有价值”，而是因为他们的生存对于拯救许多其他生命至关重要。

所以，当我们设计一个人工智能来帮助做这些决定时，我们不只是在写代码，我们是在嵌入一种道德哲学。但是，这些抽象的哲学能被翻译成算法能理解的精确数学语言吗？

### 从原则到代码：公平的数学

令人惊讶的是，答案是肯定的。我们可以在分配正义的宏大原则与机器学习模型的运作现实之间架起一座桥梁。其中一种最优雅的方法是使用一个名为**[均等化赔率](@entry_id:637744)**的概念[@problem_id:4849777]。

让我们回到那个决定谁能获得高级诊断测试的人工智能。对任何人来说，都有两个关键的错误率。第一个是**真阳性率（TPR）**：如果你*确实需要*该测试，人工智能正确为你推荐它的概率是多少？这代表你获得应有*利益*的机会。第二个是**[假阳性率](@entry_id:636147)（FPR）**：如果你*不需要*该测试，人工智能错误地为你推荐它的概率是多少？这代表你承担不应有*负担*的风险——不必要程序的成本、时间和焦虑。

分配正义告诉我们要“同类情况同等对待”。[均等化赔率](@entry_id:637744)为此赋予了精确的数学含义。它要求人工智能的表现在不同社会群体（例如，基于种族或社会经济地位）之间保持平衡。一个需要测试的人，无论属于哪个群体，都应该有同样高的机会获得测试（相同的TPR）。而一个不需要测试的人，无论属于哪个群体，都应该有同样低的机会被测试所拖累（相同的FPR）。

这是一个优美而有力的转化。它通过确保算法利益和负担的分配不被伦理上无关的因素所扭曲，从而将公平操作化。它表明，我们不仅能将公平构建于我们的内心和法律中，还能构建于我们的算法本身。

### 不只是器官的集合：维护尊严与声音

但医疗保健不仅仅是公平分配测试和治疗。它是一项深刻的人类事业。一个进入医院的人，不仅仅是症状和数据点的集合；他们是一个有故事、有历史、对疾病有独特体验的人。人工智能在医疗保健中最大的风险之一是**非人化**——在我们追求效率的过程中，我们可能允许机器将丰富、复杂而有意义的**患者叙事**压缩成贫乏的关键词摘要。

为了应对这一点，我们必须设计具有**叙事能力**的人工智能[@problem_id:4415732]。这是系统识别、吸收、解释以及最重要地，*尊重*患者故事的能力。想象一下，我们设计的AI不仅仅是为了提取“事实”，而是为了保留患者自己的声音。我们可以衡量这一点。我们可以构建一个AI，并让它对“直接引语保留率”负责，确保患者自己的话被带入病历中。我们可以通过检查患者是否是自己故事的语法主语（“*我*感到一阵剧痛”）而不是被动宾语（“据报告有疼痛”），来衡量它是否保留了患者的主体性。我们甚至可以衡量AI是否保持了原始叙事的时间流和情感内容。

这是一个根本性的视角转变。这意味着我们设计AI不仅仅是为了数据提取，而是为了保护人性。

这种保护尊严的责任超出了个人范畴。一个AI可以在不提及任何单个人的情况下造成**群体伤害**[@problem_id:4439480]。如果一个公共卫生模型生成一份报告，称某个特定族裔社区“在报告方面不太可信”，它就对整个群体造成了**尊严伤害**。它强化了刻板印象，侵蚀了该群体的“集体尊严”——他们的社会地位和被视为平等个体的权利。这可能产生实际后果，影响雇主、房东甚至临床医生如何看待该社区的成员。作为研究伦理基石的 The Belmont Report，呼吁我们尊重个人并确保公正。这要求我们认识并防止这些微妙但具有[腐蚀性的](@entry_id:164959)再现性伤害，并理解去标识化并不能抵御对一个群体尊严的攻击。

### 游戏规则：公平程序与信托责任

即使一个在结果上公平、在语言上尊重的人工智能，如果它作为一个不负责任的黑匣子运行，也可能是不公正的。*结果*的公平性是分配正义的问题，但*过程*的公平性则是一个**[程序正义](@entry_id:180524)**的问题[@problem_id:4417396]。要使一个人工智能系统在程序上是公正的，它必须建立在四个支柱之上：

1.  **透明度：** 它的逻辑和基本原理必须是可访问和可理解的，而不是隐藏在专有保险库中。
2.  **参与：** 受人工智能影响的人——患者、临床医生、社区成员——必须在其治理中有发言权。
3.  **可争议性：** 对于被认为是错误或不公平的决定，必须有一个清晰、独立且有效的申诉程序。
4.  **问责制：** 当出现问题时，必须有明确的责任链。“是算法干的”不是一个可接受的答案。

这种健全的治理必须延伸到为这些系统提供动力的数据本身。我们经常谈论数据“所有权”，但一个更深刻的概念是**管家精神**（stewardship）[@problem_id:4434069]。一个纯粹的数据“保管人”就像一个保安，专注于看门和遵守法律条文。然而，一个真正的**管理者**（steward）是在类似信托责任下运作的。他们有**注意义务**（duty of care）去胜任地管理数据，并有**忠诚义务**（duty of loyalty）去为数据中所包含的患者的最大利益行事。这意味着避免利益冲突，分享从数据中产生的利益（例如新发现带来的收入），并始终将数据主体的福祉放在首位。

**情境完整性**的框架进一步强化了这种管家精神的原则[@problem_id:4441674]。隐私不仅仅是隐藏你的名字；它是关于确保信息以适合情境的方式流动。你的临床数据属于你和你的医生之间的关怀情境。当它在你不知情的情况下流向一家商业科技公司用于产品开发时，情境就被打破了，隐私侵犯就发生了，即使数据被“匿名化”了。

最后，我们必须记住，履行这些义务不等同于避免一场官司。一个人工智能可能造成真实的、伦理上显著的伤害，而我们的法律体系可能尚未承认其为可赔偿的[@problem_id:4429849]。一个有偏见的算法可能不会造成可证明的身体伤害，但它可能侵蚀信任、造成尊严伤害，并使患者感到“被忽视和不被尊重”。**关怀伦理学**教导我们，我们的道德义务不仅仅是避免法律责任，而是维持和修复位于医学核心的关怀关系。

### 不断扩大的圈子：从地方伦理到全球和谐

随着这些强大的人工智能工具跨越国界，我们面临最后一个巨大挑战：在一个拥有多元文化和价值观的世界里，我们如何协调伦理？答案不可能是伦理帝国主义，即一个国家的价值观强加于所有国家。相反，前进的道路在于一种对**价值多元主义**采取的有原则的方法[@problem_id:4443473]。

我们可以设想一个双层框架。首先，我们建立一个**最低限度的协调**：一个由不可协商的、普适原则构成的“薄底板”。这包括对核心人权的承诺，对可接受风险的严格上限，以及透明度和问责制等强有力的程序性保障。这确保了每个系统，在任何地方，都是根本上安全和可信的。

在这个基础上，我们可以建立一个**强有力的协调**：一套通过合作伙伴之间的商议和共识创建的“更厚”的协议。在这里，不同的社区可以共同决定他们希望采纳的具体公平定义，或者人工智能的利益应该如何分享。这种方法尊重地方价值观，同时维护对人类尊严的普适承诺。

在医疗保健领域构建合乎伦理的人工智能的旅程是复杂的，但并非不可逾越。它要求我们不仅仅是优秀的工程师；它要求我们成为深思熟虑的伦理学家、细心的照护者和尽职的管理者。这是一段从“它准确吗？”这个简单问题，走向“它关怀吗？”这个深刻且最终更为重要的问题的旅程。

