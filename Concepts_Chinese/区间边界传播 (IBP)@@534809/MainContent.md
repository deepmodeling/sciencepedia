## 引言
在一个人工智能系统做出从识别医疗状况到驾驶车辆等关键决策的时代，它们的可靠性问题不仅是学术问题，更是至关重要的问题。给现代人工智能蒙上阴影的一个重大挑战是其惊人的脆弱性；强大的[神经网络](@article_id:305336)可能会被对其输入的微小、难以察觉的变化完全欺骗，这种现象被称为对抗性样本。我们如何能信任如此容易被欺骗的系统？本文探讨了一种旨在回答这个问题的基础技术：[区间边界传播](@article_id:641933) (IBP)。IBP提供了一种简单而深刻的方法，为网络行为提供数学保证。通过用保证的范围换取精确的值，它使我们能够验证鲁棒性并构建更值得信赖的人工智能。在接下来的章节中，我们将首先揭示IBP的核心“原理与机制”，从其基本直觉开始，逐步构建其在深度网络中的应用，同时也将直面其固有的局限性。然后，我们将在“应用与跨学科联系”中拓宽视野，看看IBP如何成为[人工智能安全](@article_id:640281)的关键构建模块，以及一个在其他科学领域回响的基本概念。

## 原理与机制

### 最简单的想法：传播区间

让我们从一个简单、近乎幼稚的问题开始我们的旅程。如果你不知道一个量的确切值，但你知道它存在于某个范围内——一个**区间**——那么关于涉及它的计算结果，你能说些什么呢？假设你在烘焙，一个食谱要求“大约两杯面粉”和“大约一杯糖”。一个更精确的烘焙师可能会将其解释为，比如说，面粉在 $1.9$ 到 $2.1$ 杯之间，糖在 $0.9$ 到 $1.1$ 杯之间。如果你将它们混合，干性配料的总量是多少？答案不是一个单一的数字，而是一个新的、更大的区间。你可能拥有的最少量是 $1.9 + 0.9 = 2.8$ 杯，最多是 $2.1 + 1.1 = 3.2$ 杯。你的总量在区间 $[2.8, 3.2]$ 内。

这就是**[区间边界传播](@article_id:641933) (IBP)** 背后的核心直觉。它是一种追踪量在通过一系列数学运算时的不确定性的方法。让我们把这个过程 formalize 一点。考虑一个简单的线性函数 $y = w x + b$。如果我们知道输入 $x$ 在区间 $[\ell, u]$ 内，那么 $y$ 的区间是什么？如果权重 $w$ 是正的，函数是递增的，所以输出区间就是 $[w\ell + b, wu + b]$。如果 $w$ 是负的，函数是递减的，所以边界的角色互换：$[wu + b, w\ell + b]$。

从控制工程到人工智能，现代系统不使用单个数字进行计算，而是使用向量。想象一下控制系统中的[状态估计](@article_id:323196)误差 $e_k$，它是未知但有界的。我们可以用一个方程如 $e_{k+1} = A e_k + w_k$ 来描述它的演变，其中 $w_k$ 是某个未知的扰动，同样有界于一个区间内 [@problem_id:2706824]。要找到下一个误差状态 $e_{k+1}$ 的边界，我们可以推广我们的简单规则。对于输出向量的每个分量，我们都想找到其可能的最小值和最大值。这通过选择输入的“最坏情况”组合来实现。

这导出了一个优美而紧凑的矩阵公式。如果我们的输入向量 $x$ 位于由下界和上界向量 $[\ell, u]$ 定义的“盒子”中，并且它经历一个[仿射变换](@article_id:305310) $y = Wx + b$，那么 $y$ 的新边界 $[\ell', u']$ 由以下公式给出：

$$
\ell' = W^+ \ell + W^- u + b
$$
$$
u' = W^+ u + W^- \ell + b
$$

在这里，$W^+$ 是一个只包含 $W$ 的正项（负项设为零）的矩阵，而 $W^-$ 只包含负项。这个公式只是一个巧妙的方式，一次性为每个分量表达了我们的“最坏情况”逻辑。为了得到最低的可能输出（下界 $\ell'$），你将最低的输入（$\ell$）与正权重（$W^+$）结合，将最高的输入（$u$）与负权重（$W^-$）结合，对于上界 $u'$ 则反之亦然 [@problem_id:3098472]。

### 穿越网络之旅

那么，一个神经网络，如果不是这些操作的宏大序列，又是什么呢？一个典型的前馈网络将输入向量推过交替的[仿射变换](@article_id:305310)层和非线性“激活函数”层。我们刚刚看到了如何通过仿射层传播一个区间。下一步是弄清楚如何将其推过一个[激活函数](@article_id:302225)。

让我们考虑现代深度学习中最流行的激活函数：**[修正线性单元](@article_id:641014)**，或**ReLU**。它的定义惊人地简单：$\text{ReLU}(z) = \max(0, z)$。它只是将任何负值裁剪为零。这对我们的区间 $[\ell, u]$ 有何影响？由于该函数是单调非递减的（它从不下降），很容易看出新的区间将简单地是 $[\max(0, \ell), \max(0, u)]$。该函数应用于区间的端点，就是这样！

有了这两条规则——一条用于仿射层，一条用于ReLU——我们就拥有了将整个输入盒子从头到尾传播通过一个深度神经网络所需的一切。我们从一个输入区间开始，将其推过第一层得到一个新区间，应用ReLU规则得到另一个，然后重复这个过程，直到我们到达网络的最终输出 logits [@problem_id:3098826] [@problem_id:3105264]。每一步都会扩大可能性的盒子，但我们始终保持对每个可能状态的有效包围。

### 目标：鲁棒性证书

为什么要费这么大劲呢？最重要的应用之一是在**[人工智能安全](@article_id:640281)**和**可验证鲁棒性**领域。你可能听说过“对抗性样本”——那些被微不足道地调整以欺骗强大[神经网络](@article_id:305336)的图像。一张熊猫的照片，加上一点点精心制作的噪声，突然就被高[置信度](@article_id:361655)地分类为长臂猿。温和地说，这令人不安。它揭示了这些系统中我们必须理解和控制的脆弱性。

“鲁棒性证书”是一个[数学证明](@article_id:297612)，对于给定的输入（比如我们原始的熊猫图像）和一组定义的扰动（比如允许每个像素在一个微小的范围内变化），网络的预测*不能*改变。IBP提供了一种简单而强大的方法来生成这样的证书。

所有可能的扰动图像的集合形成一个高维盒子。我们可以使用IBP将这个完整的盒子输入到我们的网络中。在输出端，我们为每个分类 logit（网络对每个类别的内部得分）得到一个区间。假设原始预测是“熊猫”。如果“熊猫”logit 区间的*下界*仍然大于其他每个 logit 区间的*上界*（例如，“长臂猿”、“扶手椅”、“客机”），那么我们就有了我们的证明！无论你如何在允许的盒子内调整输入，“熊猫”将永远获得最高分。该网络对于该输入和该扰动大小被证明是鲁棒的 [@problem_id:3098472]。我们甚至可以使用[二分搜索](@article_id:330046)等技术来找到这个证书成立的最大可能扰动半径 $\epsilon$。

### 阿喀琉斯之踵：当区间说谎时

这是一种非常简单和优雅的方法。但是，就像科学中许多简单和优雅的想法一样，它有一个陷阱。IBP计算出的边界可能，而且常常是，过于悲观。计算出的区间可能比真实可能的输出范围宽得多。

为了理解原因，让我们考虑一个来自 [@problem_id:3105258] 的极其简单的玩具网络。设输入为单个标量 $x \in [-1, 1]$。第一层有两个[神经元](@article_id:324093)，其预激活值为 $z_1 = x$ 和 $z_2 = -x$。经过[ReLU激活](@article_id:345865)后，我们得到 $a_1 = \max(0, z_1)$ 和 $a_2 = \max(0, z_2)$。最终输出只是它们的和，$y = a_1 + a_2$。

让我们遵循IBP的逻辑。
1. 输入 $x \in [-1, 1]$。
2. 预激活值：$z_1 \in [-1, 1]$ 和 $z_2 \in [-1, 1]$。
3. 激活后：IBP独立地看待每个[神经元](@article_id:324093)。对于 $z_1 \in [-1, 1]$，ReLU输出 $a_1$ 在 $[\max(0, -1), \max(0, 1)] = [0, 1]$ 内。同样，$a_2 \in [0, 1]$。
4. 最终输出：为了找到 $y = a_1 + a_2$ 的最大值，IBP假设了最坏情况：$a_1$ 可能达到其最大值 $1$，而 $a_2$ *也*可能达到其最大值 $1$。所以，$y$ 的输出区间是 $[0, 2]$。

但是等等。稍加思考就会发现这个推理中的一个缺陷。$a_1$ 和 $a_2$ 能同时为 $1$ 吗？要使 $a_1$ 为 $1$，我们需要 $x=1$。但如果 $x=1$，那么 $z_2 = -1$，并且 $a_2 = \max(0, -1) = 0$。它们不可能同时为正，更不用说同时为 $1$ 了！事实上，这些激活是完全[负相关](@article_id:641786)的。真实的输出是 $y = \max(0, x) + \max(0, -x)$，这正是[绝对值函数](@article_id:321010) $|x|$。在输入域 $x \in [-1, 1]$ 上，$y$ 的真实范围是 $[0, 1]$。

IBP告诉我们输出可能高达 $2$，但现实是它永远不会超过 $1$。该方法偏差了整整一倍！这就是臭名昭著的**依赖问题**。IBP将每个[神经元](@article_id:324093)的激活视为一个可以自由探索其区间的独立变量。它忘记了它们都共享一个共同的祖先——原始输入 $x$——因此它们是深度相关的。IBP用一个简单的、轴对齐的盒子来近似真实的、通常是弯曲且低维的可达激活[流形](@article_id:313450)。通过这样做，它包含了在现实中无法到达的角落。

### 符号稳定性：精确性的领域

那么，如果IBP可能如此宽松，它是否曾精确过？是的，理解何时精确是理解其本质的关键。我们例子中的松散性是因为预激活区间 $z_1 \in [-1, 1]$ 和 $z_2 \in [-1, 1]$ 跨越了零。这迫使ReLU真正地非线性，像一个开关一样工作。一个其预激活区间跨越零的[神经元](@article_id:324093)被称为**不稳定**的。

但是，如果一个[神经元](@article_id:324093)的预激活区间是，比如说，$[0.2, 1.4]$ 呢？由于所有可能的值都是正的，ReLU $\max(0, z)$ 就只是[恒等函数](@article_id:312550) $z$。如果区间是 $[-1.4, -0.2]$ 呢？由于所有值都是负的，ReLU的输出总是 $0$。在这两种情况下，非线性的ReLU在整个输入盒上表现为一个简单的[仿射函数](@article_id:639315)（要么是[恒等函数](@article_id:312550)，要么是零）。

当网络中*所有*[神经元](@article_id:324093)对于给定的输入盒都以这种方式**符号稳定**时，一件非凡的事情发生了。整个多层非线性网络坍缩成一个单一的、等效的仿射变换。正如我们一开始看到的，IBP对于[仿射变换](@article_id:305310)是完全精确的！在这个稳定区域，依赖问题消失了，简单的区间边界就是最紧的可能边界 [@problem_id:3105258] [@problem_id:3105244]。可验证的稳定性余量，它衡量预激活区间离跨越零有多远，为我们提供了这种稳定性的定量度量 [@problem_id:3105264]。

### 超越区间：寻求更紧的边界

IBP的简单性是其最大的优点，也是其最大的弱点。当[神经元](@article_id:324093)不稳定时，它的边界变得宽松。这促使研究人员开发更强大但更复杂的验证方法。像**CROWN**或基于**[线性规划](@article_id:298637) (LP) 对偶**的方法不传播简单的区间数字，而是传播关于输入的完整线性函数 [@problem_id:3105244] [@problem_id:3105234]。它们维持一个像 $\text{activation} \ge c_L^\top x + d_L$ 和 $\text{activation} \le c_U^\top x + d_U$ 这样的表示。通过保留对原始输入 $x$ 的依赖关系，它们可以捕获IBP丢弃的相关性。

对于IBP偏差一倍的那个网络，这些更先进的方法可以证明真实的边界是 $1$。在一个具体的数值例子中，人们可能会发现IBP给出的可验证下界是，比如说，$0.365$，而一个基于LP的方法证明了一个更紧、更好的边界 $0.4233$。这 $\approx 0.058$ 的差异是为IBP的简单性付出的代价 [@problem_id:3105234]。

这个故事最终引出了一个优美而微妙的洞见。一个验证方法的紧密性并非仅仅是[算法](@article_id:331821)本身的绝对属性；它是[算法](@article_id:331821)假设与网络内部几何结构之间相互作用的结果。设计一个“鲁棒训练”的网络是可能的，其几何结构如此巧妙地对齐，以至于它挫败了IBP，使其边界比基线网络*更宽松*。然而，同样的对齐可以被像CROWN这样的依赖感知方法完美利用，它发现其边界变得*更紧密* [@problem_id:3105278]。这是验证者与被验证者之间的一场舞蹈，提醒我们在深度学习的世界里，几何为王。而IBP，尽管其强大和简单，只是学习其语言的第一步。

