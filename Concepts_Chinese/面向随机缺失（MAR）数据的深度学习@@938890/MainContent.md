## 引言
在几乎每一个科学和技术领域，我们的知识都建立在不完整的数据之上。我们选择如何对这些信息空白进行推理，其结果可能是微不足道的烦恼与重大发现之间的天壤之别。核心挑战不仅在于数据的缺失，还在于理解数据*为何*缺失。本文通过探索我们如何教机器从部分信息中学习来解决这一根本问题，重点关注深度学习的强大能力。

在部署任何算法之前，我们必须首先理解我们“无知”的结构。本文首先介绍一个关键的统计学分类法，将[缺失数据](@entry_id:271026)分为三类：[完全随机缺失](@entry_id:170286)（MCAR）、[非随机缺失](@entry_id:163489)（MNAR）以及微妙但至关重要的[随机缺失](@entry_id:168632)（MAR）状态。您将了解为什么 MAR 为校正提供了一个独特的机会，形成了一个结构化的空白，而其解决方案的线索就存在于我们已有的数据之中。这一基础为两个主要章节奠定了基础。在“原理与机制”中，我们将深入探讨 MAR 的理论以及[深度学习模型](@entry_id:635298)如何被训练成为复杂的“[插补](@entry_id:270805)引擎”，驾驭经典的[偏差-方差权衡](@entry_id:138822)。随后，“应用与跨学科联系”将展示这些原理的实际应用，展示这种方法如何用于解决[个性化医疗](@entry_id:152668)、遗传学、物理学和[药物发现](@entry_id:261243)中的重大问题。

## 原理与机制

要理解我们如何教机器对不存在的数据进行推理，我们必须先在某种程度上成为一名哲学家。我们必须问一个看似简单的问题：数据*为何*缺失？想象你是一位正在绘制夜空星图的天文学家。你在地图上发现了一个空白点。它是空白的，是因为你的望远镜在某个多云的夜晚恰好指向了错误的方向吗？还是因为在那个确切的位置有一个吞噬光线的巨大黑洞？第一种情况只是一个烦恼；第二种情况则是一项重大发现。“缺失”的性质改变了一切。

在数据科学中，我们面临着同样的难题。我们能使用的策略——以及我们是否能成功——完全取决于我们数据中空白的性质。统计学家们在一个充满美妙清晰的时刻，将这个复杂的问题组织成一个简单而强大的分类法。要掌握从不完整信息中学习的艺术，我们必须首先理解这一基本结构。

### “无知”的分类法

让我们想象一项医学研究，试图根据患者当前的一系列特征（我们称之为 $X$）来预测其未来的健康结果（我们称之为 $Y$）。这些特征可以是任何东西，从年龄、血型到复杂的基因组数据。在我们的数据集中，某些 $Y$ 或 $X$ 的值可能会缺失。我们可以用一个“缺失指示”变量 $R$ 来表示，它告诉我们哪些数据是观测到的，哪些不是。$R$ 与数据本身 $(X, Y)$ 之间的关系定义了三个截然不同的世界。

#### [完全随机缺失](@entry_id:170286)（MCAR）：良性空白

这是最简单、最宽容的情形。如果一个值缺失的概率与我们已观测到的数据和未观测到的数据都完全无关，那么数据就是[完全随机缺失](@entry_id:170286)。用概率的语言来说，我们写作 $R \perp (X, Y)$，意味着缺失机制 $R$ 与我们所有感兴趣的变量都是独立的。

可以把它想象成一个数据录入员不小心把咖啡洒在了一部分随机的纸质记录上。信息的丢失与记录属于谁或记录包含什么内容无关。这纯粹是运气不好。

这对我们意味着什么？在 MCAR 条件下，我们*确实*拥有的数据是完整数据集的一个完美代表性（尽管规模较小）的随机样本 [@problem_id:4332669]。观测集中的变量分布与完整集中的分布相同。这是极大的幸运。虽然我们损失了统计功效（因为我们的样本量变小了），但我们没有引入系统性误差，即**偏差**。在许多情况下，我们可以简单地执行**完整案例分析**，即丢弃不完整的记录，然后继续处理其余的数据。主要的挑战是数据的丢失，而不是其被破坏 [@problem_id:4776650]。

#### [非随机缺失](@entry_id:163489)（MNAR）：恶性空白

让我们跳到另一个极端，最危险的情形。当一个值缺失的概率取决于那个缺失值本身时，数据就是[非随机缺失](@entry_id:163489)。例如，在一项抗抑郁药的临床试验中，感觉最抑郁的患者（“症状改善”这个结果的值很低）可能最有可能退出试验，从而错过他们的最终评估 [@problem_id:4689941]。缺失与未观测到的信息直接相关。

这就是天文学家的黑洞。我们星图上的空白点不是意外；它是一个特定的、未观测到的现象的标志。我们拥有的数据现在在根本上具有误导性。完成了研究的患者可能看起来对药物反应良好，但这仅仅是因为没有反应的患者已经从数据集中选择性地消失了。

在这种情况下，缺失被称为**不可忽略**。我们不能简单地只看观测数据就期望能校正偏差。为了有望做出有效的推断，我们必须建立一个*联合*模型，同时描述数据（给定 $X$ 的 $Y$）和缺失机制（给定 $Y$ 和 $X$ 的 $R$）。这需要关于那个空白性质的强有力的、无法检验的假设——这在没有外部知识的情况下通常是不可能的任务。标准的处理方法，无论多么复杂，都会失败，因为校正偏差所需的信息本身就是缺失的 [@problem_id:4332669] [@problem_id:4776650]。

#### [随机缺失](@entry_id:168632)（MAR）：结构化空白

在 MCAR 的良性混乱和 MNAR 的恶性密谋之间，存在着第三种微妙而极其重要的状态：[随机缺失](@entry_id:168632)。这就是[深度学习](@entry_id:142022)展示其真正力量的世界。如果一个值缺失的概率仅依赖于我们*已观测*到的信息，而不依赖于缺失信息本身，那么数据就是 MAR。

让我们回到那个医学研究。假设某个血液检测结果 $X_j$ 对于年龄较大的患者来说更容易缺失。我们观测了每位患者的年龄。这里的 MAR 假设意味着，*在任何给定的年龄组内*，血液检测缺失的原因相对于实际的血液检测值是随机的。例如，可能是检测设备对老年患者样本的成功率较低。所以，如果我们知道一个病人是80岁，知道他们的检测结果缺失并不会比我们从其他80岁老人那里已经知道的，提供更多关于他们可能的血液检测结果的信息。

形式上，我们说缺失指示符 $R$ 在我们以观测数据 $X_{\text{obs}}$ 为条件时，与缺失数据 $X_{\text{mis}}$ 是独立的：$R \perp X_{\text{mis}} \mid X_{\text{obs}}$ [@problem_id:4332669]。

这是“侦探工作”的情景。缺失的模式不是随机噪声，而是一个结构化的信号。关键的是，解读这个信号的线索就存在于我们拥有的数据中。一个简单的完整案例分析会产生偏差——我们的样本会偏向于年轻患者。但我们并非束手无策。因为缺失可以从观测数据中*预测*出来，我们可以利用这一事实来校正偏差。这就是为什么 MAR 机制也被称为**可忽略**：不是因为我们可以忽略缺失的数据，而是因为如果我们小心处理，我们可以在不显式地对缺失机制本身建模的情况下做出有效的推断。我们只需要巧妙地利用观测数据来解释它所造成的不平衡 [@problem_id:4689941]。

### 深度学习解决方案：对空白建模

MAR 假设为解决方案打开了大门。如果缺失的模式被编码在观测数据中，我们就可以建立一个模型来学习那个模式。这就是深度学习登场的时刻。考虑**插补**任务：用合理的估计值填充缺失值。我们想学习缺失变量在给定观测变量下的[条件分布](@entry_id:138367)，$p(X_{\text{mis}} \mid X_{\text{obs}})$。

在许多现实世界的领域中，比如我们问题中提到的电子健康记录和[多组学](@entry_id:148370)数据 [@problem_id:4332669] [@problem_id:4689941]，这些条件关系异常复杂。一个缺失的基因表达水平的值可能依赖于成千上万个其他基因、临床测量值和患者人口统计学信息的微妙、非线性的组合。

简单的插补方法，比如用一列的平均值填充，会惨败，因为它们忽略了这些复杂的相关性。它们抹平了我们正需要保留的结构。另一方面，深度神经网络非常适合这项任务。作为[通用函数逼近器](@entry_id:637737)，它们有能力学习将观测数据映射到缺失数据的复杂、高维函数。它们可以充当“[插补](@entry_id:270805)引擎”，创建一个统计上完整的数据集版本，以校正由 MAR 机制引入的偏差。

### 智慧训练的艺术：偏差-方差之舞

仅仅将一个强大的深度网络用于插补任务是不够的。所有机器学习中的一个核心挑战是**[偏差-方差权衡](@entry_id:138822)**。一个高**偏差**的模型过于简单；它做出强假设，无法捕捉到真实的基础模式（就像使用均值插补）。一个高**方差**的模型过于复杂；它完美地拟合了训练数据的特定噪声和怪癖，以至于无法泛化到新数据上——这种现象被称为过拟合。

想象一下雕刻一座雕像。高偏差的方法就像使用一把大锤；你会得到一个块状的、大致的形状，但会错过所有细节。高方差的方法就像试图用一根小针雕刻每一个毛孔和头发；你更有可能是在描摹大理石纹理中的[随机图](@entry_id:270323)案，而不是捕捉你主体的真实形态。

一种名为 **Mixup** 的优雅深度学习技术，为我们训练[插补模型](@entry_id:169403)时驾驭这种权衡提供了一种美妙的方式。这个想法非常反直觉：我们不仅在已有的数据点上训练网络，还在通过混合两个真实数据点创建的“虚拟”数据点上进行训练。我们创建一个新的输入 $\tilde{x} = \lambda x_i + (1-\lambda) x_j$ 和一个新的目标标签 $\tilde{y} = \lambda y_i + (1-\lambda) y_j$，并要求网络学习这种“中间”映射。

这个过程充当了一个强大的正则化器。它鼓励网络学习到的函数更平滑、更简单，这显著降低了方差并防止了过拟合。然而，这种平滑性引入了少量的偏差，因为真实的数据生成过程在任意两点之间可能并非完全线性。

在这里，对学习过程的深刻理解启发了一个绝妙的策略：**退火计划** [@problem_id:3169325]。
1.  **在训练早期**，模型不稳定且方差很高。我们使用强 Mixup（选择接近 $0.5$ 的 $\lambda$）来施加平滑性，减少方差，并引导模型进入[解空间](@entry_id:200470)中一个有希望的、稳定的区域。我们用少量偏差换取方差的大幅降低。
2.  **在训练[后期](@entry_id:165003)**，模型已经稳定，其方差也得到了更好的控制。现在的主要目标是减少偏差并捕捉数据分布的细粒度细节。我们逐渐减弱 Mixup 的强度，将混合参数 $\alpha(t)$ [退火](@entry_id:159359)至零。
3.  **在最后几个轮次**，我们完全关闭 Mixup。这消除了正则化器带来的偏差，并允许模型充分利用其全部能力来达到对真实数据的最佳拟合。

这种对[偏差-方差权衡](@entry_id:138822)的动态控制是这些原理作用的完美体现。我们从*为何*数据会缺失的哲学问题开始，使用 MCAR、MAR 和 MNAR 的严谨分类法来诊断我们的情况，并部署深度学习的力量——不是作为一个黑箱，而是作为一个由深刻理论原理指导的、经过精细调校的工具——来填补空白，揭示完整的图景。

