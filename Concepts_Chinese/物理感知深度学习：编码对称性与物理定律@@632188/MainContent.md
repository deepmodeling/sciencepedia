## 引言
深度学习与物理学领域之间正在形成一种深刻的伙伴关系，有望重塑科学发现的格局。这种结合超越了简单地将通用算法应用于大型数据集的范畴，它涉及一种更深层次、更有原则的融合，即教人工智能像物理学家一样“思考”。其核心挑战在于弥合数据驱动的[模式识别](@entry_id:140015)与支配宇宙的基本定律之间的鸿沟。我们如何能确保一个模型不仅能拟合数据，还能尊重对称性和[能量守恒](@entry_id:140514)等基本概念？

本文通过探讨如何将物理原理直接编码到[深度学习模型](@entry_id:635298)的架构和训练过程中来回答这个问题。在第一部分 **“原理与机制”** 中，我们将深入探讨实现这一目标的核心概念。我们将揭示物理演化和算法优化之间共通的数学语言，了解对称性如何被硬编码到网络中，并考察利用现有物理知识指导学习的方法。随后，在 **“应用与跨学科联系”** 部分，我们将展示这些原理的实际应用。我们将通过一系列从[粒子物理学](@entry_id:145253)到[分子生物学](@entry_id:140331)的真实案例，了解这些物理感知模型不仅在进行预测，更在推动新的科学突破。

## 原理与机制

要真正领会[深度学习](@entry_id:142022)与物理学之间蓬勃发展的伙伴关系，我们必须超越新闻头条，深入其内部一探究竟。机器究竟是如何学习自然法则的？这其中所运用的原理并非魔法，而是物理学、计算机科学和数学思想的美妙融合。这一进程并非要用算法取代物理学家，而是要构建一种新型的科学仪器——一个计算伙伴，它既拥有在复杂性中发现模式的非凡能力，又受到永恒物理定律的指引。

### 物理学与学习的共通语言

乍一看，物理动力学的世界——粒子运动、场演化、[能量守恒](@entry_id:140514)——似乎与[机器学习优化](@entry_id:169757)的抽象领域相去甚远。但让我们仔细观察。想象一个球滚下崎岖的山坡。它会怎么做？它会自然地寻找最低点，其运动受重力和[摩擦力](@entry_id:171772)支配。它的路径是一条使其[势能](@entry_id:748988)最小化的轨迹。

现在，考虑训练[神经网](@entry_id:276355)络的过程。我们定义一个“[损失函数](@entry_id:634569)”，它本质上是一个衡量网络预测错误程度的数学景观。损失值越高，意味着预测越差。这个被称为 **梯度下降** 的训练过程，是一种沿着最陡峭的方向微调网络参数（或称“权重”）以降低损失的算法。换句话说，这就像让一个球在损失函数的山坡上滚下，以找到一个最低点。

这不仅仅是一个松散的比喻，其联系在数学上是精确的。一个向[能量最小化](@entry_id:147698)演化的物理系统可以用一个“梯度流”方程来描述。如果我们用最简单的方法——前向欧拉法——来数值求解这个方程，我们得到的更新规则与梯度下降的更新规则是 *完全相同* 的 [@problem_id:2172192]。两者都由同一个基本思想描述：沿着负梯度的方向迈出一小步。这揭示了一种深刻的统一性：机器 *学习* 的过程是物理系统 *演化* 的镜像。这种由景观、梯度和最小值构成的共通语言，是连接这两个领域的基础桥梁。

### 教会机器关于对称性的知识

从 [Isaac Newton](@entry_id:175889) 到 [Emmy Noether](@entry_id:155198)，对称性是整个物理学中最强大的思想之一。如果你把实验搬到另一个城市（[平移对称性](@entry_id:171614)），或者换个方向（旋转对称性），或者明天而不是今天进行实验（[时间平移对称性](@entry_id:261093)），物理定律并不会改变。一个声称学习物理学的深度学习模型必须尊重这些相同的对称性。否则，它的预测将是毫无物理意义的胡言乱语。

考虑一个高能物理实验，其中[粒子碰撞](@entry_id:160531)产生一簇被称为“喷注”的新粒子。我们希望建立一个模型，例如，能够分类引发这个喷注的粒子类型。喷注本质上是一组粒子；我们在计算机中列出它们的顺序是完全任意的。交换集合中两个粒子的标签不应影响喷注的总能量。这个性质被称为 **[置换不变性](@entry_id:753356)**。一个计算整个集合属性的函数 $f$ 必须满足 $f(..., x_i, ..., x_j, ...) = f(..., x_j, ..., x_i, ...)$。

但是，如果我们想为每个 *单个* 粒子分配一个属性，比如一个表明其类型的标签呢？如果我们交换粒子 $i$ 和 $j$，它们的标签也应该随之交换。标签属于粒子，而不是它在列表中的任意位置。这个性质被称为 **[置换](@entry_id:136432)[等变性](@entry_id:636671)**。函数的输出必须以其输入变换的相同方式进行变换 [@problem_id:3510650]。

我们如何构建一个尊重这些对称性的[神经网](@entry_id:276355)络呢？对于[置换不变性](@entry_id:753356)，一个优雅而强大的策略是独立地对每个粒子的[特征向量](@entry_id:151813)进行某种变换，然后在进一步处理之前简单地将结果相加。由于加法是可交换的（$a+b = b+a$），最终结果自然对粒子的顺序不敏感。这个简单而深刻的思想是 **Deep Sets** 等架构的基础，这些架构通过构造来保证[置换不变性](@entry_id:753356) [@problem_id:3510650]。

### 用几何乐高搭建：[等变网络](@entry_id:143881)

[置换](@entry_id:136432)只是对称性的一种。在化学和[材料科学](@entry_id:152226)中，分子和晶体的几何结构至关重要。水分子的能量取决于氢原子和氧原子之间的距离，但同样关键的是两个 O-H 键之间的夹角。一个只“看到”距离的模型将无法区分被拉伸的分子和被弯曲的分子，因此永远无法预测依赖于形状的属性，比如[振动频率](@entry_id:199185)或固体中的剪切刚度 [@problem_id:2777670]。

为了建立一个物理上真实感的模型，我们需要尊重三维空间的对称性：平移和旋转。模型对分子能量的预测不能因为我们平移或旋转整个分子而改变。这就是 **[等变神经网络](@entry_id:137437)** 发挥作用的地方。这些是根据几何原理设计的复杂架构。它们的内部特征不是简单的数字，而是几何对象本身——向量、张量以及在旋转下具有明确行为的其他量。

支撑这些网络的数学是深刻而优美的，直接源于量子力学中用于描述角动量的群论。在这些网络中组合两个几何特征的过程遵循着与物理学家用来确定两个耦合电子[总角动量](@entry_id:155748)相同的规则——使用 **Clebsch–Gordan系数** [@problem_id:3449548]。网络成为这些“神经张量积”的级联，创造出越来越复杂和抽象的几何特征，同时严格保持系统的[旋转对称](@entry_id:137077)性。这使得模型能够学习支配物质世界的复杂、依赖于方向的力。

然而，至关重要的是要理解，虽然架构是基于物理原理的，但网络内部训练的单个权重和偏置并没有直接的[一一对应](@entry_id:143935)的物理解释。它们不是“[键强度](@entry_id:149044)”或“[部分电荷](@entry_id:167157)”。它们仅仅是一个高度灵活函数的最优参数。许多不同的参数集可以导致相同的正确物理预测，这使得网络成为一个强大但通常不透明的“黑箱” [@problem_id:2456341]。

### 站在巨人的肩膀上：物理知识指导的学习

虽然[神经网](@entry_id:276355)络能从零开始学习物理学令人印象深刻，但效率也很低。我们不需要每次建立模型都重新发现牛顿定律。一个更强大的方法是将[深度学习](@entry_id:142022)的模式发现能力与现有的大量物理知识相结合。

其中最有效的策略之一被称为 **Delta-learning ($\Delta$-learning)**。想象一下，我们想用一种计算成本极高的[量子化学](@entry_id:140193)理论，如[耦合簇](@entry_id:190682) (CC)，来以极高的精度预测某个属性。我们还有一个更便宜、精度较低的方法，如[密度泛函理论](@entry_id:139027) (DFT)，它能提供一个不错的初步猜测。与其训练一个[神经网](@entry_id:276355)络从零开始学习整个复杂的 CC 能量，我们可以训练它学习一个更小、更简单的 *修正* 或 *残差*：$\Delta = E^{\mathrm{CC}} - E^{\mathrm{DFT}}$。因为基准 DFT 模型已经捕捉了大部分底层物理，残差 $\Delta$ 是一个“更简单”的待学习函数。这意味着模型需要少得多的数据就能达到高精度，从而极大地提高了样本效率 [@problem_id:2903824]。我们实际上是站在 DFT 这个巨人的肩膀上。

一种更直接地注入物理知识的方法是通过 **[物理信息神经网络](@entry_id:145229) ([PINNs](@entry_id:145229))**。在这里，物理定律本身，以[偏微分方程](@entry_id:141332) (PDEs) 的形式，成为训练目标的一部分。网络不仅在其预测与测量数据点不符时受到惩罚，而且在其输出违反控制性[偏微分方程](@entry_id:141332)时也会受到惩罚。例如，如果我们正在模拟[流体流动](@entry_id:201019)，我们可以在空间和时间的数千个随机点上检查网络预测的速度和压[力场](@entry_id:147325)是否满足 [Navier-Stokes](@entry_id:276387) 方程。因此，网络被迫寻找一个既与数据一致又与基本运动定律一致的解。

### 前沿挑战：细微之处与巧妙技巧

当然，没有一种工具是没有局限性的，通往发现的道路上铺满了已解决的挑战。PINNs 尽管功能强大，但有一个与 **谱偏差** 效应相关的特殊弱点。具有平滑激活函数的[神经网](@entry_id:276355)络发现，学习低频、平滑的函数比学习高频、快速变化的函数要容易得多。这被称为 **频率原则**。

在处理涉及尖锐锋面或冲击波的物理现象时，例如在[对流](@entry_id:141806)主导的[流体流动](@entry_id:201019)中，这种偏差就成了一个问题。这些尖锐特征包含大量高频内容。一个标准的 PINN 在其对平滑性的偏好引导下，将难以捕捉陡峭的梯度，通常会产生一个模糊、弥散的真实解的近似。研究人员正在积极开发巧妙的缓解策略，例如自适应地将计算精力集中在变化剧烈的区域，或将问题重新表述为对尖锐梯度不那么敏感的“弱形式” [@problem_id:3410614]。

另一个实际障碍源于基于梯度的学习的本质。从输入参数到最终损失的整个流程必须是可微的。但许多科学分析涉及不可微的操作，如取集合中的最大值（`max`）或对列表进行排序（`sort`）。为了克服这个问题，我们可以用平滑、可微的近似来代替这些“硬”操作。例如，`max` 函数可以用“软最大值”函数（LogSumExp 操作）来近似，该函数在任何地方都提供可用的梯度，并在受控的极限下收敛到真正的 `max`。类似的技术也存在于排序中，允许梯度信息流经整个复杂的模拟和分析流程 [@problem_id:3511357]。

### 学习定律本身：[算子学习](@entry_id:752958)

到目前为止，我们讨论的网络是学习如何为 *特定* 输入集求解一个物理系统。但是，如果我们能更进一步，学习那个将 *任何* 有效输入函数映射到其相应解函数的底层数学 *算子* 呢？这就是 **[算子学习](@entry_id:752958)** 的宏伟目标。

我们不再学习有限维向量之间的映射，而是寻求学习无限维[函数空间](@entry_id:143478)之间的映射。两种杰出的架构引领着这一潮流。**[深度算子网络](@entry_id:748262) ([DeepONet](@entry_id:748262))** 通过学习一组[基函数](@entry_id:170178)和组合它们以形成输出解所需的系数来工作。这就像为手头的问题学习一个定制的[傅里叶级数](@entry_id:139455)。**[傅里叶神经算子 (FNO)](@entry_id:749541)** 采用了一种受信号处理启发的不同方法。它在[频域](@entry_id:160070)中执行“卷积”，有效地学习一个广义滤波器来应用于输入函数的[光谱](@entry_id:185632)。由于其对[傅里叶变换](@entry_id:142120)的依赖，FNO 对常规网格上的物理学具有强大的[归纳偏置](@entry_id:137419)，并且在学习复杂[流体动力学](@entry_id:136788)问题的解算子方面取得了显著成功 [@problem_id:3513285]。

### 知道我们所不知道的：[量化不确定性](@entry_id:272064)

一个科学模型的预测如果没有对其不确定性的估计，就是不完整的。在物理学的[深度学习](@entry_id:142022)中，我们必须区分两种基本类型的不确定性。

**偶然不确定性** 是系统或测量过程中固有的、不可减少的随机性。即使有完美的模型和无限的数据，我们也无法消除这种噪声。

另一方面，**认知不确定性** 是我们模型自身因缺乏知识而产生的不确定性。它源于训练数据有限。在参数空间中我们有大量数据的区域，认知不确定性会很低。在未探索的区域，它会很高。这是我们可以通过收集更多数据来减少的不确定性，或者更有趣的是，通过 [PINNs](@entry_id:145229) 融入更多物理知识来减少，因为 PINNs 以物理约束的形式提供了“数据” [@problem_id:3513334]。

一种简单而有效的估计认知不确定性的方法是训练一个模型的 **集成**。通过使用不同的随机初始化或在数据的不同[子集](@entry_id:261956)上训练几个网络，我们得到了一系列预测结果。一个大的[分布](@entry_id:182848)范围表明[认知不确定性](@entry_id:149866)高——模型之间存在分歧，因为数据不足以将它们约束到单一的解决方案。

即使有了这些工具，我们也必须保持谦逊。这些方法量化的是 *在假设的模型类别内* 的不确定性。但是，如果我们自己的控制方程本身就是对现实的不完整描述呢？这种 **模型形式差异** 代表了更深层次的不确定性，标准方法可能无法捕捉到，提醒我们科学永远是一个不断完善我们对世界理解的过程 [@problem_id:3513334]。

