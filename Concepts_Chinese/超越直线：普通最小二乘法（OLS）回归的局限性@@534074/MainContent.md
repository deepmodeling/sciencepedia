## 引言
[普通最小二乘法](@article_id:297572)（OLS）回归是[统计分析](@article_id:339436)的基石，因其能够优雅简洁地在一组数据点中找到“最佳”直线而备受推崇。它通常是从业者学习的第一个，有时也是唯一一个回归工具。然而，其有效性建立在一系列严格的假设之上，而这些假设在混乱复杂的现实世界中很少能被完美满足。OLS 的理论理想与其在实践中的应用之间的差距，正是更深层次统计理解的起点。本文旨在探讨这一差距，通过探索 OLS 的根本性缺点，但这并非为了批判，而是一场通往更稳健、更复杂建模方法的旅程。

首先，在“原理与机制”一章中，我们将剖析 OLS 的核心机制，以理解其对离群值的内在敏感性、当假设崩溃时的脆弱性，以及其对非设计任务（如分类）的不适用性。随后，“应用与跨学科联系”一章将阐明，这些失败本身如何成为创新的[催化剂](@article_id:298981)，推动了从生物化学到金融等领域先进方法的发展。通过理解 OLS 的直线在何处失效，我们才能开始欣赏世界丰富的非线性画卷，以及为探索它而设计的工具。

## 原理与机制

[普通最小二乘法](@article_id:297572)（OLS）回归通常是我们在统计学中学习的第一个工具，这不无道理。它优雅、直观，而且用途惊人地广泛。它承诺能找到穿过一[团数](@article_id:336410)据点的唯一“最佳”直线。但“最佳”意味着什么？对 OLS 而言，这意味着找到一条能使每个点到直线的*平方*距离之和最小化的线。这个简单的选择带来了深远的影响，创造了一个强大但专门化的工具。要真正理解统计学，我们必须像一位大师级工匠一样，不仅要了解工具能做什么，还要了解它*不能*做什么。这段探寻 OLS 局限性的旅程并非关乎失败，而是为了更深入地洞察数据丰富多样的面貌。

### 平方的暴政：当离群值主宰一切

OLS 的第一个也是最直观的缺点直接源于其核心原理：最小化*平方*误差。对一个数进行平方会产生戏剧性的效果——小数保持小数，但大数会变得巨大。一个为 2 的误差变成了 4，但一个为 10 的误差变成了 100。这意味着 OLS 极度惧怕大误差，并会不惜一切代价避免它们。

想象一下，我们有一组简单的观测值：$\{1, 1, 1, 1, 21\}$。我们想找到一个单一的数字 $\beta_0$ 来最好地代表这个集合。OLS 告诉我们使用[样本均值](@article_id:323186)，即 $\frac{1+1+1+1+21}{5} = 5$。请注意，单个[离群值](@article_id:351978) 21 是如何将估计值拖拽到远离大部分数据的地方。OLS 看到 5 到 21 的距离是 16，它更关心的是最小化其平方（$16^2=256$），而不是其他四个点的小误差。

现在，考虑另一种选择，一种称为[最小绝对偏差](@article_id:354854)（LAD）的稳健估计器，它最小化的是*绝对*误差之和，即 $\sum |y_i - \beta_0|$。对于一组点，这其实就是中位数。集合 $\{1, 1, 1, 1, 21\}$ 的中位数是 $1$。LAD 估计值牢牢地锚定在“典型”值上，将离群值视为众多选票中的一票，而不是一个尖叫的暴君 [@problem_id:3183059]。这种对[离群值](@article_id:351978)的敏感性就是“平方的暴政”—— OLS 的一个基本特性，使其在存在异常数据时变得脆弱。

### 沙上之屋：当模型的假设崩塌时

OLS 的理论依据是著名的**[高斯-马尔可夫定理](@article_id:298885)**。该定理指出，如果满足一组特定的“理想”条件，OLS 就是**[最佳线性无偏估计量](@article_id:298053)（BLUE）**——这意味着在一类简单、表现良好的模型中，它是你能得到的最精确的估计量。但是，当这些理想条件，即 OLS 的基本假设，在现实世界中不成立时会发生什么？事实证明，OLS 之屋可能建在沙滩之上。

#### 恒定方差的神话

OLS 的核心假设之一是**[同方差性](@article_id:638975)**：即我们测量中的随机噪声或误差，无论输入值如何，都具有相同的方差。想象一下，你正在模拟工厂生产线上次品数量与生产速度的关系。你可能会预期，在低速时，次品数量始终很低（例如，$1 \pm 1$），但在高速时，它可能会变得更加不稳定（例如，$10 \pm 10$）。方差不是恒定的；它随着均值增长。

这是**[异方差性](@article_id:296832)**的典型案例。在这种情况下，数据违反了[同方差性](@article_id:638975)假设。当你绘制[残差](@article_id:348682)（OLS 拟合的误差）与预测值的关系图时，你会看到一个独特的漏斗形状——误差的[散布](@article_id:327616)随着预测值的增加而增加 [@problem_id:3114940]。虽然 OLS 仍然会给你一个关于关系的[无偏估计](@article_id:323113)，但它不再是“最佳”估计量。更重要的是，OLS 软件产生的标准误和置信区间变得不可靠，因为它们是在假定方差恒定的情况下计算的。你对结果的确定性可能比你想象的要高得多或低得多。

#### 误差不相关的幻觉

另一个关键假设是误差是不相关的。一次测量的随机误差不应该对下一次测量有任何“记忆”或影响。这通常是一个合理的假设，但对于随时间收集的数据来说，它完全失效了。考虑模拟每日气温。如果今天异常炎热（一个正误差），那么明天也很可能炎热。误差是序列相关的。

当 OLS 应用于具有相关误差的数据时，会出现与[异方差性](@article_id:296832)类似的问题。系数估计值仍然是无偏的，但它们不再是最高效的。至关重要的是，OLS 计算标准误的公式是系统性错误的，通常会低估真实的不确定性 [@problem_id:2885116]。这可能导致危险的过度自信，我们可能会宣称一个关系在统计上是显著的，而实际上数据并不支持如此强的结论。为了正确处理这类数据，我们需要像[广义最小二乘法](@article_id:336286)（GLS）这样的方法，这些方法专门设计用来解释误差中的“记忆”。

#### 隐藏变量的危险

也许最微妙和危险的假设是**[外生性](@article_id:306690)**——即我们的预测变量 $X$ 与未观测到的误差项 $\epsilon$ 不相关。当这个假设被违反时，我们就有了**[内生性](@article_id:302565)**，此时 OLS 产生的结果可能不仅是低效的，而且是根本性有偏和误导的。一个经典的例子是，通过一个简单的 OLS 回归发现冰淇淋销量与溺水事件之间存在正相关关系，从而得出冰淇淋销量导致溺水的结论。问题在于一个隐藏的或“遗漏的”变量：温度。炎热的天气既导致了更多的冰淇淋销量，也导致了更多的人去游泳（从而导致更多的溺水事件）。因为我们的预测变量（冰淇淋销量）与这个未观测到的因素相关，OLS 错误地将温度的影响归因于冰淇淋。在经济学和社会科学中，受控实验很少见，[内生性](@article_id:302565)及相关的[选择偏差](@article_id:351250)等问题是 OLS 单独无法解决的核心挑战 [@problem_id:1936115]。

### 用错了工具：在分类领域使用回归

到目前为止，我们已经看到 OLS 即使在其预定用途——预测一个连续的数值——上也会出现问题。当我们滥用 OLS 去完成一个完全不同的任务时，问题变得更加明显：**分类**，其目标是预测一个分类标签，例如“垃圾邮件”或“非垃圾邮件”，通常编码为 $1$ 或 $0$。

#### 无意义的预测

如果我们使用 OLS 来预测一个二元的 $0/1$ 结果，我们实际上是在试图模拟结果为 $1$ 的概率。但根据定义，概率被限制在区间 $[0, 1]$ 内。OLS 拟合的是一条无界的直线，它完全不尊重这个约束。它可能，而且经常会，产生像 $1.5$ 或 $-0.2$ 这样的预测。这些作为概率是毫无意义的。

为什么会发生这种情况？原因深藏于 OLS 的机制之中。单个数据点的预测值 $\hat{y}_i$ 是*所有*观测响应的加权平均值，即 $\hat{y}_i = \sum_{j} h_{ij} y_j$。权重 $h_{ij}$ 来自一个称为**[帽子矩阵](@article_id:353142)**的[特殊矩阵](@article_id:375258)。一个关键的洞见是，虽然每行权重之和为一，但并不能保证它们都为正。一个具有高**杠杆**的数据点——即在预测变量空间中远离其他点的点——可以对回归线产生强大的拉力。这可能导致其他点的权重为大的负值，迫使[加权平均](@article_id:304268)值远远超出 $[0, 1]$ 的合理范围 [@problem_id:3117177]。如果我们通过添加多项式项来使 OLS 模型更加“灵活”，这个问题可能会变得更糟。模型可能更好地拟合训练数据，但多项式可能会在观测数据范围之外剧烈[振荡](@article_id:331484)，导致更极端和荒谬的预测 [@problem_id:3117120]。

#### 用错误的记分卡评判

当 OLS 对一个真实值为 $0$ 的观测值产生 $1.01$ 的预测时，它自己的记分卡——平方误差是 $(1.01 - 0)^2 \approx 1.02$。这似乎是一个温和的惩罚。然而，从概率的角度来看，这是一个灾难性的失败。模型预测结果有“101%的可能性”是类别 1，而实际上它是类别 0。

评估概率预测的正确记分卡不是均方误差（MSE），而是像**[二元交叉熵](@article_id:641161)（BCE）**或[对数损失](@article_id:642061)这样的度量。[对数损失](@article_id:642061)会严厉惩罚那些既自信又错误的预测。对于真实标签为 $0$ 的同一个 $1.01$ 的预测（在计算[对数损失](@article_id:642061)时，这个值会被截断到略小于 $1$），将导致近乎无限的惩罚。一个 OLS 模型可以获得一个具有欺骗性的低 MSE，同时却有灾难性的高[对数损失](@article_id:642061)，这证明它从根本上是在为错误的目标进行优化 [@problem_id:3117164]。

#### 为什么准确率不够：校准与决策

这就引出了对 OLS 用于分类的最重要的批评。即使一个简单的 OLS 模型（通过将其输出在 $0.5$ 处设定阈值）获得了良好的分类准确率，它仍然是一个劣等的工具。原因是**校准**。一个校准良好的模型，其概率估计是值得信赖的。当它预测有 70% 的降雨概率时，大约 70% 的时间里确实会下雨。OLS 的预测通常没有得到很好的校准。

这在现实世界中至关重要，因为决策通常涉及不对称的成本。对于一个医生决定是否进行一项有风险的手术，假阴性（漏诊）的代价远高于[假阳性](@article_id:375902)（进行更多测试）。最优决策规则不是在预测疾病概率 >0.5 时手术，而是在大于某个其他阈值（比如 $0.2$）时手术，这个阈值取决于相对成本 [@problem_id:3117142]。为了有效地使用这样的阈值，医生需要可靠、校准良好的概率。因为 OLS 提供的是校准很差的分数，它会导致次优的决策和更高的成本，即使其简单的准确率看起来可以接受 [@problem_id:3117104]。这凸显了一个优美的原则：好的建模通常涉及将*估计概率*的任务与*做出决策*的任务分开。像逻辑回归这样的模型是为第一个任务设计的，为第二个任务提供校准好的输入。OLS 则粗暴地将它们混为一谈。同样的逻辑也适用于结果有多个有序类别（例如“轻度”、“中度”、“重度”）的情况。通过分配像 $0, 1, 2$ 这样的数字强行将其纳入 OLS 框架，会丢失关键信息，并导致与专为[序数数据](@article_id:343380)设计的模型相比更差的预测 [@problem_id:3117144]。

### 维度灾难：OLS 在数据前沿

最后，OLS 在现代高维世界中面临巨大挑战，我们可能有数百个预测变量，而观测数量（$p \approx n$）仅略多一些。在这种情况下，很可能一些预测变量与其他变量高度相关（**共线性**）。当这种情况发生时，OLS 很难分清它们的各自影响。

在数学上，OLS 估计的协方差矩阵 $\text{Cov}(\hat{\beta}) = \sigma^2(X^{\top}X)^{-1}$ 涉及对矩阵 $X^{\top}X$ 求逆。当预测变量高度相关时，这个矩阵变得接近奇[异或](@article_id:351251)**病态**，其逆矩阵会“爆炸”。这意味着我们系数估计的方差可能会变得巨大。OLS 报告的标准误会变得不稳定且大到毫无意义，使得我们无法对单个预测变量的重要性得出任何可靠的结论 [@problem_id:3176559]。OLS 在高维情况下的这种崩溃，是现代[正则化方法](@article_id:310977)如 Ridge 和 [Lasso](@article_id:305447) 回归发展的主要动机，这些方法旨在即使面对维度灾难也能表现良好。

