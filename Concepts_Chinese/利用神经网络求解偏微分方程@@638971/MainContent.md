## 引言
数百年来通过[微分方程](@entry_id:264184)表达的物理定律与现代机器学习力量的交汇，催生了[科学计算](@entry_id:143987)的新[范式](@entry_id:161181)。尽管传统的数值方法功能强大，但其计算量可能很大，而标准的[机器学习模型](@entry_id:262335)在缺乏大型标记数据集的情况下往往会失效。本文探讨了一种弥合这一差距的革命性方法：[物理信息神经网络](@entry_id:145229)（PINNs），它不是从数据中学习，而是从物理学基本定律本身中学习。我们将踏上一段旅程，去理解这一强大的方法论。首先，在“原理与机制”部分，我们将揭示[神经网](@entry_id:276355)络如何利用物理残差和[自动微分](@entry_id:144512)等概念来学习物理规则。随后，在“应用与跨学科联系”部分，我们将见证这一框架如何被应用于解决科学、金融和工程领域的复杂问题，从而开辟模拟和设计的新前沿。

## 原理与机制

每一次伟大的科学飞跃背后，都蕴含着一个简单而强大的思想。用[神经网](@entry_id:276355)络[求解微分方程](@entry_id:137471)的概念也不例外。这是跨越几个世纪的数学物理学与二十一世纪机器学习的美妙结合。但它是如何工作的呢？一台为识别照片中的猫而设计的机器，如何能学会流体的复杂舞蹈、[振动弦](@entry_id:138456)的嗡鸣，或是热量在固体中的流动？答案不在于向它展示数百万个例子，而在于教它游戏的规则——物理定律本身。

### 一种新的导师：物理残差

想象一下，你想训练一个[神经网](@entry_id:276355)络来预测一个[偏微分方程](@entry_id:141332)（PDE）的解，我们称之为 $u(x, t)$。传统的机器学习方法是生成一个庞大的已知解数据集——即 $(x, t)$ 与其对应 $u$ 值的配对——然后训练网络在这些点之间进行插值。这就像通过背诵一本字典来学习一门语言。你或许能背出单词，但无法构成一个新的、有意义的句子。对于许多科学问题而言，生成这样的[数据集成](@entry_id:748204)本高得令人望而却步，甚至根本不可能。

这时，一个深刻的视角转变发生了。我们不再教网络解“看起来像什么”，而是教它解必须“遵守什么规则”。任何PDE都可以写成 $\mathcal{N}[u] = 0$ 的形式，其中 $\mathcal{N}$ 是一个微分算子。例如，对于热方程 $u_t - \alpha u_{xx} = 0$，算子是 $\mathcal{N} = \frac{\partial}{\partial t} - \alpha \frac{\partial^2}{\partial x^2}$。

一个[神经网](@entry_id:276355)络，我们称之为 $u_{\theta}(x, t)$，是一个函数，它以坐标 $(x, t)$ 为输入，并为一组给定的参数 $\theta$ 输出一个预测值。如果我们将这个网络的输出输入到微分算子中，我们就会得到所谓的**物理残差**：$r(x,t;\theta) = \mathcal{N}[u_{\theta}(x, t)]$。如果我们的网络 $u_{\theta}$ 是*精确*解，这个残差将在任何地方都为零。如果它是一个糟糕的近似，残差将会很大。

就这样，我们有了我们的目标。我们可以通过告诉优化器：“找到使物理残差在整个域上尽可能接近零的参数 $\theta$”来训练网络。我们不需要任何预先计算的解。物理方程本身成为了导师，提供信号来引导网络。这将任务从一个监督学习问题转变为一个自监督问题，其中“标签”就是物理定律所要求的零。这就像教人画一个完美的圆，不是通过给他看无数张画，而是给他一个圆规和一条规则：“保持铅笔到中心的距离恒定。”网络正在学习的是“规则”。

### 发现的引擎：[自动微分](@entry_id:144512)

这个优雅的想法取决于一个关键问题：我们究竟如何计算残差？算子 $\mathcal{N}$ 涉及导数，有时是高阶导数。对于模拟板弯曲的双谐方程 $\nabla^4 u = f$，我们需要计算像 $\frac{\partial^4 u}{\partial x^4}$ 这样的四阶导数。我们如何对一个复杂的多层[神经网](@entry_id:276355)络进行[微分](@entry_id:158718)？

有人可能会想到使用[有限差分](@entry_id:167874)，这是传统数值方法的得力工具。但这是一种近似方法，并且众所周知，它容易产生数值误差，特别是对于高阶导数。另一个想法是[符号微分](@entry_id:177213)，就像你手工做的那样。但对于一个有数百万参数的网络来说，得到的表达式将是天文数字般的复杂。

解决方案是现代计算中“不合理有效性”的故事之一：**[自动微分](@entry_id:144512)（AD）**。AD是驱动[深度学习](@entry_id:142022)的引擎，它完美地适用于我们的任务。一个[神经网](@entry_id:276355)络，无论多深，都只是一长串基本运算（加法、乘法、激活函数）的序列。AD是一种巧妙的计算技术，它逐步应用链式法则，计算网络输出相对于其输入（如 $x$ 和 $t$）或其参数（$\theta$）的精确导数。它不是近似；它是通过算法计算出的精确的、解析的导数。

 nhờ vào AD, chúng ta có thể lấy mạng $u_{\theta}(x, t)$ của mình, có thể là một hàm hợp phức tạp như $u_{\theta}(x, y) = v \sigma(w_x x + w_y y + b) + c$, và tính toán các项 như $\nabla^4 u_{\theta}$ với độ chính xác máy tính. điều này cho chúng ta một cách để đánh giá chính xác vật lý còn lại cho bất kỳ điểm nào $(x, t)$ và bất kỳ bộ tham số nào $\theta$, mở đường cho bộ tối ưu hóa của chúng ta làm việc.

多亏了AD，我们可以用我们的网络 $u_{\theta}(x, t)$（它可能是一个像 $u_{\theta}(x, y) = v \sigma(w_x x + w_y y + b) + c$ 这样的复杂[复合函数](@entry_id:147347)）来以[机器精度](@entry_id:756332)计算诸如 $\nabla^4 u_{\theta}$ 这样的项。这为我们在任何点 $(x, t)$ 和任何参数集 $\theta$ 下精确评估物理残差提供了一种方法，为我们的优化器开展工作铺平了道路。

### 约束的交响

一个物理系统不仅仅由其控制方程定义。一把[振动](@entry_id:267781)的小提琴弦在其两端是固定的。一根金属棒的温度在实验开始时是已知的。这些是**边界条件（BCs）**和**初始条件（ICs）**，它们与PDE本身同样至关重要。一个纯粹由数据驱动的模型，如果只在边界数据上训练，将不知道如何在域内部表现；有无数个函数可以拟合边界。PDE提供了缺失的约束。

一个**[物理信息神经网络](@entry_id:145229)（PINN）**被训练来同时满足所有这些约束。我们构建一个复合**[损失函数](@entry_id:634569)**，它是几个项的加权和：
*   **PDE残差损失**：平方残差 $\left| \mathcal{N}[u_{\theta}] \right|^2$ 在大量随机点（称为“[配置点](@entry_id:169000)”）上的平均值。这迫使网络遵守物理定律。
*   **边界条件损失**：网络在边界上的预测与规定的边界值之间的平[方差](@entry_id:200758)。例如，如果我们有 $u(0, t) = 1$，这个损失将是 $|u_{\theta}(0, t) - 1|^2$。
*   **初始条件损失**：网络在时间 $t=0$ 时的预测与给定的初始状态之间的平[方差](@entry_id:200758)，即 $|u_{\theta}(x, 0) - u_0(x)|^2$。

总损失是一场宏大的“约束交响”。优化器的工作是调整网络的参数 $\theta$ 以最小化这个总损失，找到一个能够将控制方程与所有初始和边界约束协调起来的单一函数。这个框架非常灵活。对于多物理场问题，我们可以简单地为不同的方程添加更多的残差，并为其界面上的条件添加更多的损失项，从而教网络尊重多种物理定律及其耦合关系。

### 架构即命运

事实证明，[神经网](@entry_id:276355)络的结构本身——其架构——不仅仅是一个细节，而是一个具有深远物理意义的选择。

#### 平滑的重要性

假设我们正在解决一个像[泊松方程](@entry_id:143763)或亥姆霍茲方程这样的问题，它涉及到像 $u_{xx}$ 这样的[二阶导数](@entry_id:144508)项。为了评估残差，我们需要网络的[二阶导数](@entry_id:144508)是良定义的。如果我们选择一个平滑的[激活函数](@entry_id:141784)，比如[双曲正切](@entry_id:636446) $\tanh(z)$ 或正弦函数，我们的网络 $u_{\theta}$ 就变成了一个无穷次[可微函数](@entry_id:144590)（$C^\infty$）。这非常完美；AD可以计算任意阶的导数，并且残差是一个行为良好的函数。

但如果我们选择流行的**整流线性单元（ReLU）**[激活函数](@entry_id:141784) $\sigma(z) = \max\{0, z\}$ 呢？这将是一场灾难。[ReLU网络](@entry_id:637021)是一个连续但分段线性的函数。它的一阶导数是一系列阶跃函数，而其[二阶导数](@entry_id:144508)几乎处处为零，在“拐点”处有无限大的尖峰（[狄拉克δ分布](@entry_id:267680)）。当AD计算[二阶导数](@entry_id:144508)时，它几乎总是在导数为零的点[上采样](@entry_id:275608)。网络对二阶[物理信息](@entry_id:152556)变得“盲目”。它无法学习曲率，而曲率正是算子 $u_{xx}$ 所描述的本质。为了求解[二阶PDE](@entry_id:175326)的强形式，平滑的激活函数不仅仅是一种偏好，而是一种必需。

#### 硬约束与软约束

有时我们可以巧妙地将物理定律直接构建到架构中。想象一下，我们需要在区间 $[0, 1]$ 上解决一个问题，边界条件为 $u(0)=0$ 和 $u(1)=0$。我们可以将这些作为“软”约束添加到损失函数中。但我们也可以通过构造来“硬”性地强制它们。我们可以将我们的解的[拟设](@entry_id:184384)定义为 $u_{\theta}(x) = x(1-x) \hat{u}_{\theta}(x)$，其中 $\hat{u}_{\theta}(x)$ 是[神经网](@entry_id:276355)络的原始输出。无论 $\hat{u}_{\theta}$ 产生什么，乘法因子 $x(1-x)$ 都保证了 $u_{\theta}(0)=0$ 和 $u_{\theta}(1)=0$。这将优化器从学习边界条件的任务中解放出来，使其能够将其全部能力集中在满足域内部的PDE上。

#### 与经典方法的深层联系

网络架构与物理学之间的关系甚至更深，呼应了经典数值方法的结构。

*   一个以其“[跳跃连接](@entry_id:637548)”而闻名的**[残差网络](@entry_id:634620)（[ResNet](@entry_id:635402)）**，通过 $x_{k+1} = x_k + \mathcal{N}(x_k)$ 来更新其状态，其中 $\mathcal{N}$ 是一个网络块。这在形式上与[求解常微分方程](@entry_id:635033)（ODE）的**前向欧拉法** $x_{k+1} = x_k + h f(x_k)$ 完全相同，其中[网络深度](@entry_id:635360)扮演着时间的角色。我们甚至可以设计“隐式”网络层来求解像 $x_{k+1} = x_k + h f(x_{k+1})$ 这样的方程，模仿著名的稳定的**[后向欧拉法](@entry_id:139674)**。
*   在求解瞬态PDE时，我们可以选择一种**全局时空方法**，即单个网络 $u_{\theta}(x, t)$ 一次性学习整个解。这避免了传统方法中逐步累积的误差。或者，我们可以使用一种**序贯时间步进方法**，训练一系列网络，其中每个网络根据前一个网络的解来学习当前时间步的解。这种方法通过构造强制了因果关系，但可能会像经典求解器一样遭受[误差累积](@entry_id:137710)。
*   我们甚至可以修改损失函数来模仿不同的经典方法。与其最小化“强形式”残差 $\mathcal{N}[u_{\theta}]$，我们可以使用分部积分来构建一个“弱形式”或**变分**损失。这是强大的**有限元法（FEM）**的基础。弱形式PINN需要较低阶的导数（例如，对于[二阶PDE](@entry_id:175326)只需要[一阶导数](@entry_id:749425)），这使其与更广泛的[激活函数](@entry_id:141784)兼容，并常常导致一个行为更好的[优化问题](@entry_id:266749)。

这些联系不仅仅是奇闻趣事。它们揭示了一种美妙的、潜在的统一性：[神经网络架构](@entry_id:637524)不仅仅是黑盒子；它们是一种用于[编码计算](@entry_id:266286)和物理原理的新的、富有表现力的语言。

### 学习的前沿：挑战与创新

尽管[PINNs](@entry_id:145229)优雅，但它们并非万能灵药。它们面临一个重大而有趣的挑战，称为**谱偏差**。[神经网](@entry_id:276355)络在使用标准梯度下降进行训练时，有一种令人困惑的倾向，即学习简单、低频模式的速度远快于复杂、[高频模式](@entry_id:750297)。

对于许多现实世界的物理系统来说，这是一个巨大的问题。想想超音速飞机前的冲击波、反应器中锋利的化学前沿，或是[湍流](@entry_id:151300)中的微小[涡流](@entry_id:271366)。这些现象富含高频内容。一个标准的PINN会很乐意学习解的光滑、缓慢变化的部分，但会极力挣扎于捕捉尖锐的梯度，导致模糊、不准确的近似。这是学习过程的一种偏差，不要与PDE的**刚性**相混淆，后者是物理算子具有巨大不同尺度的内在属性。虽然刚性也会使PINN的[损失景观](@entry_id:635571)难以优化，但谱偏差是一种更基本的学习病理。

克服谱偏差是研究的前沿。最优雅的想法之一是通过改变网络的基本构建块来改变网络的[归纳偏置](@entry_id:137419)。如果我们用`sin`激活函数代替`[tanh](@entry_id:636446)`激活函数会怎样？通过用[正弦波](@entry_id:274998)构建网络，我们创造了一个天生适合表示频率的函数逼近器。像**正弦表示网络（SIRENs）**这样的架构，通常与[残差连接](@entry_id:637548)相结合，可以更有效地学习复杂解的全谱，使其能够准确地解析[非均匀介质](@entry_id:750241)中波的多尺度行为。

最后，机器学习工具箱中的标准技巧，如正则化，又如何呢？在这里，我们必须小心行事。PDE残差已经是一个强大的正则化器，可以说是我们能要求的最具物理意义的正则化器。添加一个强大的、通用的正则化器，如L2[权重衰减](@entry_id:635934)，可能会引入一个冲突的偏差，将解从真实的物理学中拉开，损害保真度。少量正则化可能有助于[稳定训练](@entry_id:635987)，但在一个采样密集的[适定问题](@entry_id:176268)中，应该让物理学自己说话。在PINNs的世界里，更少的正则化往往意味着更多。

物理信息学习的旅程才刚刚开始。这是一个充满深刻问题、惊人联系和巨大潜力的领域。通过将自然的基本定律直接编码到学习过程中，我们不仅仅是在构建更好的模拟器；我们正在探索一种科学发现本身的新[范式](@entry_id:161181)。

