## 初始之法：从常规网络到奇异架构及其超越

在我们之前的讨论中，我们揭示了一个相当优美的思想：[神经网络](@article_id:305336)在其诞生之初——即初始化时——是一个精密的动力学系统。为了有效地学习，它必须被置于一个特殊的状态，一个“[混沌边缘](@article_id:337019)”的状态，信号可以在其中穿过许多层而不会消失于无形或爆炸成无意义的东西。我们看到，仔细选择初始随机权重的方差，遵循像 Xavier 或 He 这样的方案，是实现这种微妙平衡的秘诀。

但是，我们之前考虑的网络是简单的层链，整洁有序。而现代深度学习的世界堪称一片真正的丛林，一个由各种奇异而强大的架构组成的动物园，其中的连接会分支、合并，甚至回环。当我们冒险进入这片荒野时，我们优雅的信号传播原理会发生什么？初始之法是否仍然重要，还是我们需要新的技巧？让我们踏上旅程一探究竟。

### 驯服架构动物园：[残差网络](@article_id:641635)与并行网络

现代深度学习最重要的突破之一是[残差网络 (ResNet)](@article_id:638625) 的发明。其思想看似简单：除了通过一个层块进行标准变换外，我们还增加了一个“跳跃连接”，允许输入绕过该层块并直接与输出相加。现在，该层块学习一个*[残差](@article_id:348682)*函数 $f(x)$，最终输出为 $y = x + f(x)$。这个简单的技巧让研究人员能够训练出前所未有深度的网络——数百甚至数千层深。

但这个看似无害的添加为我们的初始化理论提出了一个新的难题。想象一下，流入该层块的信号 $x$ 具有一定的方差——衡量其“摆动”或信息内容的指标。如果使用像 Xavier 这样的方案进行适当初始化，变换 $f(x)$ 旨在产生一个具有大致相同方差的输出。那么，当我们把它们相加时会发生什么呢？当你把两个不相关的“摆动”源相加时，它们的方差会相加。$y$ 的方差大约变成 $x$ 方差的*两倍*。如果你堆叠许多这样的块，信号将呈指数级爆炸！

稳定的信号流原理没有被打破，但它需要一个更局部、更细致的应用。事实证明，解决方案与问题本身一样优雅而巧妙。为了抵消方差加倍，我们可以简单地将输出乘以一个因子 $1/\sqrt{2}$。由于方差与信号的平方成正比，这种缩放会将方差减半，使其回到[期望](@article_id:311378)的水平并恢复稳定性 [@problem_id:3200151]。这是一个美妙的提醒：虽然基本原理是普适的，但其应用需要仔细考虑网络的局部结构。

同样的想法也适用于其他复杂架构。以使用多分支结构的“Inception”模型为例。它们不是将信号强制通过单一路径，而是将其发送到几个具有不同滤波器大小的并行卷积路径上，然后通过求和来合并结果。这使得网络能够同时捕捉多个尺度的特征。在这里，将（比如说）三个分支的输出相加的行为同样可能导致方差增加三倍。解决方案是相同的：我们必须缩减每个分支的贡献，以使它们的组合方差不会爆炸 [@problem_id:3200132]。总的“方差预算”必须分配给所有贡献路径。

### 与其他力量的精妙共舞：[归一化](@article_id:310343)与优化

[权重初始化](@article_id:641245)并非在真空中运作。它的效果与[深度学习](@article_id:302462)中另外两种强大的力量紧密交织：[归一化层](@article_id:641143)和[优化算法](@article_id:308254)。

以[批量归一化](@article_id:639282) (BN) 为例，这是一种将每个小批量数据内的激活值[归一化](@article_id:310343)为零均值和单位方差的技术。这在训练过程中具有强大的稳定作用。在一个非常深的 [ResNet](@article_id:638916) 中，我们可以用一种极其巧妙的方式将初始化和归一化结合起来。我们希望网络在开始时易于训练。对于深度网络来说，一个简单的任务是学习一个[恒等函数](@article_id:312550)——即将输入原封不动地传递过去。这确保了梯度可以从网络末端一直流回起点而不会消失。我们可以通过将[残差块](@article_id:641387)的 BN 层中最后的缩放参数（称为 `gamma` 或 $\gamma$）初始化为零来实现这一点。这有效地“静音”了[残差](@article_id:348682)分支，使得该块计算 $y = x + 0 = x$，一个完美的[恒等映射](@article_id:638487)！

但是，如果[残差](@article_id:348682)分支是静音的，网络如何学习任何复杂的东西呢？这正是 He 初始化发挥其关键作用的地方。它确保了被静音分支的*内部*工作机制是良好且随时准备启动的。随着训练的开始，网络可以逐渐学习将 $\gamma$ 从零增加。由于分支被正确初始化，它可以平滑而稳定地“淡入”其贡献，而不会引起突然的爆炸或不平衡 [@problem_id:3134429]。这是一种大师级的协同作用：一个技巧为梯度建立了一条稳定的高速公路，而另一个技巧则为学习有用特征准备好了辅路。

那么优化器呢？像 Adam 这样能够动态调整每个参数学习率的复杂自适应[算法](@article_id:331821)，是否会使精心的初始化变得过时？乍一看，这似乎是合理的。Adam 对参数的更新大致与其梯度除以其梯度平方的平方根成正比。这种归一化使得步长对梯度的绝对大小不那么敏感。因此，如果一个糟糕的初始化导致所有梯度都大了十倍，Adam 基本上可以纠正它，而标准的[随机梯度下降](@article_id:299582) (SGD) 则会采取毁灭性的大步长 [@problem_id:3200114]。

然而，这种鲁棒性有其局限性。如果我们的初始化非常糟糕，以至于将所有[神经元](@article_id:324093)都推入其“饱和”区域——即输出平坦且[导数](@article_id:318324)为零的区域——那么初始梯度将几乎为零。Adam 尽管聪明，也无法无中生有。如果没有梯度信号，就没有什么可以适应的，学习就会陷入停滞。因此，一个好的初始化不仅仅是为了获得正确尺度的梯度；它关乎于确保信号的存在。它提供了一片肥沃的土壤，强大的优化器可以在其上施展魔法。

### 超越预测：生成模型与[科学计算](@article_id:304417)

初始之法远远超出了仅仅训练分类器的范畴。其原理在新兴的[生成模型](@article_id:356498)和科学计算领域中至关重要。

以[变分自编码器 (VAE)](@article_id:301574) 为例，这是一种可以学习生成新数据的模型，例如人脸图像，或者在更科学的背景下，合成生物数据 [@problem_id:2439757]。VAE 面临一种被称为“后验坍塌”的奇特失败模式。该模型由一个[编码器](@article_id:352366)（将输入[数据压缩](@article_id:298151)成一个潜码 $z$）和一个解码器（试图从 $z$ 重建输入）组成。在后验坍塌中，解码器变得过于强大，以至于它学会了完全忽略潜码。它只是记住一个“平均”输出，并无论 $z$ 中的信息如何都产生这个输出。编码器放弃了，模型也就无法学习到数据的有意义表示。

初始化如何提供帮助？通过一种精心计算的“示弱”策略。如果我们将解码器最后一层的[权重初始化](@article_id:641245)得非常小，我们就能使解码器在初始时变得既弱又“懒惰”。它无法凭一己之力产生好的重建结果。为了提高性能并降低损失，它*被迫*去关注编码器在潜码 $z$ 中提供的信息。这就提供了一个必要的学习信号，引导[编码器](@article_id:352366)学习一个有用的表示，从而避免坍塌。这是一个绝佳的例子，说明了初始化不仅可以用于保持稳定性，还可以主动引导学习过程走向一个更有意义的解决方案。

“良好起点”这一思想在科学计算领域找到了更深刻的表达，例如在物理信息神经网络 (PINN) 中。这些网络不仅基于数据进行训练，还基于物理定律本身，这些定律以[偏微分方程](@article_id:301773) (PDE) 的形式表达。想象一下，你已经训练了一个 PINN 来求解粘度为 $\nu_1$ 时物体周围的[流体流动](@article_id:379727)。现在，你需要为略有不同的粘度 $\nu_2$ 求解同样的问题。你会从头开始，用一组随机的权重吗？

当然不会。由于 $\nu_2$ 接近 $\nu_1$，新的解也将接近旧的解。你已经训练好的网络是解决新问题的一个绝佳起点。将旧的、已训练网络的权重用作新训练任务的*初始化*，是一种[迁移学习](@article_id:357432)。这种“热启动”将新网络置于广阔参数空间中非常接近其目标解的位置，使其能够比随机初始化的网络收敛得快得多 [@problem_id:2126311]。在这里，初始化超越了一个单纯的统计技巧，成为在相关问题之间传递知识的一种机制。

### 普适原理：对称性的危害

也许，关于初始化重要性的最令人惊叹的例证，来自于一个将神经网络的人工世界与[量子化学](@article_id:300637)基本定律联系起来的类比。

在训练[神经网络](@article_id:305336)时，有一个不可饶恕的原罪：绝不要将一个层中的所有[权重初始化](@article_id:641245)为相同的值（例如，全为零）。为什么？因为如果所有[神经元](@article_id:324093)开始时都完全相同，它们就是完美对称的。它们接收相同的输入，因此会计算出相同的输出。在反向传播期间，它们将接收到相同的梯度并经历完全相同的更新。它们将永远保持一致，每一个都是其他[神经元](@article_id:324093)的完美克隆。网络的容量被严重削弱；就好像你只有一个[神经元](@article_id:324093)一样。解决方案简单却至关重要：打破对称性。通过用微小的*随机*值来初始化权重，我们赋予每个[神经元](@article_id:324093)一个独特的身份，让它们能够学习不同的特征，从而释放网络的真正力量。

现在，考虑一个看似无关的[计算化学](@article_id:303474)问题：计算[氧分子](@article_id:371446) $\text{O}_2$ 的[电子结构](@article_id:305583)。这是通过一个称为[自洽场](@article_id:297003) (SCF) 方法的迭代过程完成的。开始时，必须为电子密度提供一个初始猜测。一个自然但危险地天真的猜测是采用一个完美对称的初始值，以反映分子的几何对称性。如果你这样做，这个迭代计算过程，就像[神经网络](@article_id:305336)中的更新一样，将会保持这种对称性。该过程会收敛，但会收敛到一个并非真正[基态](@article_id:312876)的驻点。它找到了一个物理上不正确的高能量解。

解决方案？完全相同的原理适用。[计算化学](@article_id:303474)家必须故意*打破*初始猜测的对称性，例如通过轻微扰动[简并轨道](@article_id:314735)的占据数。这个小小的推动使得迭代过程能够逃离对称子空间的陷阱，并找到通往分子真实的、能量更低的[基态](@article_id:312876)的路径 [@problem_id:2453655]。

这难道不非凡吗？同一个深刻的原理——即从一个完美对称的状态优化一个复杂系统会使其陷入次优的境地，而需要一个微小的、随机的、打破对称性的推动才能找到真正的解——同时支配着人工大脑的训练和真实世界分子的计算。这表明，[权重初始化](@article_id:641245)不仅仅是一个工程技巧，它是一个深刻而优美的概念的实际应用，这个概念在整个科学领域中回响，提醒我们其定律的基本统一性。