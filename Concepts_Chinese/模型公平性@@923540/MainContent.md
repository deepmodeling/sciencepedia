## 引言
随着人工智能在医学和金融等高风险决策领域变得不可或缺，确保这些系统公平运行已成为我们这个时代最关键的挑战之一。构建一个无偏见算法这个看似简单的目标，很快就揭示出一个充满数学悖论和深刻伦理问题的复杂景象。算法通常基于反映历史和社会不平等的数据进行训练，即使开发者怀有最好的意图，也可能无意中延续甚至放大现有偏见，导致歧视性结果。本文旨在弥合追求公平的愿望与实现公平的技术及伦理现实之间的知识鸿沟。

为了驾驭这一复杂领域，本文首先在“原则与机制”一章中解析[算法公平性](@entry_id:143652)的核心思想。我们将探讨直觉公平性的瓦解，深入研究相互竞争的群体公平性定义，并直面一个惊人的数学不可能定理，该定理迫使我们在同样可取的属性之间做出权衡。在这一理论基础之上，“应用与跨学科联系”一章将展示这些抽象概念如何产生深远的现实世界影响。我们将考察它们在临床医学、基因组学和在线平台中的应用，并审视追求公平如何与法律、政策乃至科学研究的诚信本身相互交织。

## 原则与机制

一个算法要怎样才算“公平”？这个问题看似简单，但正如所有真正有趣的问题一样，它一旦被打开，便会揭示一个充满惊人复杂性、数学悖论和深刻伦理困境的世界。要驾驭这个世界，我们需要像物理学家一样思考，从第一性原理出发，无论它们引向何方，哪怕是令人不安的结论，都要紧随其后。我们的旅程并非要为“公平”找到一个单一的、万能的答案，而是要理解这个问题的本质。

### 一条规则统御一切？个体公平性的幻象

让我们从我们能想象到的最直观的公平观念开始：相似的人应被相似地对待。如果两个人在所有相关方面都完全相同，他们就应该得到相同的结果。这通常被称为**个体公平性**。它是我们正义感的一块基石。

现在，想象一下医院的急诊室，那里正在使用一个新的AI系统来预测患者病情突然恶化的风险，以帮助医生优先决定谁能获得稀缺的重症监护室（ICU）床位[@problem_id:4849766] [@problem_id:5186037]。该模型被输入了大量临床相关特征——生命体征、实验室结果、年龄等等。我们将这组数据集合称为一个向量$X$。

想象一下，患者A和患者B同时到达。非常巧合的是，他们的临床数据完全相同。每一个实验室数值，每一个生命体征——都一样。我们有$X_A = X_B$。如果我们那条“相似的人应被相似地对待”的规则有任何意义的话，那么这两位患者肯定应该被赋予相同的风险评分。

但是，该算法为了追求准确性，还使用了其他一些变量进行训练，例如患者的保险类型和其居住邮政编码地区的[中位数](@entry_id:264877)收入。医院没有告诉模型像种族这样的敏感属性。这是一种常见的做法，称为**“通过无知实现公平”**——即相信如果你不告诉模型关于种族的信息，它就不可能带有种族歧视。

这种信念是危险而天真的。在许多社会中，历史上的居住隔离和经济不平等模式意味着邮政编码和保险类型等变量与种族高度相关。它们充当了**代理变量**。算法看不到种族，但它看到了种族在数据上留下的模式。

因此，患者A拥有公共保险，居住在低收入社区，他得到的风险评分与患者B不同，后者拥有私人保险，居住在更富裕的地区，尽管他们的临床特征$X_A$和$X_B$完全相同[@problem_id:4849766]。个体公平性在第一步就被违反了。那条简单而优雅的“相似个体相似对待”的规则，立即被现实世界中混乱、相关的本质所复杂化。我们的模型通过包含这些代理变量，学会了根据反映受保护群体身份的社会结构性因素，来区别对待临床上相同的个体。

### 转移视线：群体的公平性

个体公平性的瓦解迫使我们放眼全局。也许我们应该从一个不同的角度来看待这个问题。与其关注个体，不如关注群体？我们可以问：无论模型在做什么，它的结果和误差是否在不同的人口群体（例如，种族群体、性别）之间*公平地*分布？这就是**群体公平性**的领域，也正是在这里，我们那个关于“什么是公平？”的简单问题，碎裂成了十几个相互竞争的定义。

让我们用我们的医院场景来探讨三个最著名的群体公平性思想。

#### 思想1：[人口均等](@entry_id:635293)

最简单的想法是要求模型的预测在各群体间保持平衡。如果我们正在决定谁能进入ICU，**[人口均等](@entry_id:635293)**（或**统计均等**）要求每个群体被接纳的患者比例是相同的[@problem_id:4981026]。形式上，如果$\hat{Y}=1$表示“被接纳”，$A$是群体属性（例如，种族），这意味着：

$P(\hat{Y}=1 \mid A = \text{group 1}) = P(\hat{Y}=1 \mid A = \text{group 2})$

这具有一种吸引人的简单性。它似乎承诺了平等的结果。但稍加思索就会发现一个严重的缺陷。如果潜在的医疗状况在一个群体中比另一个群体更普遍怎么办？假设群体1的真实疾病患病率为40%，而群体2的患病率为25%[@problem_id:4849766]。强制要求接纳率相等将意味着我们要么拒绝群体1中真正生病的人，要么接纳群体2中健康的人。这似乎一点也不公平；事实上，这似乎违反了治疗最需要帮助者和避免不必要治疗的基本医学原则[@problem_id:4961940]。[人口均等](@entry_id:635293)常常将平等*结果*误认为平等*机会*，这样做可能会造成实际伤害。

#### 思想2：[机会均等](@entry_id:637428)与几率均等

好吧，看来忽略事实真相是个坏主意。让我们构建一个更聪明的公平性定义，将事实真相考虑在内。让$Y=1$代表一个真正高风险且需要ICU的患者。

一个更合理的要求是**[机会均等](@entry_id:637428)**。它指出，在所有真正高风险（$Y=1$）的人中，每个人都应该有相同的机会被模型正确识别，无论他们属于哪个群体[@problem_id:5206108]。这是为了确保所有群体的**真正率（TPR）**是相同的：

$P(\hat{Y}=1 \mid Y=1, A = \text{group 1}) = P(\hat{Y}=1 \mid Y=1, A = \text{group 2})$

这是一个强有力的思想。它表明系统应该在每个社区中同等地擅长识别需求。

但错误呢？我们可能犯另一种错误：将低风险的人标记为高风险（[假阳性](@entry_id:635878)）。我们可能也要求这种错误的发生率，即**假正率（FPR）**，在各群体间是相等的。一个既满足相等TPR又满足相等FPR的系统，被称为满足**几率均等**[@problem_id:4524831]。形式上：

$P(\hat{Y}=1 \mid Y=y, A = \text{group 1}) = P(\hat{Y}=1 \mid Y=y, A = \text{group 2})$ 对$y=1$（真实例）和$y=0$（假实例）都成立。

这感觉像是一个稳健、精英主义的理想。我们不是在强制整体结果相等，而是在要求模型的诊断准确性对所有群体都相同。我们似乎找到了一个坚实、可辩护的公平性定义。

### 令人不安的不可能性

就在我们以为已经站稳脚跟时，数学的根基却在我们脚下崩塌了。我们可能对我们的风险评分模型提出另一个完全合理的要求。如果模型给一个患者的风险评分为，比如说，$s = 0.7$，我们希望这意味着该患者有70%的几率发生不良事件，无论其属于哪个群体。这种一个分数$s$对每个人都意味着同样事情的属性，被称为**校准**[@problem_id:4961940] [@problem_id:4389119]。形式上：

$P(Y=1 \mid \text{score}=s, A=g) = s$ 对每个群体$g$都成立。

校准是一个可信赖分数的基础。医生不能使用一个对一个群体意味着一回事，而对另一个群体意味着另一回事的分数。

所以现在我们有两个极好的、看似不容商榷的属性：几率均等和校准。我们两者都想要。但计算机科学中一系列引人注目的“不可能定理”已经证明了一个惊人的事实：你不可能同时拥有两者。

**如果不同群体之间病症的潜在流行率不同，一个非完美的预测模型在数学上不可能同时满足几率均等和校准**[@problem_id:4981026] [@problem_id:4961940]。

这不是一个观点问题，也不是找到一个更聪明算法的问题。这是概率数学中固有的一个基本冲突。如果群体的基础患病率不同，一个经过校准的模型（分数可信）必须在不同群体间具有不同的假正率。一个具有相等假正率的模型（几率均等）必须至少对一个群体是未校准的。其证明是贝叶斯定理的一个漂亮应用，但直观的解释是，不同的基础率迫使分数分布的形状不同，而你无法同时以两种相互矛盾的方式将它们对齐。

我们面临着一个不可能的选择。我们应该构建一个模型，其中70%的风险评分对每个人都意味着70%，但该模型对一个群体的[假阳性](@entry_id:635878)错误比另一个群体多？还是我们应该构建一个对所有群体都有相同错误率的模型，但其中一个群体的70%风险评分可能意味着60%的真实风险，而对另一个群体则意味着80%？

### [超越数](@entry_id:154911)学：回归伦理

这个不可能定理是我们理解过程中的关键转折点。它告诉我们，不存在“完美公平”的技术解决方案。校准和几率均等之间的选择不是一个技术选择，而是一个伦理选择。它迫使我们展开对话，讨论在特定情况下我们最看重哪种公平。这就是**统计公平性**（我们一直在探索的数学定义集合）和**规范公平性**（在这些定义中进行选择所需的价值判断和伦理推理）之间的区别[@problem_id:4875745]。

在这里，我们必须回归到更广泛的伦理原则，比如贝尔蒙报告（Belmont Report）为人类研究概述的那些原则：正义、善行和自主[@problem_id:4439498]。像几率均等这样的统计指标并非“正义”。它是一个简单的数学代理，在某些情况下，可能帮助我们朝着更丰富、更规范的利益与负担公平分配的目标努力。真正的正义要求我们权衡不同类型错误的危害，考虑历史背景，并追问谁因模型的部署而承受负担，谁又从中受益[@problem_id:5186037]。

### 迷宫深处：交叉公平性

就在情况看起来已经复杂到极点时，我们又发现了一扇门。到目前为止，我们一直在讨论由单一属性（如种族或性别）定义的群体之间的公平性。但人们并非生活在单一属性的盒子里。他们生活在多种身份的交叉点上。

考虑一个用于分析医学图像的模型，它在来自不同医院扫描仪的数据上进行训练[@problem_id:4530599]。我们审计该模型，发现它在男性与女性之间满足几率均等。很好。我们还针对扫描仪A与扫描仪B进行审计，它在那里也满足几率均等。看起来很公平。

但是，当我们审视交叉点时会发生什么呢？我们可能会发现一些令人震惊的事情。该模型可能对在扫描仪B上成像的女性极不公平，而对所有其他组合都公平。这是现实世界版的辛普森悖论（Simpson's Paradox）：在不同数据组中出现的趋势，在这些组合并时消失或逆转。沿单个轴线的公平并不能保证其交叉点的公平。为了发现这些隐藏的偏见，我们必须积极寻找它们，审查越来越精细的子群体。

我们理解公平原则与机制的旅程，从一条简单、直观的规则，走向了一个充满竞争性定义、不可避免的数学权衡和棘手伦理选择的领域。我们学到，公平不是一个可以从清单上勾选掉的简单属性。它是一个质疑、衡量和辩论相互竞争的价值观的动态过程。模型公平性的真正原则是，不存在最终的原则——只有深刻的责任去理解我们构建的工具、它们包含的悖论，以及它们最终将塑造的社会。

