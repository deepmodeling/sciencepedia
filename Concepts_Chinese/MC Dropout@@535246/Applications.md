## 应用与跨学科联系

我们已经花了一些时间探讨了 [Dropout](@article_id:640908)——一个看似为防止过拟合而设计的临时技巧——与深刻的[贝叶斯推断](@article_id:307374)原理之间优美的数学联系。我们看到，通过在预测期间保持 [Dropout](@article_id:640908) 激活，我们可以引导我们的模型不仅揭示它们*想*什么，还揭示它们有多*自信*。这是一项了不起的成就。它将一个[深度神经网络](@article_id:640465)从一个分发答案的“黑箱”先知，转变为一个更深思熟虑、更细致入微的发现伙伴。它赋予了模型说“我不确定”的能力。

但这仅仅是数学上的好奇心吗？是统计学家的一个巧妙的派对戏法吗？远非如此。这个想法的真正力量，就像物理学中任何伟大的原理一样，体现在其应用之中。在本章中，我们将踏上一段旅程，看看蒙特卡洛（MC）[Dropout](@article_id:640908) 如何重塑从[计算机视觉](@article_id:298749)到医学的各个领域，以及它如何为新一代人工智能铺平道路——这代人工智能不仅能力更强，而且更可靠、更值得信赖、更负责任。

### 磨利智能的工具

在我们涉足其他科学领域之前，让我们先看看量化不确定性的能力如何使人工智能本身在其核心任务上做得更好。赋予模型一种怀疑感，使其能够以更高的复杂性去观察、解释和创造。

想象一下[自动驾驶](@article_id:334498)汽车中的一个[目标检测](@article_id:641122)系统。它可能会围绕一个行人画出几十个略有不同的[边界框](@article_id:639578)。经典方法，[非极大值抑制](@article_id:640382)（NMS），只会选择“[置信度](@article_id:361655)分数”最高的那个框。但如果那个高分框是一个变幻莫测、不确定预测的结果，而另一个得分稍低的框来自一个非常稳定、确定的预测呢？一个具备不确定性意识的系统可以做出更明智的选择。通过使用 [MC Dropout](@article_id:639220)，我们可以估计每个提议框的稳定性（即*认知不确定性*）。然后我们可以修改 NMS，使其偏爱那些不仅平均[置信度](@article_id:361655)高，而且在模型的多个随机视角下都稳定可靠的预测。这与[决策论](@article_id:329686)中一个更深层次的原则是一致的：我们希望选择能最大化我们[期望](@article_id:311378)成功的行动，而高不确定性天生就会降低这种[期望](@article_id:311378) [@problem_id:3146116]。

这个原则从观察世界延伸到了创造世界。考虑一下[生成对抗网络](@article_id:638564)（GANs），这些模型以创造逼真的人脸和其他图像而闻名。当一个 GAN 转换一张图像时——比如，把夏日风景变成冬日雪景——它究竟发挥了多大的“创作自由”？某片被雪覆盖的地面是一个自信的转换，还是模型只是在猜测？通过使用 [MC Dropout](@article_id:639220) 运行生成器，我们可以为同一个夏日输入生成一整套可能的冬日场景。生成的像素在不同[前向传播](@article_id:372045)中的方差，为我们提供了一个直接、可量化的模型“想象力”不确定性的度量。我们甚至可以利用这些信息来改进训练过程本身，通过创建一个[损失函数](@article_id:638865)来惩罚模型在它本应自信的区域表现出不确定性 [@problem_id:3127723]。

即使在语言世界里，不确定性也扮演着至关重要的角色。当一个模型生成一个句子时，它会做出一系列选择，一次一个词。一种简单的“贪婪”方法只是在每一步选择最可能的下一个词。但如果模型在两个词之间几乎同样纠结呢？一个 [Dropout](@article_id:640908) 掩码可能偏爱其中一个，而另一个掩码则偏爱另一个。[MC Dropout](@article_id:639220) 让我们能看到这种分歧。这为更复杂的解码策略打开了大门，这些策略不仅仅是选择一条单一的、可能脆弱的路径，而是在每一步都整合模型的不确定性，以生成更稳健、更连贯的文本 [@problem_id:3132521]。

### 搭建通往科学的桥梁

也许[不确定性量化](@article_id:299045)最令人兴奋的应用不在于人工智能本身，而在于它如何赋能其他科学学科。在许多领域，一个错误的答案远比没有答案更危险。模型能够说“我不知道”不是一种失败，而是一项关键的安全特性。

这一点在医学领域体现得尤为真切。假设我们训练了一个[图神经网络](@article_id:297304)来预测一个新分子是否对肝脏有毒性——这是药物发现中的关键一步。该模型将分子的结构作为图输入，并输出毒性概率。一个简单的“是”或“否”是不够的。制药公司需要知道：你有多确定？通过使用 [MC Dropout](@article_id:639220) 执行几次[前向传播](@article_id:372045)，我们可以得到一个毒性概率的分布。这个分布的方差为我们提供了模型[认知不确定性](@article_id:310285)的度量。高方差是一个警示信号，表明模型的预测，无论结果如何，都不可信，需要进一步研究 [@problem_id:1436718]。

我们可以通过分解模型的总不确定性，将这个想法推得更远。回想一下认知不确定性（模型的无知）和[偶然不确定性](@article_id:314423)（数据中的[固有噪声](@article_id:324909)）之间的区别。这种分解是一个强大的诊断工具。想象一个[深度学习](@article_id:302462)模型，旨在从医学图像中检测一种罕见疾病。将患者的扫描结果输入模型，我们使用 [MC Dropout](@article_id:639220) 获得多个预测。
- 如果**[认知不确定性](@article_id:310285)**很高，这意味着模型感到困惑，因为它没有见过足够多像这样的例子。这对于罕见疾病或代表性不足的患者群体很常见。正确的做法不是相信机器的输出，而是“上报给专家审查”[@problem_id:3197096]。模型知道自己不知道什么。
- 如果**[偶然不确定性](@article_id:314423)**很高，这意味着单个预测本身就不确定，即使它们都达成了一致。这表明输入数据本身是问题所在——也许图像模糊或有噪声。正确的做法是“要求重新扫描”以获得更好的数据。

这种智能分诊系统——由不确定性指导临床工作流程——是从一个简单的分类器到负责任的人工智能伙伴的[范式](@article_id:329204)转变。

指导我们在医学领域的相同原则也可以加速其他领域的发现，如[材料科学](@article_id:312640)和合成生物学。当使用像 [U-Net](@article_id:640191) 这样的分割模型分析材料的[微观结构](@article_id:309020)时，我们不仅想要找到晶粒之间的边界；我们还想要一张关于这些边界不确定性的地图。使用 [MC Dropout](@article_id:639220)，我们可以优美地分解边界预测位置的总方差。偶然部分告诉我们图像的哪些区域本身就模棱两可或充满噪声，而认知部分则告诉我们模型自身在哪些地方缺乏知识。总不确定性，优雅地，是这两个方差之和，这是全方差定律的直接结果 [@problem_id:38596]。在合成生物学中，合成和测试一个新的 DNA 序列成本高昂，我们可以使用一种“[主动学习](@article_id:318217)”策略。通过训练一个模型委员会（一个与 [MC Dropout](@article_id:639220) 密切相关的概念），我们可以找到模型们分歧最大的未标记 DNA 序列。这些是接下来最值得测试、信息量最大的序列，使我们能够尽可能高效地了解系统，并将我们宝贵的实验资源引导到最能产生影响的地方 [@problem-id:2749040]。

### 铺就通往负责任与合乎伦理的人工智能之路

[不确定性量化](@article_id:299045)的最终承诺在于构建我们可以在现实世界中信赖的人工智能系统。这需要的不仅仅是好的[算法](@article_id:331821)；它需要对负责任和合乎伦理的部署做出深刻的承诺。

其核心是**拒绝预测**（abstention）原则：模型应该知道何时拒绝做出预测。我们在医学背景中看到的[不确定性分解](@article_id:362623)可以被推广。对于任何关键任务，特别是训练数据稀缺的任务（如在“小样本学习”中），我们可以建立一个双层系统。首先，我们识别出具有高[认知不确定性](@article_id:310285)的例子——即模型超出其能力范围的情况——并将它们留给人类专家处理。然后，从剩下的案例中，我们可以标记那些具有高[偶然不确定性](@article_id:314423)的案例，认为它们本质上是模棱两可的 [@problem_id:3125763]。这种方法也迫使我们重新思考我们如何评估模型。我们可以设计一个“不确定性感知的 F1 分数”，而不是单一的 F1 分数，该分数基于模型[预测分布](@article_id:345070)上的*[期望](@article_id:311378)*性能。我们甚至可以构建策略，有策略地对不确定的阳性样本拒绝预测，以专门提升模型在它*确实*做出的预测上的精确度 [@problem_id:3105703]。

这个框架成为**审计公平性**的强大工具。如果一个在人口统计数据上训练的模型对某个群体表现出系统性更高的认知不确定性，这是一个清晰、可量化的信号，表明这个群体在训练数据中[代表性](@article_id:383209)不足。模型实际上在告诉我们：“我对这个群体的预测不太确定，因为我见过的来自他们的数据较少。”这为检测和解决我们人工智能系统中潜在的偏见提供了一种严谨、有原则的方法 [@problem_id:3197036]。

所有这些线索最终汇集于为高风险社会问题部署人工智能的宏大挑战中，比如为沿海社区预测风暴潮。仅仅给出一个数字——预测的浪涌高度——在科学上是幼稚的，在伦理上是不负责任的。一种负责任的方法，植根于[决策论](@article_id:329686)，要求对不确定性进行全面核算。它要求量化天气的内在随机性（[偶然不确定性](@article_id:314423)）和我们模型的局限性（认知不确定性）。它要求凭经验校准我们模型的概率预测，以确保它们是可靠的。并且它要求将这种不确定性透明地传达给利益相关者——不是作为一个单一的、可怕的最坏情况数字，而是通过可操作的信息，如[预测区间](@article_id:640082)和超过关键洪水位的概率。这个从严谨建模到合乎伦理的沟通的完整的、端到端的系统，代表了我们所讨论思想的最终应用。这就是我们如何从仅仅做出预测，转向支持明智和人道的决策 [@problem_id:3117035]。

从一个简单的[正则化技术](@article_id:325104)到一个负责任人工智能的基石的旅程，证明了深刻原理的统一力量。通过拥抱不确定性，我们不是在让我们的模型变弱；我们是在让它们变得无限更智能、更有用，也更值得我们信赖。