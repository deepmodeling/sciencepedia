## 引言
对称性是支配我们宇宙法则和我们所感知模式的基本概念。但是，我们如何能教会机器理解这种内在结构呢？答案在于[等变性](@article_id:640964)（equivariance），这是一个强大的数学原理，已成为现代人工智能和[科学计算](@article_id:304417)的基石。它解决了传统机器学习中的一个关键缺陷：我们无需强迫模型从海量数据中费力地学习旋转或平移等基本对称性，而是可以将这些知识直接构建到其架构中。本文将对[等变性](@article_id:640964)进行全面探讨，引导您从其核心理论走向其革命性应用。

第一章“原理与机制”将解构[等变性](@article_id:640964)的概念，解释其数学定义，以及如何通过[权重共享](@article_id:638181)、[群卷积](@article_id:639745)和专门的[坐标系](@article_id:316753)等技术将其构建到神经网络中。我们将探讨那些让模型能够内在地尊重其数据几何结构的精妙机制。随后的“应用与跨学科联系”一章将揭示这一强大思想在哪些领域产生影响。我们将从其在物理学和[微分几何](@article_id:306240)中的起源，一路追溯到其在计算机视觉、[分子建模](@article_id:351385)和[无监督学习](@article_id:320970)中的现代应用，展示[等变性](@article_id:640964)如何提供一个统一的视角，用于构建更高效、更鲁棒、更符合物理现实的智能系统。

## 原理与机制

### 对称性原理：与世界的可交换性

想象一下你在编辑一张照片。你决定应用一个“锐化”滤镜。是先将图像旋转 $90^\circ$ 再锐化，还是先锐化再旋转，这两者有区别吗？直觉上，你会[期望](@article_id:311378)最终结果相同，只是朝向不同。如果“锐化”操作以这种方式工作，我们就说它对旋转是**等变的**（equivariant）。它与旋转“交换”（commutes）；操作的顺序不改变本质结果。

这个简单的想法是现代科学和机器学习中最强大的概念之一的核心。形式上，一个函数或系统（我们称之为 $f$）如果对于一组变换 $G$（如旋转或平移）是等变的，那么先变换输入再应用函数，与先应用函数再变换输出，将得到相同的结果。用数学语言表述为：

$$
f(g \cdot x) = g \cdot f(x)
$$

在这里，$g$ 是我们群组中的一个变换（例如，一次特定的旋转），$x$ 是输入（我们的图像），$g \cdot x$ 是变换后的输入，而 $g \cdot f(x)$ 是相应变换后的输出。这个方程表达了一种美妙的和谐：函数 $f$ 尊重变换 $g$ 的结构。

必须将此概念与一个相关概念区分开来：**不变性**（invariance）。一个不变的函数是指当输入被变换时，其输出*完全不发生改变*的函数：

$$
f(g \cdot x) = f(x)
$$

不变的函数对变换是“视而不见”的。而等变的函数则能“看到”变换，并在其输出中反映出来。

考虑在图像中识别一只猫的任务。一个好的猫*分类器*应该对旋转是**不变的**。无论猫是正立、侧卧还是倒置，标签“猫”都保持不变。[期望](@article_id:311378)的输出是稳定的。但如果我们的任务是*分割*图像，即创建一个勾勒出猫尾巴的掩码呢？这个系统就应该是**等变的**。如果我们旋转猫的图像，我们[期望](@article_id:311378)尾巴的输出掩码也随之旋转。[期望](@article_id:311378)的输出随输入一同变换。正如我们将看到的，当任务要求不变性时，鼓励系统具有[等变性](@article_id:640964)可能会产生一种[张力](@article_id:357470)，这种冲突可能导致学习过程中的一场拉锯战。

### [等变性](@article_id:640964)的蓝图：共享的力量

我们究竟如何才能构建一个拥有这种非凡属性的系统，比如神经网络呢？秘诀出奇地简单：**[权重共享](@article_id:638181)**（weight sharing）。

最著名的例子是**[卷积神经网络](@article_id:357845)（CNN）**，它是现代计算机视觉的主力。CNN 被设计为对平移等变。如果你有一张鸟的图片，并将其向右移动，网络内部关于“鸟性”的表示也仅仅是向右移动。

这种“魔力”源于卷积操作本身。卷积通过将一个小的、可学习的滤波器（称为**核**，kernel）在整个图像上滑动来工作。在每个位置，它都会计算一个响应。关键在于，它在每一个位置都使用*完全相同的核*。这就是[权重共享](@article_id:638181)。通过在所有空间位置上共享权重，网络在架构上被强制在任何地方寻找相同的模式。如果它学习到一个能检测垂直边缘的核，那么无论这个垂直边缘出现在左上角还是右下角，这个核都会被激活。

要理解为什么这是[等变性](@article_id:640964)的来源，可以考虑一个替代方案：**局部连接层**（locally connected layer）。在这种层中，图像中的每个位置都有自己独特的滤波器，没有[权重共享](@article_id:638181)。这样的网络不具有[平移等变性](@article_id:640635)。一个学会在图像中心识别鸟眼的滤波器，当鸟眼移动到角落时，它完全不知道鸟眼是什么样子，必须从头开始学习。[权重共享](@article_id:638181)不仅仅是为了减少参数数量的效率技巧；它是一种根本性的架构选择，将[平移对称性](@article_id:350762)直接硬编码到网络中。

### 泛化对称性：[群卷积](@article_id:639745)巡览

平移只是一种对称性。那么其他对称性，比如旋转，又该如何处理呢？我们可以将[权重共享](@article_id:638181)这一优雅思想扩展到处理更一般的变换群。这就引出了**[群卷积](@article_id:639745)**（Group Convolutions）的概念。

假设我们想要一个对 $90^\circ$ 旋转等变的网络——这是循环群 $C_4$ 的一部分。一个标准的 CNN 核如果学会了检测水平边缘，当这个边缘被旋转成垂直时，它就无法检测出来。我们如何解决这个问题？

我们不再只使用一个在不同位置共享的核，而是从一个**基础核**（base kernel）开始。然后，我们通过对这个基础核应用群中的所有变换，生成一整族的核。在我们的 $C_4$ 例子中，我们会取基础核，并创建另外三个分别旋转了 $90^\circ$、$180^\circ$ 和 $270^\circ$ 的副本。我们的“滤波器”现在就是这四个旋转核的完整集合。

当我们执行[群卷积](@article_id:639745)时，我们将这一整族旋转核在图像上滑动。输出不再是单个二维特征图，而是一个由四个[特征图](@article_id:642011)组成的堆栈，每个方向一个。第一个图可能显示水平模式的位置，第二个图显示旋转 $90^\circ$ 后的模式的位置，依此类推。

这种构造的美妙之处在于，当你旋转输入图像时会发生什么。如果你将输入旋转 $90^\circ$，输出中的特征不仅仅是在空间上移动。这些值本身会在四个方向通道之间以一种完全可预测的方式进行[排列](@article_id:296886)。原先在“水平”通道中的响应现在出现在了“$90^\circ$”通道中。这就是通过构造强制实现的旋转[等变性](@article_id:640964)。如果我们使用四个独立、不相关的核，而不是一个核的旋转副本，这个宏伟的属性将荡然无存。这个原理可以推广到更复杂的群，比如连续[旋转群](@article_id:383013) $SO(2)$，通过设计本身对旋转等变的[核形状](@article_id:318638)来实现。

### 精妙之舞：如何保持[等变性](@article_id:640964)

构建一个等变的卷积层是伟大的第一步，但这种对称性很容易被后续的操作破坏。保持[等变性](@article_id:640964)需要一种审慎的设计哲学，即系统中的每个组件都必须尊重底层的对称性。

一个常见的陷阱是**池化**（pooling）。一个标准的**[最大池化](@article_id:640417)**（max-pooling）层，它可能会取一个小空间区域内的最大值，但它对于一般的群作用不是等变的。考虑我们那个具有四个方向通道的旋转[等变网络](@article_id:304312)。如果我们简单地在每个空间点上取*所有方向通道*的最大值，我们就把所有方向信息压缩成了一个单一的数字。我们丢弃了我们费尽心力创造的结构。输出便不再是等变的。解决方案是设计一个等变的池化算子。例如，我们可以在空间上进行[最大池化](@article_id:640417)，但在每个方向通道*内部*独立进行。这种“逐纤维”（fiber-wise）的方法尊重了群结构，并保持了[等变性](@article_id:640964)。

另一个危险潜伏在像**dropout**这样的[正则化技术](@article_id:325104)中。标准的 dropout 会随机地将单个[神经元](@article_id:324093)的激活值设为零。如果应用到我们的方向通道上，它会随机地在我们精心构建的表示上打洞，破坏对称性。这种[随机过程](@article_id:333307)的单次实现几乎肯定会摧毁[等变性](@article_id:640964)。解决方法同样是尊重结构。我们可以使用**同步 dropout**（synchronized dropout），即我们做出一个单一的随机决定，在某个位置上要么保留要么丢弃*整套*方向通道。

也许最微妙的挑战来自于**步进**（striding）或空间**[下采样](@article_id:329461)**（downsampling）。当我们对一个信号进行下采样时，高频分量可能会“折叠”并伪装成低频分量，这种现象称为**[混叠](@article_id:367748)**（aliasing）。在一个群[等变网络](@article_id:304312)中，每个方向的特征图都有不同的[频谱](@article_id:340514)特征——它们是彼此的旋转版本。当我们进行[下采样](@article_id:329461)时，产生的混叠失真取决于[频谱](@article_id:340514)的方向。这意味着每个通道会以不同的方式被破坏，从而打破它们之间的旋转关系。解决方案是群论与信号处理的美妙结合：在下采样之前，我们应用一个[低通滤波器](@article_id:305624)来移除有问题的**高频**。至关重要的是，这个滤波器本身必须是[旋转对称](@article_id:297528)的（各向同性的），这样滤波操作才不会破坏它试图拯救的对称性。

### 驯服尺度巨兽：视角之变

到目前为止，我们讨论了离散群（如 $C_4$）和紧凑连续群（如 $SO(2)$）。但是对于非[紧群](@article_id:306707)，比如[缩放变换](@article_id:345729)群，又该怎么办呢？构建一个对物体尺寸变化等变的网是一个艰巨的挑战。像使用[扩张卷积](@article_id:640660)（dilated convolutions）这样的简单方法并不完全奏效；它们缺乏真正与缩放操作交换所需的丰富结构。

解决方案堪称神来之笔，让人联想到物理学中的伟大变换。我们不试图在我们当前的[参考系](@article_id:345789)中解决这个难题，而是改变我们的坐标。如果我们不把图像表示在标准的笛卡尔 $(x,y)$ 网格上，而是将其[重采样](@article_id:303023)到一个**对数-[极坐标](@article_id:319829)网格**（log-polar grid）上，会怎么样？

在这个新世界里，一个点由其对数半径 $u = \ln(r)$ 和角度 $\theta$ 来描述。现在看看会发生什么。如果我们取原始图像并将其缩放一个因子 $s$，一个在半径 $r$ 处的点会移动到 $s \cdot r$。在对数-极坐标世界里，这对应于其对数半径从 $\ln(r)$ 变为 $\ln(s \cdot r) = \ln(s) + \ln(r)$。缩放变成了一个沿对数半径轴的简单**平移**！类似地，原始图像中的旋转只是沿角度轴的平移。

突然之间，棘手的尺度-旋转[等变性](@article_id:640964)问题被转化为了我们熟悉的[平移等变性](@article_id:640635)问题。我们现在可以在这个新的对数-极坐标表示上应用我们标准的、平移等变的卷积机制，从而在原始域中实现对缩放和旋转的[等变性](@article_id:640964)。这种深刻的视角转换揭示了不同[几何变换](@article_id:311067)之间深层的统一性，将一个看似棘手的问题变成了一个我们已经知道如何解决的问题。

### 通往[等变性](@article_id:640964)的两条道路：硬性植入与温和引导

我们已经看到了如何构建将对称性融入其核心的系统。这种方法被称为施加**硬性约束**（hard constraint）或**架构[归纳偏置](@article_id:297870)**（architectural inductive bias）。

通过设计像[群卷积](@article_id:639745)网络这样的架构，我们将网络可以学习的所有可能函数的宇宙，限制在一个更小的、保证是等变的函数子集内。如果我们的数据确实拥有这种对称性（例如，物理定律不因方向而改变），这是一个极其强大的先验知识。模型不需要浪费时间和数据来学习对称性；它从诞生之初就拥有了这种知识。这可以极大地减少学习一个好解所需的训练数据量。

然而，还有另一种方式。我们可以使用一个更灵活、通用的架构，然后仅仅“鼓励”它变得等变。这是一种**软性约束**（soft constraint），通常实现为模型[损失函数](@article_id:638865)中的一个惩罚项。我们可以定义一个损失 $\mathcal{L}_{\mathrm{eq}}$，它衡量网络输出与理想等变行为的偏差程度，并将其加到我们的主任务损失中。在训练期间，优化器将同时尝试最小化任务误差和这个[等变性](@article_id:640964)误差。这种方法为对称性提供了一种“温和的引导”，而不是将其作为绝对法则来强制执行。当数据中的对称性只是近似存在时，这种方法可能很有用。

这两种哲学——架构强制和[正则化](@article_id:300216)——代表了智能系统设计中的一个根本选择。我们是应该将关于世界结构的知识直接构建到模型中，还是赋予它们灵活性去自行发现这些结构？对[等变性](@article_id:640964)的研究为我们探索这个问题提供了一个严谨而优美的框架。

