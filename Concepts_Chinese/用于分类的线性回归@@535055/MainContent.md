## 引言
一个为预测连续值（如线性回归）而设计的工具，能否被重新用于分类这样的排序任务？这个问题开启了一段进入机器学习中最具启发性的思想实验之一的旅程。虽然最直接的答案涉及一个简单而优雅的数学技巧，但这种方法充满了根本性的缺陷。本文探讨了使用[线性回归](@article_id:302758)进行分类的悖论性质，不将其视为一种推荐的技术，而是将其视为一面“扭曲的透镜”，通过它揭示现代数据科学深刻而统一的原则。

读者将首先探索该方法的核心“原理与机制”。这包括理解最小二乘分类法的机制、其在异常值和未校准输出方面的关键弱点，以及为什么像[逻辑斯谛回归](@article_id:296840)这样的模型通常更优越。我们还将揭示其在高维范畴下的惊人第二幕，它与[双下降](@article_id:639568)和[隐式正则化](@article_id:366750)等前沿概念相联系。随后，“应用与跨学科联系”一章将拓宽视野，利用该模型的失败之处来说明降维中的基本概念、[特征缩放](@article_id:335413)的重要性，甚至[算法公平性](@article_id:304084)的社会影响。通过将一个简单的工具推向其极限，我们对整个机器学习领域获得了更丰富的理解。

## 原理与机制

### 一个简单、朴素而又绝妙的想法

让我们从一个简单到近乎愚蠢的问题开始：我们能否用一个画线的工具——[线性回归](@article_id:302758)——来解决一个将事物分类到不同盒子里的问题——分类？假设你有一些数据点，分属于两个类别，比如“类别0”和“类别1”。回归旨在预测连续的数值，如温度或价格。分类则是预测一个离散的标签。这两者怎么可能互相替代呢？

最直接的方法就是*假装*类别标签是数字。我们可以为类别0中的每个点赋值$y=0$，为类别1中的每个点赋值$y=1$。现在，我们有了一组点$(x_i, y_i)$，就可以让我们熟悉的朋友——[线性回归](@article_id:302758)——来找到最能拟合这些数据的直线（或在高维空间中的[超平面](@article_id:331746)）。模型形式为$f(x) = w^{\top}x + b$，其中$w$是权重向量，$b$是截距。目标是找到参数$(w,b)$，以最小化预测分数$f(x_i)$与数值标签$y_i$之间的平方差之和。

一旦我们有了这条线，我们如何进行分类呢？一个自然而然的规则出现了：如果模型对一个新点的输出$f(x)$更接近1而不是0，我们就预测为类别1。如果更接近0，我们就预测为类别0。[决策边界](@article_id:306494)，即完全无法抉择的点，将是分数恰好在中间的位置：$w^{\top}x + b = 0.5$。

或者，我们也可以将类别标记为$-1$和$+1$。在这种情况下，自然的决策边界是分数为零的地方：$w^{\top}x + b = 0$。正分意味着我们倾向于类别$+1$，负分则意味着我们倾向于类别$-1$。正如我们将看到的，这种设置具有一些相当优雅的属性。无论哪种情况，决策边界都是一条直线（或一个平面），即一个**[线性分类器](@article_id:641846)**。[@problem_id:3144333]

### 最小二乘法的机制

这种我们可以称之为**最小二乘分类法**的方法，有一个直截了当的数学引擎。任务是最小化总平方误差，这个[目标函数](@article_id:330966)我们可以写成$L(w,b) = \sum_{i=1}^n ((w^{\top}x_i + b) - y_i)^2$。利用微积分的工具，我们可以找到最小化该损失函数的精确参数$(w,b)$。通过将[损失函数](@article_id:638865)的梯度设为零，我们得到了一组著名的线性方程组，称为**正规方程**。

在一个紧凑的矩阵形式中，如果我们在特征矩阵$X$中增加一列全为1的向量（我们称之为$Z$），并将参数堆叠成一个向量$\theta$，那么正规方程就是：
$$
Z^{\top} Z \theta = Z^{\top} y
$$
如果矩阵$Z^{\top} Z$是可逆的，我们可以直接解出$\theta$：
$$
\theta = (Z^{\top} Z)^{-1} Z^{\top} y
$$
这给了我们一个[闭式](@article_id:335040)解析解。不需要迭代搜索；这是一次性的计算。如果$Z^{\top} Z$不可逆（当某些特征是冗余的时可能发生），解仍然存在，并且可以使用**[Moore-Penrose伪逆](@article_id:307670)**找到，这会给出具有最小可能范数的解。[@problem_id:3144333] [@problem_id:3148069]

所以，我们有了一个简单的方法，并且它有一个优雅、精确的解。故事本应到此结束，对吗？简单性的胜利！但大自然，正如她经常做的那样，准备了一些惊喜。当我们仔细观察时，会发现这个简单的想法有一些深刻且富有启发性的缺陷。

### 当善意走向歧途：[异常值](@article_id:351978)的暴政

[最小二乘回归](@article_id:326091)最大的优点——其数学上的简单性——源于它的损失函数：平方误差和。但这也是它最大的弱点。通过对误差进行平方，我们赋予了那些远离回归线的点巨大的权力。一个距离直线两倍远的点会贡献四倍的误差。一个十倍远的点会贡献一百倍的误差。模型变得痴迷于安抚这些遥远而苛刻的点。

现在，在我们的分类情境中想象一下。假设我们有一组表现良好、分离清晰的数据点。我们的最小二乘分类器找到了一个非常合理的边界。然后，一个新点出现了。它离其他数据很远（一个“高杠杆”点），并且由于某种错误，它被赋予了错误的标签。例如，假设我们的初始数据表明一条斜率为$+1$的线。新点位于$x=10$处，所以模型会[期望](@article_id:311378)$y \approx 10$。但如果它被错误地标记为$y=-10$呢？

这一个点的平方误差是巨大的。为了减少这个巨大的惩罚，最小二乘法会做出一些 drastic 的事情：它会倾斜整条线，牺牲在所有其他点上的良好拟合，只为了更接近这一个麻烦的异常值。一个斜率为$+1$的完美分类器，可能会因为一个被错误标记的[高杠杆点](@article_id:346335)，而被猛烈地倾斜到斜率接近$-1$。[@problem_id:3172785]

这种极端的敏感性使得最小二乘分类成为一种脆弱且不可靠的方法。它缺乏**鲁棒性**。这就像一个政治体系，谁喊得最大声，谁就得到所有的关注。其他方法，如逻辑斯谛回归，使用更温和的损失函数，不会对异常值如此恐慌。[鲁棒回归](@article_id:299654)方法甚至使用“下降”损失函数，如Tukey的双权损失，其中一旦点的误差变得过大，其影响实际上会*减小*并最终降至零。模型实质上学会了忽略那些与数据其余部分病态不一致的点。[@problem_d:3169363] [@problem_id:3151640]

### “分数”意味着什么？对校准概率的追求

第二个更微妙的问题出现了。我们的最小二乘分类器的输出$f(x)$只是一个原始分数。我们决定使用$0.5$作为阈值，但一个$0.7$的分数与$0.9$的分数有实质性区别吗？它是否意味着这个点有$70\%$的概率属于类别1？完全不是。这些分数不是**校准的概率**。

这时，**逻辑斯谛回归**作为主角登场。逻辑斯谛回归不是直接将一条线拟合到0/1标签上，而是对属于某个类别的*概率*进行建模。它是一个**[判别模型](@article_id:639993)**；它不对数据$X$的分布做任何假设，这与像**[线性判别分析](@article_id:357574)（LDA）**这样的[生成模型](@article_id:356498)不同，后者假设每个类别中的数据来自高斯分布。[@problem_id:1914082]

逻辑斯谛回归提出，属于类别1的几率的对数是$x$的线性函数：
$$
\ln\left(\frac{P(Y=1|x)}{1-P(Y=1|x)}\right) = w^{\top}x+b
$$
通过解出概率$P(Y=1|x)$，我们得到了著名的S形函数（sigmoid function），$\sigma(w^{\top}x+b)$。这个公式的美妙之处在于其输出总是在0和1之间，并且当通过最大化数据的[似然性](@article_id:323123)进行适当训练时，它产生一个**[概率校准](@article_id:640994)的**模型。这意味着预测概率为$0.8$可以解释为有$80\%$的[置信度](@article_id:361655)认为该点属于类别1。[@problem_id:3151640] 这比[最小二乘法](@article_id:297551)得到的任意分数是一个更有用、更可解释的输出。

当我们考虑如何评估我们的模型时，概率输出的优越性变得更加清晰。像准确率这样的简单指标可能具有危险的误导性，尤其是在[类别不平衡](@article_id:640952)的情况下。如果一种疾病只影响$1\%$的人口，一个总是预测“健康”的平凡分类器将有$99\%$的准确率，但它完全无用，因为它对病患的召回率为零。这就是**准确度悖论**。[@problem_id:3169385] 一个[概率分类](@article_id:641547)器允许我们使用更细致的评估指标，如**Brier分数**或**ROC/P[R曲线](@article_id:362970)下面积**，这些指标评估概率本身的质量，而不仅仅是最终的硬分类结果。[@problem_id:3169385]

有趣的是，拟合逻辑斯谛回归模型的机制，看起来与[最小二乘法](@article_id:297551)如此不同，实际上是秘密相连的。最常见的[算法](@article_id:331821)，**[迭代重加权最小二乘法](@article_id:354277)（IRLS）**，通过求解一系列*加权*[最小二乘问题](@article_id:312033)来求解[逻辑斯谛回归](@article_id:296840)的参数，其中的权重是根据模型自身在每一步的方差巧妙地导出的。[@problem_id:3183052]

### [过拟合](@article_id:299541)悖论：当更多变得更好

到目前为止，这个故事似乎是一个警世寓言：不要用回归来做分类。但现代机器学习告诉我们，这个简单的想法有一个令人惊讶和深刻的第二幕。这发生在我们进入高维度的奇异[世界时](@article_id:338897)，其中特征数量$p$远大于数据点数量$n$（$p \gg n$）。

在这种“过[参数化](@article_id:336283)”状态下，我们从[经典统计学](@article_id:311101)中得出的直觉会失效。当维度多于数据点时，线性模型有如此多的自由度，以至于它可以完美地拟合*任何*一组标签。以概率1，你可以找到一个权重向量$w$，为每个训练点产生完全[期望](@article_id:311378)的输出（$Xw = y$）。这是**维度灾难**的一种表现。这似乎是[过拟合](@article_id:299541)的终极配方——模型只是记住了训练数据。[@problem_id:3181635]

通过这种方式实现最小可能参数范数$\|w\|_2$的模型被称为**最小范数[插值器](@article_id:363847)**。经典理论会预测，这样一个通过蛮力实现零[训练误差](@article_id:639944)的模型，在推广到新数据时会表现得非常糟糕。事实上，随着特征数量$p$接近样本数量$n$，[测试误差](@article_id:641599)会趋向无穷大。

但神奇的事情发生了。当我们继续增加特征，将$p$推到远超$n$的程度时，[测试误差](@article_id:641599)在达到峰值后，又开始下降了！这种现象被称为**[双下降](@article_id:639568)**。事实证明，在极度过[参数化](@article_id:336283)的状态下，并非所有完美的解决方案都是生而平等的。[@problem_id:3181635]

### [算法](@article_id:331821)的隐藏智慧

谜题的最后一块不在于模型，而在于我们用来训练它的[算法](@article_id:331821)。从零权重开始的简单**梯度下降**[算法](@article_id:331821)，有一个隐藏的偏好。它不只是找到*任何*拟合数据的解；它隐式地引导模型走向一种非常特殊的解，一种具有卓越属性的解。

*   当我们对**[平方误差损失](@article_id:357257)**（我们最初的最小二乘分类器）使用梯度下降时，提前停止训练过程的效果等同于**$\ell_2$（岭）正则化**。它优先学习数据中的“简单”模式（与数据矩阵的大[奇异值](@article_id:313319)相关），并收缩对应于更复杂、更嘈杂模式的分量。提前停止充当了防止[过拟合](@article_id:299541)的内置防御机制。[@problem_id:3169369]

*   当我们对**[逻辑斯谛损失](@article_id:642154)**使用梯度下降时，会发生更令人惊讶的事情。随着[算法](@article_id:331821)的运行，权重[向量的范数](@article_id:315294)$\|w\|$趋向于无穷大。但权重向量的*方向*会收敛到一个**硬间隔支持向量机（SVM）**的唯一解！该[算法](@article_id:331821)隐式地寻找与两个类别的数据点具有最大可能几何间隔的[决策边界](@article_id:306494)。[@problem_id:3169369]

所以，我们用来寻找解决方案的[算法](@article_id:331821)本身就赋予了一种**[隐式正则化](@article_id:366750)**，一种隐藏的智慧，它引导过参数化的模型走向一个泛化良好的解决方案。[损失函数](@article_id:638865)的选择——平方误差与[逻辑斯谛损失](@article_id:642154)——在这种隐式偏好上烙下了根本不同的烙印。

从一个简单、朴素的想法——用回归做分类——开始，我们经历了一场穿越[现代机器学习](@article_id:641462)核心原则的旅程。我们发现了它的缺陷——对异常值的敏感性和缺乏概率基础——这让我们欣赏到[逻辑斯谛回归](@article_id:296840)的优雅。但随后，在高维世界中，我们发现这个简单的想法，当与一个简单的[算法](@article_id:331821)配对时，蕴含着隐藏的深度，连接到[正则化](@article_id:300216)、[最大间隔分类器](@article_id:304667)，以及[双下降现象](@article_id:638554)这个令人惊讶的前沿领域。事实证明，这个“有缺陷的”方法一直是一位出色的老师。

