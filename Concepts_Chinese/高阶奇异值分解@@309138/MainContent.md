## 引言
在一个数据泛滥的世界里，从混乱中发现结构的能力至关重要。对于能够整齐地放入二维表格或矩阵的数据，奇异值分解（SVD）提供了一个“魔镜”，将复杂的变换分解为简单的旋转和拉伸。但是，当我们的数据本质上是多维的时——例如一个包含高度、宽度和时间维度的视频片段；一个在三维空间中追踪变量的科学模拟；或者一个跨越国家、[指标和](@article_id:368537)年份的经济数据集——情况又会如何呢？这就是[张量](@article_id:321604)的领域，矩阵SVD的简洁优雅在此不再直接适用。我们面临着在一个多面“数据立方体”中寻找主成分和基本模式的挑战。

本文将揭开[高阶奇异值分解](@article_id:379527)（[HOSVD](@article_id:376509)）的神秘面纱，这是一种针对[张量](@article_id:321604)的SVD的强大而优雅的推广。它解决了如何从高维数据中提取有意义信息这一根本问题，其方法是将数据分解为每个维度的一组主成分以及一个控制它们相互作用的核心[张量](@article_id:321604)。读者将从该方法的理论基础出发，逐步了解其在各个科学学科中的实际应用和深远影响。接下来的章节将首先阐明[HOSVD](@article_id:376509)的核心原理和机制，揭示驯服[张量](@article_id:321604)复杂性的巧妙“展开”技巧。随后，我们将探索其多样化的应用和跨学科联系，见证[HOSVD](@article_id:376509)如何实现从大规模[数据压缩](@article_id:298151)到发现复杂系统中隐藏驱动因素等一系列功能。

## 原理与机制

要真正领会[高阶奇异值分解](@article_id:379527)（[HOSVD](@article_id:376509)）的威力，我们必须首先重温一个老朋友：矩阵的奇异值分解（SVD）。可以把SVD想象成一副魔法眼镜，用来看[排列](@article_id:296886)在表格（矩阵）中的数据。无论一个矩阵执行多么复杂的操作，都可以理解为一个简单的三步序列：一次旋转，一次拉伸或挤压，以及另一次旋转。SVD明确地给出了这种分解：$A = U \Sigma V^T$。$U$和$V$矩阵是**旋转**（正交矩阵），而$\Sigma$是**缩放**（一个由奇异值构成的对角矩阵）。它告诉你数据中最重要的“方向”——即主成分——以及每个方向上汇集了多少“能量”或方差。

但是，当我们的数据不是一个平坦的表格，而是一个多维立方体，比如一堆图像、一个视频片段，或者一个包含许多相互作用变量的数据集时，会发生什么呢？这就是[张量](@article_id:321604)的领域。我们如何找到一个立方体的“主成分”？我们能简单地发明一个看起来像$U \Sigma V^T$的“[张量](@article_id:321604)SVD”吗？你可能不会惊讶地发现，答案是，自然界比这更微妙，也更有趣。

### 展开技巧：如何驯服[张量](@article_id:321604)

[HOSVD](@article_id:376509)的天才之处在于它没有试图重新发明轮子。它用一个巧妙的技巧，将不规则的多维[张量](@article_id:321604)带回到我们所熟悉的、平坦的矩阵世界。这个技巧被称为**展开**（unfolding）或**矩阵化**（matricization）。

想象你有一副扑克牌——一个三阶[张量](@article_id:321604)，其维度是（点数，花色，牌号）。你可以把所有牌并排摊开，形成一个长长的矩形图像。这是一个矩阵。但你也可以选择按不同的顺序[排列](@article_id:296886)，比如按花色分组。这会得到一个*不同*的矩阵，但它是由完全相同的牌构成的。

这正是展开所做的事情。对于一个有$N$个维度（或称“模态”）的[张量](@article_id:321604)，我们可以为它创建$N$个不同的[矩阵表示](@article_id:306446)。每个**模态-$n$展开**（mode-$n$ unfolding），记作$X_{(n)}$，是通过取出沿第$n$维延伸的所有向量“纤维”，并将它们[排列](@article_id:296886)成一个矩阵的列来创建的。

例如，考虑一个简单的$2 \times 2 \times 2$[张量](@article_id:321604)$\mathcal{X}$，它可能代表来自两个传感器在两个时间点上测量的两个不同属性的数据[@problem_id:1561885]。为了得到模态-1展开$X_{(1)}$，我们将[张量](@article_id:321604)“压平”，保留第一维度（传感器类型）作为行，而所有其他维度组合（属性和时间）则被拉伸成列。这样我们得到了一个大小为$2 \times (2 \times 2) = 2 \times 4$的矩阵。类似地，我们可以创建模态-2展开$X_{(2)}$来分析属性，或者创建模态-3展开$X_{(3)}$来分析时间模式[@problem_id:2154082]。

这个简单的重新[排列](@article_id:296886)行为是打开大门的关键。我们可能不知道如何找到[张量](@article_id:321604)的主成分，但我们当然知道如何对矩阵这样做！

### 多维世界的主轴

有了展开后的矩阵，我们现在可以对每一个矩阵应用熟悉的SVD。让我们以模态-1展开$X_{(1)}$为例，计算它的SVD。该矩阵的左奇异向量（其SVD中$U$矩阵的列）构成了一组标准正交轴，能够最好地描述模态1的变化。在我们的例子中，这些是传感器维度的“主成分”。第一个向量可能代表平均传感器响应，而第二个向量则捕捉它们之间的差异。

我们对每个模态重复此过程。我们沿模态2展开[张量](@article_id:321604)，得到矩阵$X_{(2)}$，并找出其左[奇异向量](@article_id:303971)。这些向量构成了我们的第二个**因子矩阵**$U^{(2)}$的列。我们再对模态3这样做以得到$U^{(3)}$，以此类推，对所有$N$个模态都进行同样的操作。这就是[HOSVD](@article_id:376509)程序的核心：一系列标准的SVD，每个SVD对应于数据在一个维度上的“视角”[@problem_id:1527690]。

但为什么这些奇异向量是“正确”的选择呢？事实证明，它们是可以证明的最优选择。例如，如果你想找到最好的ㄧ维子空间来表示模态2的变化，你必须选择由模态-2展开的主奇异向量所张成的那个子空间。该向量最大化了从该模态捕获的“能量”，这等同于最小化重构误差[@problem_id:1561872]。本质上，[HOSVD](@article_id:376509)是沿着[张量](@article_id:321604)的每个维度，一次一个地执行[主成分分析](@article_id:305819)（PCA）。

### 问题的核心：信息藏在哪里

我们现在有了一组正交因子矩阵$\{U^{(1)}, U^{(2)}, \dots, U^{(N)}\}$，每个维度一个。每个矩阵代表了其对应模态的主“轴”。但这些轴之间如何相互关联？当我们考虑了这些主方向后，原始[张量](@article_id:321604)还剩下什么？

答案在于**核心[张量](@article_id:321604)**（core tensor），通常记为$\mathcal{G}$或$\mathcal{S}$。我们通过将原始[张量](@article_id:321604)$\mathcal{X}$“投影”到这些新的轴集上来计算它。在数学上，它看起来是这样的：

$$ \mathcal{S} = \mathcal{X} \times_1 (U^{(1)})^T \times_2 (U^{(2)})^T \times \dots \times_N (U^{(N)})^T $$

在这里，$\times_n$表示$n$-模积（$n$-mode product），它相当于沿特定模态的[矩阵乘法](@article_id:316443)的[张量](@article_id:321604)版本。其美妙之处在于这个过程是完全可逆的：

$$ \mathcal{X} = \mathcal{S} \times_1 U^{(1)} \times_2 U^{(2)} \times \dots \times_N U^{(N)} $$

这就是完整的[HOSVD](@article_id:376509)。它告诉我们，任何[张量](@article_id:321604)都可以看作是一个**核心[张量](@article_id:321604)**，它描述了主成分之间的相互作用，而这些主成分本身则由**因子矩阵**定义。

是什么让这一切如此深刻？魔术就在这里。因为因子矩阵是正交的（它们是纯旋转），它们不改变[张量](@article_id:321604)的总“能量”——即所有元素平方和，或平方**[弗罗贝尼乌斯范数](@article_id:303818)**（Frobenius norm）——的大小。这导出了一个非凡的守恒定律[@problem_id:1561833]：

$$ \|\mathcal{X}\|_F^2 = \|\mathcal{S}\|_F^2 $$

原始杂乱数据[张量](@article_id:321604)的总能量与整洁紧凑的核心[张量](@article_id:321604)的总能量*完全相同*！[HOSVD](@article_id:376509)就像一个[棱镜](@article_id:329462)，它将$\mathcal{X}$中杂乱的信息进行分离，并将其能量集中到$\mathcal{S}$的少数几个元素中。通常，核心[张量](@article_id:321604)的最大值聚集在一个角落（例如，像$\mathcal{S}_{111}, \mathcal{S}_{112}, \dots$这样的元素）。这就是为什么[HOSVD](@article_id:376509)在数据压缩方面如此出色。我们可以只保留核心[张量](@article_id:321604)中那个小的、能量密集的一角，丢弃其余部分，仍然可以重构出原始[多维数据](@article_id:368152)集的一个极佳近似。

### [张量](@article_id:321604)前沿的注意事项与奇特现象

这幅图景美丽而强大，但正如任何深刻的科学思想一样，完整的故事中也包含一些对实践者而言有趣的细微差别和警告。

首先，一个实用技巧。如果你的数据全部由非负数组成，比如客户在网站上花费的时间，那么最主要的“模式”仅仅是每个人都花费了*一些*时间；数据有一个很大的平均值或“直流分量”。如果你直接应用[HOSVD](@article_id:376509)，你每个模态的第一个也是最“重要”的主成分将只是这个恒定的平均值。这可能会掩盖你正在寻找的更微妙、更有趣的变化。解决方法很简单：在开始分解之前，通过减去均值来**对你的数据进行中心化**[@problem_id:1561840]。

第二，一个理论上的精确点。虽然[HOSVD](@article_id:376509)是一种出色且计算直接的方法，但对于给定的Tucker秩，它提供的近似通常*不是*最小二乘意义下的绝对最佳拟合。一种称为交替[最小二乘法](@article_id:297551)（ALS）的迭代方法通常可以找到更好的拟合，但代价是更大的计算量和陷入局部最小值的风险。在实践中，[HOSVD](@article_id:376509)提供了一个非常好的起点，因此常被用来初始化ALS [@problem_id:1561884]。

此外，这种分解不是唯一的。你可以对因子矩阵应用[旋转变换](@article_id:378757)，并对核心[张量](@article_id:321604)应用*逆*旋转，重构后的[张量](@article_id:321604)$\mathcal{X}$保持不变[@problem_id:1542441]。[HOSVD](@article_id:376509)通过对因子矩阵强制施加正交性以及对核心[张量](@article_id:321604)条目进行排序来减少这种模糊性，从而创建一个更规范的表示，但其基本属性仍然存在。

最后，也许是最令人愉快的一点是，我们从矩阵世界中关于“秩”的直觉在这里完全失效了。对于一个矩阵，秩是[线性无关](@article_id:314171)的行或列的数量，也是非零[奇异值](@article_id:313319)的数量。最佳的秩-$r$近似，其秩就是$r$。对于[张量](@article_id:321604)，情况就没这么简单了。我们有**多线性秩**$(R_1, R_2, \dots, R_N)$，这是你选择的核心[张量](@article_id:321604)的维度。但我们还有**典范秩**（canonical rank），即构建你的[张量](@article_id:321604)所需的最少简单秩-1[张量](@article_id:321604)（向量的外积）的数量。这两者之间没有简单的决定关系。

考虑一个看似简单的 $2 \times 2 \times 2$ [张量](@article_id:321604)，其切片定义为 $\mathcal{T}(:,:,1) = \begin{pmatrix} 1 & 0 \\ 0 & 1 \end{pmatrix}$ 和 $\mathcal{T}(:,:,2) = \begin{pmatrix} 0 & 1 \\ -1 & 0 \end{pmatrix}$。该[张量](@article_id:321604)具有完全的多线性秩(2,2,2)。你可能会猜测其典范秩为2。但可以证明，任何两个秩-1[张量](@article_id:321604)的和都无法构成它。事实上，它的典范秩是3 [@problem_id:1535337]。这不仅仅是一个数学上的奇特现象；它是[高维几何学](@article_id:304622)的一个基本特征，提醒我们从平面世界到立方体世界的飞跃是深刻的，充满了新的规则和惊人的真理。