## 引言
在日常生活中，我们认为“信息”是事实、新闻或知识。但在科学背景下，它的含义要精确得多，甚至可能与直觉相悖。一条冗长详细的消息如果只是告诉我们已经预料到的事，那么它可能只包含极少的新信息。相反，一个单一的、出乎意料的数据点可能信息量巨大。这是因为，信息的核心是对惊奇（surprise）的度量。因此，核心挑战在于超越直觉，发展出一种严谨的方法来量化这种“惊奇”。一个[稀有事件](@article_id:334810)比一个常见事件的信息量大多少？

本文深入探讨了 Surprisal 的基本概念，它也被称为[自信息](@article_id:325761)（self-information），为上述问题提供了数学上的答案。首先，我们将探讨 Surprisal 的**原理与机制**，推导其简洁而优雅的公式，讨论其单位，并揭示其不可打破的规则。我们将看到这个针对单个事件的概念如何通过[香农熵](@article_id:303050)扩展到描述整个系统的平均不确定性。随后，关于**应用与跨学科联系**的章节将揭示这一思想惊人的应用广度，展示 Surprisal 如何作为一种通用语言，描述[热力学](@article_id:359663)、[数据分析](@article_id:309490)、基因组学甚至免疫系统运作中的现象。

## 原理与机制

想象一下你拿起晨报。一条标题写着：“太阳从东方升起。”你可能会把报纸扔到一边。这不是新闻，而是一个确定无疑的事实。但如果标题是：“太阳从西方升起”呢？你会感到，说得客气点，非常震惊。这则信息将是颠覆性的。这个简单的思想实验抓住了我们在科学意义上所说的“信息”的本质。它与消息的长度或词语的复杂性无关。**信息是对惊奇（surprise）的度量**。一个确定的事件携带零信息。一个极不可能发生的事件则携带大量信息。

因此，我们的目标是建立一把数学标尺来衡量这种“惊奇”。这把标尺必须具备哪些性质？首先，一个事件的概率越低，它就应该越令人惊奇。相反，一个高概率事件应该非常不令人惊奇。并且，如果两个[独立事件](@article_id:339515)发生，比如说，你抛硬币得到正面，而你在另一个城市的朋友也抛硬币得到正面，那么总的惊奇程度应该就是各个独立惊奇程度的总和。

能够优雅地满足所有这些性质的函数是对数。我们定义一个以概率 $p$ 发生的事件的 **Surprisal**，或称**[自信息](@article_id:325761)**（self-information），为：

$$
I(p) = -\log(p)
$$

让我们花点时间来欣赏这个优美而简洁的公式。如果一个事件是确定会发生的，其概率为 $p=1$。那么 Surprisal 就是 $I(1) = -\log(1) = 0$。正如我们所[期望](@article_id:311378)的，完全没有惊奇。考虑一个非常可靠的计算机存储单元。它保持在`0`状态而不翻转的概率可能是 $p=0.85$。我们观察到它确实保持为`0`所获得的信息是 $I(0.85) = -\log_2(0.85) \approx 0.234$ 比特 [@problem_id:1666601]。这是一个微小的信息量，因为这是预期的结果。

那么，极不可能的事件呢？想象一个深空探测器上的传感器极其可靠，但产生[假阳性](@article_id:375902)信号的概率极小，比如 $p=0.015$。当任务控制中心接收到那个假阳性信号时，其惊奇程度是巨大的。信息内容为 $I(0.015) = -\log_2(0.015) \approx 6.06$ 比特 [@problem_id:1604149]。这比未翻转的存储单元所含信息多出 25 倍以上！这个公式是有效的。它定量地证实了我们的直觉：稀有事件更具信息量。公式中的负号只是为了使结果成为一个正数，因为一个介于 0 和 1 之间的数的对数总是负的。

### 单位问题：比特、奈特与哈特利

你可能已经注意到上面例子中对数上的小下标‘2’。Surprisal 的公式有一个“庄家自选”项：对数的底数。这个选择不改变信息的性质，但它定义了我们测量信息所用的**单位**。这就像决定是用米、英尺还是英寸来测量长度一样。

在计算机科学和通信领域，最常见的单位是**比特**（bit），它源于使用以 2 为底的对数（$I(p) = -\log_2(p)$）。对于一个建立在二进制逻辑上的世界来说，这是自然的选择。一个比特是你从一次公平的硬币投掷结果中获得的[信息量](@article_id:333051)。硬币有两个结果，正面或反面，每个的概率都是 $p=0.5$。其 Surprisal 是 $-\log_2(0.5) = -\log_2(2^{-1}) = 1$ 比特。

其他学科使用不同的底数。在许多理论物理和高等统计学领域，使用自然对数（底为 $e \approx 2.718$）很方便，这产生了一个称为**奈特**（nat）的单位。例如，在临床试验中，p 值表示在假设零假设（例如，药物无效）为真的情况下，观察到至少与已发现结果一样极端的结果的概率。一个小的 p 值，比如 $p=0.015$，是一个令人惊奇的结果。这次观察的 Surprisal，以奈特为单位测量，将是 $I(0.015) = -\ln(0.015) \approx 4.20$ 奈特 [@problem_id:1666572]。

如果我们使用我们在小学都学过的以 10 为底的对数呢？这会产生一个叫做**哈特利**（hartley）的单位。如果你对一个有五个选项的多项选择题进行纯粹的随机猜测，你答对的概率是 $p = 1/5 = 0.2$。当你得知正确答案时获得的信息是 $I_{10}(0.2) = -\log_{10}(0.2) = \log_{10}(5)$ 哈特利 [@problem_id:1666610]。如果一个学生错误地使用以 10 为底而不是以 2 为底的对数来计算一个公平的 16 面骰子掷出的信息，他们会发现结果是 $\log_{10}(16) \approx 1.20$ 哈特利，而不是正确的 $\log_2(16) = 4$ 比特 [@problem_id:1666589]。单位变了，但惊奇的基本概念保持不变。

### 信息论的铁律

如果我们试图打破规则会发生什么？根据其定义，概率必须是一个介于 0 和 1 之间的数。但假设一个糊涂的研究人员的模型输出了一个 $p=1.6$ 的概率。如果他们将这个值代入 Surprisal 公式会怎样？他们会计算出 $I(1.6) = -\log_2(1.6)$，这是一个*负数*。

这个结果是荒谬的，但它揭示了一个深刻的真理。信息，作为不确定性减少的度量，不能是负数 [@problem_id:1666609]。观察一个事件，任何事件，只能确认你已经知道的（提供零信息）或告诉你一些新的东西（提供正信息）。它永远不会让你比以前*更加*不确定。因此，一个核心原则是**Surprisal 必须是非负的**，$I(p) \ge 0$。只要我们使用有效的概率（$0 < p \le 1$），这一点就能得到保证。

这个原则不仅仅是一个哲学观点；它是一个强大的分析工具。想象一个在一维盒子里的量子粒子。找到它的概率不是均匀的；比如说，它在一端出现的概率更高。一个探测器只能告诉我们粒子是在`区域 1`还是`区域 2`。假设我们进行一个实验，发现从在`区域 1`找到粒子获得的信息恰好是从在`区域 2`找到它所获信息的*两倍*。这意味着 $I_1 = 2 I_2$，或者 $-\log_2(p_1) = 2(-\log_2(p_2))$。利用对数的性质，这个简单的关系告诉我们，其潜在的概率必须满足 $p_1 = (p_2)^2$。因为我们还知道 $p_1 + p_2 = 1$，所以我们可以解出确切的概率，并由此确定两个区域之间精确的物理边界 [@problem_id:1356019]。我们利用了一条关于信息的定律来推断系统的物理性质。

### 从惊奇到熵：平均信息

到目前为止，我们一直关注单个孤立事件的惊奇程度。但我们关心的大多数系统是产生一系列事件的信息源，每个事件都有其自身的概率。想想英语，字母‘E’非常常见，而‘Z’则很罕见。或者一个分析外星大气层的太空探测器上的仪器，它可能 50% 的时间检测到“阿尔法”信号，20% 的时间检测到“贝塔”信号，20% 的时间检测到“伽马”信号，10% 的时间检测到“德尔塔”信号 [@problem_id:1361070]。

对于这样一个信息源，我们可以提出一个全新的、更强大的问题：每个事件的*平均惊奇度*是多少？这个平均 Surprisal 是信息论的基石，被称为**[香农熵](@article_id:303050)**，用字母 $H$ 表示。它的计算方法是，取每个可能结果的 Surprisal，用该结果的概率对其进行加权，然后将它们全部相加。

$$
H = \sum_{i} p_i I(p_i) = -\sum_{i} p_i \log_2(p_i)
$$

对于这颗[系外行星探测](@article_id:320764)器，熵将是每个接收信号的平均[信息量](@article_id:333051)。非常常见的“阿尔法”信号（$p=0.5$）具有 1 比特的低 Surprisal。罕见的“德尔塔”信号（$p=0.1$）具有约 3.32 比特的高 Surprisal。该信息源的熵是所有这些 Surprisal 的[加权平均](@article_id:304268)值，计算结果约为每个信号 1.76 比特 [@problem_id:1361070]。这个单一的数字表征了该源的整体不可预测性。一个结果几乎确定的源具有非常低的熵（它是可预测的）。一个所有结果都等可能性的源具有最大可能的熵（它是完全不可预测的）。因此，熵是我们衡量系统平均不确定性的尺度。

### 情境中的信息：波动与关系

Surprisal 的概念开启了一个全新的统计分析世界。熵给了我们平均[信息量](@article_id:333051)，但我们也可以探究围绕该平均值的*波动*。对于一个简单的二元事件（比如一个可能不均匀的硬币投掷），我们可以计算 **Surprisal 的方差**。这告诉我们信息内容的“波动性”有多大。如果一个结果非常可能，而另一个非常不可能，那么 Surprisal 的方差可能很大，因为你要么得到一个非常无聊的信号，要么得到一个非常惊奇的信号 [@problem_id:1667116]。

也许最重要的是，信息论为我们提供了一种精确的方式来讨论不同事件之间的关系。知道一个变量 $Y$ 的值对另一个变量 $X$ 了解多少？这由**互信息**（mutual information）$I(X;Y)$ 来量化。它被定义为在了解 $Y$ 之后 $X$ 的不确定性的减少量：$I(X;Y) = H(X) - H(X|Y)$，其中 $H(X|Y)$ 是在已知 $Y$ 的情况下关于 $X$ 的剩余不确定性。

正如单个事件的 Surprisal 不能为负一样，[互信息](@article_id:299166)也有其自身优美而不可打破的规则：$I(X;Y) \ge 0$ [@problem_id:1643396]。平均而言，知识只会有所帮助。了解一件事，平均来说，永远不会让你对另一件事*更加*不确定。在最坏的情况下，如果两个变量完全独立（比如巴黎的天气和中国的茶价），了解一个变量对另一个变量毫无帮助，互信息恰好为零。这种非负[信息增益](@article_id:325719)的原则是贯穿整个理论的一条统一线索，从最简单的单个事件到最复杂的相互关联的变量网络。