## 应用与跨学科联系

我们现在已经仔细审视了 [IEEE 754](@entry_id:138908) 数字的剖析。我们已经看到了它的比特骨架、有限的精度以及支配其生命的奇特[舍入规则](@entry_id:199301)。人们可能很想将这些知识归档为技术奇闻，一个供计算机架构师和[数值分析](@entry_id:142637)师研究的课题。但这就像研究颜料的性质却从不看一幅画一样。浮点运算真正的魔力——以及偶尔的恶作剧——只有在它实际应用时才会显现出来。

我们所建立的世界，从我们视频游戏中广阔的虚拟景观到驱动我们金融市场的复杂模型，都建立在这种有限数字的基础上。我们所讨论的那些看似深奥的细节，实际上是我们数字现实的无形建筑师。现在让我们踏上一段旅程，去看看它们所建造的美丽、奇特，有时甚至是惊人的结构。

### 充满陷阱的计算艺术

我们在算术中学到的第一件事就是加法是结合的：$(a+b)+c$ 总是和 $a+(b+c)$ 一样。这是一条像我们脚下的大地一样坚实可靠的规则。但果真如此吗？在浮点数的世界里，这片大地会突然塌陷。

想象一个交易平台在计算其每日盈亏（P&L）。假设它有一笔巨大的[已实现收益](@entry_id:754142) $a = 100,000,000$ 美元，一笔同样巨大的融资成本 $b = -100,000,000$ 美元，以及一笔小额的手续费返还 $c = 1$ 美元。确切的总额当然是 $1$ 美元。现在，使用 `binary32` 算术的计算机会怎么说？如果它计算 $(a+b)+c$，它首先将两个大数相加。确切的和是零，这是可以完美表示的。然后它加上 $1$ 美元的返利，得到最终正确的答案 $1$。但是，如果由于代码中一些无伤大雅的改变，它计算了 $a+(b+c)$ 呢？在 $100,000,000$ 的量级上，`binary32` 的精度相当粗糙。一个可表示的数与下一个之间的差距大约是 $8$ 美元！当计算机试图将 $1$ 美元的返利加到 $-100,000,000$ 的融资成本上时，这个微小的返利在舍入过程中完全丢失了——这就像试图用一个为卡车制造的秤来测量一根羽毛的重量。$b+c$ 的结果被舍入回 $b$。最终的计算变成了 $a+b$，结果是零。这 $1$ 美元仅仅因为运算顺序的不同就消失在了数字[以太](@entry_id:275233)中 [@problem_id:2427689]。

这种现象被称为**淹没** (swamping)，是一个普遍的危险。当对一系列[数量级](@entry_id:264888)差异巨大的数字求和时，将小数加到一个大的累计总和上是导致它们丢失的良方。一个聪明但并非总是足够的技巧是对数字进行排序，并按量级递增的顺序相加。这使得小值能够累积成一个足够大的和，以便在最终引入大数时能够产生影响 [@problem_id:3240489]。

但即使这样也有其局限性。考虑一个真正惊人的情景：如果我们从数字 $1.0$ 开始，并不断地给它加上 $1.0$，会发生什么？你可能认为这可以永远进行下去。但在 `binary32` 的世界里，有一堵墙。随着累计和的增长，可表示数字之间的差距——最后一位单位（ULP）——也在增长。最终，总和达到了巨大的值 $2^{24} = 16,777,216$。在这个量级上，ULP 已经增长到 $2.0$。如果我们现在尝试加上 $1.0$，确切的结果是 $16,777,217$，它恰好位于两个可表示数字 $16,777,216$ 和 $16,777,218$ 的正中间。平局决胜规则（“舍入到最近，平局取偶”）迫使结果回到 $16,777,216$。总和停滞了。在成功进行了 $16,777,215$ 次加法之后，这个过程再也无法前进了 [@problem_id:3214591]。为了克服这些根本性的障碍，数学家们发明了更复杂的技巧，如 **Kahan 求和**，它巧妙地使用一个补偿变量来跟踪每次加法中“丢失的零钱”，并将其反馈到下一步中，从而使总和能够增长到远超朴素加法放弃的点。

有时，错误不是关于丢失小数，而是关于无中生有地制造大错误。考虑计算 $a \times b + c$，其中 $a \times b$ 非常接近 $-c$。中间乘积 $p = a \times b$ 中的一个微小舍入误差，在加上 $c$ 时可能会被极大地放大，这就是所谓的**灾难性抵消**。一个像 $1+2^{-24}$ 这样看似无害的数被舍入为 $1$，可能会使一个微小的最终结果偏差达到 $100\%$ 或更多 [@problem_id:2215617]。正是这个问题，是现代处理器包含**[融合乘加 (FMA)](@entry_id:167576)** 指令的原因之一，该指令在一次操作中完成整个 $a \times b + c$ 的计算，只在最后进行一次舍入，从而优雅地避开了中间的舍入误差。

### 用不完美的数字绘制世界

这些数值怪癖的后果并不仅限于电子表格和[科学模拟](@entry_id:637243)。它们被描绘在我们玩的每一款视频游戏的屏幕上。如果你曾见过游戏中的远山或重叠表面相互闪烁、争夺可见性，你就目睹了一种称为 **Z-fighting**（深度冲突）的现象。这是 `binary32` 如何表示深度的直接结果。

在 3D 图形中，Z 缓冲（Z-buffer）存储每个像素的深度。这些深度可能从近平面 $0.1$ 米到远平面 $1000$ 米不等，通常被映射到 $[0, 1]$ 的范围并存储为 `binary32` [浮点数](@entry_id:173316)。然而，浮点数的[分布](@entry_id:182848)不是均匀的。它们在零附近极其密集，随着接近一而变得越来越稀疏。不幸的是，透视[投影公式](@entry_id:152164)将远处的物体（大的深度值）映射到非常接近 $1$ 的数字上。在数轴的这个稀疏区域，游戏世界中相隔数米的两个物体可能会映射到缓冲中完全相同的深度值。渲染器无法决定哪个在前面，所以它会渲染来自两者的片段，导致特有的闪烁。通过简单地将近平面向外移动，比如从 $0.1$ 米移到 $1.0$ 米，我们可以显著提高远处物体的精度，减少这些难看的瑕疵——这是各地游戏开发者都使用的实用技巧 [@problem_id:3240447]。

这种范围和精度之间的权衡也出现在其他空间领域。考虑一个存储经纬度坐标的地理信息系统（GIS）。我们应该使用 `binary32` 吗？它提供了巨大的动态范围，能够表示行星尺度和微观尺度上的位置。但如果我们只关心精确到米级的精度呢？在经度为 $180$ 度时，`binary32` 格式的精度大约是米级别。我们可以改用一个 32 位的*整数*作为定点数，我们只需约定该整数值代表以 $10^{-6}$ 度为单位的坐标。这种定点方案在全球范围内提供了一个恒定、已知的精度。对于这个特定应用，定点表示在所需范围内可能比 `binary32` 更精确，而使用的内存量完全相同。这是一个经典的工程教训：选择正确的工具需要了解所有选项的局限性 [@problem_id:3662510]。

### 机器中的幽灵：人工智能与现代科学

在人工智能领域，浮点数的微妙行为比任何地方都更为关键。现代[神经网](@entry_id:276355)络使用诸如[随机梯度下降](@entry_id:139134)（SGD）之类的算法进行训练，这涉及到基于微小的“梯度”更新迭代调整数百万个参数。模型通过采取小步来最小化误差来学习。但如果一步太小了怎么办？

`binary32` 格式对它能表示的最小数字有限制。在最小的[规格化数](@entry_id:635887) $2^{-126}$ 以下，我们进入了次[规格化数](@entry_id:635887)的领域，在这里精度被逐渐牺牲以表示更接近零的值。但在最小的次[规格化数](@entry_id:635887) $2^{-149}$ 以下，这条路就走到了尽头。任何比这更小的结果都会**[下溢](@entry_id:635171)**为零。如果 AI 模型中的梯度更新如此微小，它就变成了零。参数没有被更新。模型停止学习，完全卡住，不是因为理论错误，而是因为数字失效了 [@problem_id:3260965]。

这是训练大规模模型时一个真实而紧迫的问题。解决方案，在几乎所有现代[深度学习](@entry_id:142022)框架中都使用，是一种称为**梯度缩放**的技术。在 `binary32` 中进行计算之前，将微小的梯度乘以一个大的缩放因子（例如 $2^{16}$）。这将它们“提升”出下溢危险区，进入[规格化数](@entry_id:635887)的稳健范围。计算继续进行，最后，结果被相同的因子缩放回去。这是一项优美的数值工程，使得学习能够在否则会是数值沙漠的地方继续进行。

同样的“消失效应”也出现在其他 AI [范式](@entry_id:161181)中，比如[遗传算法](@entry_id:172135)。在这些算法中，一个数字“生物”（解决方案）的种群根据适应度分数进行竞争。[适应度](@entry_id:154711)最高的更有可能被选中来产生下一代。但是，如果竞争个体之间的适应度差异相对于它们的基线适应度非常小，`binary32` 的[舍入误差](@entry_id:162651)可能会导致它们看起来都具有相同的适应度。[选择压力](@entry_id:175478)消失，进化陷入停顿。算法寻找更好解决方案的能力被其数字的有限性所扼杀 [@problem_id:3257678]。

### 精度的声音

也许关于[浮点精度](@entry_id:138433)最诗意的例证来自音乐世界。一个音符由其频率定义，这是一个数字。一个和声是这些音符的总和。十二平均律音阶，即大多数西方音乐的基础，由关系式 $f(n) = f_0 \cdot 2^{n/12}$ 定义，这是一个非常适合[浮点](@entry_id:749453)计算的公式。

让我们比较一下使用 `binary32` 与更精确的 `[binary64](@entry_id:635235)`（双精度）计算的音乐和弦的频率。差异是微乎其微的——相对误差通常小于一亿分之一。这肯定无关紧要吧？

但是声波是一个时间过程。它的相位由 $2 \pi f t$ 给出。即使频率上一个微小的误差 $\Delta f$，当乘以时间后，也会导致一个不断增长的[相位漂移](@entry_id:266077)，$\Delta \phi = 2 \pi (\Delta f) t$。仅仅十秒钟后，音符的相位就可能漂移出可察觉的量。如果我们听得更久，漂移可能会变得很大，导致一个完美和弦的纯净、稳定的声音因令人不快的[失谐](@entry_id:148084)而波动和悸动。

和声波形总偏差的上限可以被严格推导出来，它与组成音符累积的[相位漂移](@entry_id:266077)直接相关 [@problem_id:3231577]。这是一个极其优雅的联系：声音的数值不稳定性是其各部分相位不稳定性的直接度量。音乐和声的质量，其纯净度和稳定性，可能取决于我们用来记录其频率的比特数。

### 结论

我们的旅程带我们从金融到视频游戏，从[地图学](@entry_id:276171)到人工智能，最后到音乐。在每个领域，我们都看到了 `binary32` 算术的相同基本原理在起作用。我们看到了它的有限性、非[均匀性](@entry_id:152612)如何带来了令人惊讶的挑战——[舍入误差](@entry_id:162651)导致求和崩溃、物体闪烁、[进化停滞](@entry_id:169393)、和声变味。

但我们也看到了这些挑战所激发的独创性：[补偿求和](@entry_id:635552)算法、巧妙的投影设置、梯度缩放以及对数字格式的谨慎选择。理解我们数值工具的深层结构不是一项深奥的练习。它是现代科学和工程的根基。世界在这些数字上运行，欣赏它们固有的美，以及它们固有的缺陷，使我们能够建立一个更好、更可靠、更有趣的数字宇宙。