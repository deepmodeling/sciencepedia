## 应用与跨学科联系

现在我们对遗憾的原则有了一些了解，让我们在科学和工程的花园中漫步，看看这个想法在何处开花结果。你可能会感到惊讶。遗憾，以其冷静、计算的形式，不仅仅是计算机科学家的工具；它是一个统一的视角，通过它我们可以理解各处的决策制定，从鸟类的觅食策略到人工智能的伦理护栏。对于因非全知而付出的代价，它是一种通用货币。

### 不完美知识的代价：在迷雾世界中决策

很多时候，我们必须在看不清世界的情况下采取行动。我们的信息可能是错误的、不完整的，或者根本不存在。遗憾分析为我们提供了一种量化这种“迷雾”成本的方法，并且在某些情况下，还为我们提供了导航的指南针。

想象一只在野外觅食的捕食者。它遇到两种猎物。一种体型大但难以捕捉和处理；另一种体型小但容易得手。捕食者对每种猎物的收益率——即获得的能量 $E$ 与处理时间 $h$ 的比率——形成了一种“信念”。遵循[最优觅食理论](@article_id:323726)的逻辑，它总是会吃掉它认为收益率最高的猎物，并且只有在将次等猎物加入其食谱所带来的能量摄入率高于专注于最优猎物时，才会这样做。但如果它的信念是错误的呢？如果那些看起来不起眼的小猎物实际上是能量炸弹，而大猎物却不如看起来那么有营养呢？捕食者根据其有缺陷的世界模型行动，可能会采取次优的食谱，或许会忽略真正更有利可图的食物来源。这里的遗憾不是一种情绪，而是一个非常真实的量：损失的卡路里，即它在拥有完美知识的情况下*本可以*达到的能量摄入率与它*实际*达到的摄入率之间的差异。生态学家可以对这种情况进行建模，以了解[动物行为](@article_id:300951)在面对环境误判时的稳健性[@problem_id:2515985]。

这种困境并非动物王国所独有。考虑一家网约车公司设定其高峰期定价。理想的价格取决于其客户的“需求弹性”——即他们对乘车的需求如何随价格变化。这种弹性不是一个固定的数字；它是一个[随机变量](@article_id:324024)，随一天中的时间、天气和成千上万个其他因素而变化。公司无法知道其真实分布。相反，它必须依赖有限数量的过去观察来建立一个统计模型。当它使用这个基于样本的模型来设定价格时——这种技术被称为[样本均值近似](@article_id:639454)——所选的价格几乎肯定会与[预言机](@article_id:333283)所选择的真正能实现收入最大化的价格不同。遗憾是收入上的真实差异，是因决策基于世界的有限统计快照而非完整的潜在现实而损失的金钱[@problem_id:3174785]。

有时，“迷雾”不是数据的缺乏，而是使用了错误的地图。在机器学习中，我们经常使用方便的指标来训练模型。[数据科学](@article_id:300658)家可能会构建一个分类器来检测一种罕见疾病，并将其优化以获得高的 $F_1$ 分数，这是一个平衡[精确率和召回率](@article_id:638215)的标准指标。但使用该分类器的医院并不关心 $F_1$ 分数。它关心的是成本：假阴性（漏掉一个生病的病人）的成本极高，而[假阳性](@article_id:375902)（不必要地惊动一个健康的病人）的成本要低得多。通过为“错误”的目标进行优化，分类器的决策阈值可能被设置在一个虽然最大化了 $F_1$ 分数，却导致灾难性高昂的真实世界成本的点上。在这里，遗憾分析提供了模型抽象目标与用户真实潜在[效用函数](@article_id:298257)之间不一致所导致的惩罚的确切金额[@problem_id:3118850]。

最后，如果迷雾是绝对的呢？如果我们面临“严重不确定性”，甚至无法为未来的世界[状态分配](@article_id:351787)概率呢？想象一个生物安全委员会正在为敏感的基因研究制定发表政策。这项研究可[能带](@article_id:306995)来巨大的公共利益，但也带有被滥用的风险。结果取决于未来的社会条件——合规性会很高，还是会有敌对分子积极寻求滥用信息？我们没有可靠的方法来知道这些状态的概率。需要概率的[期望效用理论](@article_id:301069)此时就[无能](@article_id:380298)为力了。在这里，最小化最大遗憾准则就显示出其优势。对于每个可能的未来状态，我们可以计算每项政策的“遗憾”——即其回报与*针对该特定状态*的最佳政策的回报之间的差异。然后，最小化最大遗憾方法告诉我们选择那个能最小化我们最大可能遗憾的政策。这是一种稳健、保守的策略，旨在避免灾难性的“早知如此”情景，使其成为从生物安全到[气候变化经济学](@article_id:304160)等领域中，进行伦理和公共政策决策的宝贵工具[@problem_id:2738600] [@problem_id:2525839]。

### 急于求成的代价：贪心算法与长远之计

遗憾的另一个来源是我们自己的急躁。我们常常偏爱简单的、“贪心”的[算法](@article_id:331821)，这些[算法](@article_id:331821)做出*当下*看起来最好的决策。这种短视可能导致与全局最优相去甚远的结果。

思考一下计算机程序如何学习构建[决策树](@article_id:299696)，这是一种机器学习中常见的工具。在每一步，它都必须决定如何分割数据。贪心的方法是选择能产生最纯净的直接子节点的分割，这通过像[误差平方和](@article_id:309718)（SSE）这样的指标来衡量。这似乎很合理。然而，一个稍微“差一点”的初始分割可能会创造出更容易解决的子问题，从而在几步之后得到一个好得多的整体树。“单步前瞻”策略，即考虑*下一次*分割的后果，可以胜过纯粹的贪心策略。遗憾，定义为最终树的SSE的差异，精确地量化了这种[算法](@article_id:331821)层面急于求成的代价[@problem_id:3168079]。

这种矛盾是“在线”[算法](@article_id:331821)和“离线”[算法](@article_id:331821)之间分歧的核心。离线[算法](@article_id:331821)可以一次性看到所有数据，就像能看到整个棋盘的国际象棋大师。而[在线算法](@article_id:642114)则看到数据逐个到达，并且必须当场做出不可撤销的决定。考虑在线背包问题：你正在打包一个袋子，但物品是逐一呈现给你的，你必须立即决定是否拿走每个物品，而不知道接下来会出现什么。一个聪明的在线策略可能会使用像割平面这样的复杂数学工具来做出有根据的猜测，但其表现将永远以那个从一开始就知道整个物品序列的“离线”[预言机](@article_id:333283)为基准。遗憾是由于被迫“在当下”决策而损失的总利润，这是[在线算法](@article_id:642114)分析中的一个基本概念[@problem_id:3115630]。

### 边做边学：[探索-利用困境](@article_id:350828)

或许，遗憾分析最活跃和现代的应用是在[在线学习](@article_id:642247)领域，它量化了根本的“探索-利用”权衡。为了学习，你必须探索并尝试新事物，而这些新事物可能不是最佳选择。但为了表现良好，你必须利用你已经知道的最优选择。累积遗憾是在这个学习过程中产生的总性能损失，而一个好的学习[算法](@article_id:331821)的目标是让这个遗憾增长得尽可能慢。

这个困境以多种形式出现。在调整机器学习模型的“超参数”时，我们实际上是在一个巨大的空间中寻找最佳设置。我们应该在空间中均匀采样，还是应该利用我们的[先验信念](@article_id:328272)来集中搜索？遗憾分析可以通过计算在一定数量的试验后找到的最佳模型的[期望](@article_id:311378)损失来比较这些策略，为我们设计更高效的[搜索算法](@article_id:381964)提供了一种有原则的方法[@problem_id:3129509]。

一个经典的例子是 A/B 测试。一个网站想知道两个版本 A 或 B 中哪一个[能带](@article_id:306995)来更多的转化。一种朴素的策略是运行一个固定时长的测试，收集数据，然后将所有用户切换到获胜者。一种更智能的“老虎机”方法会实时更新其信念。在每一刻，它都面临一个选择：将下一个用户导向当前看起来更好的版本（利用），还是将他们导向另一个版本以收集更多数据并减少不确定性（探索）。目标是设计一个策略，以最小化总[期望](@article_id:311378)遗憾——即与从一开始就知道最佳版本的[预言机](@article_id:333283)相比，损失的转化次数[@problem_id:3154933]。

同样的逻辑可以扩展到大规模的[推荐系统](@article_id:351916)。当你访问一个在线商店时，它有数百万件商品可以推荐。它需要学习你的偏好。它可以通过向你展示与你以前喜欢的东西相似的商品来进行利用，也可以通过向你展示一些新东西来进行探索。像基于上置信界（UCB）的复杂[算法](@article_id:331821)会明确地为其对你偏好的估计中的[不确定性建模](@article_id:332122)。它们为认知较少的商品提供“不确定性奖励”，以鼓励探索。对于这些[算法](@article_id:331821)，可以从数学上证明，累积遗憾仅随时间呈对数增长，这是一个了不起的结果，意味着系统以惊人的效率学习[@problem_id:3145687]。

这个框架的美妙之处在于其普适性。我们可以将“用户和商品”替换为“控制器和动作”，然后发现自己身处[自适应控制](@article_id:326595)的世界。想象一个机器人学习移动。它的“大脑”包含一个关于其自身物理学的近似模型——即控制其状态 $x_t$ 在施加控制 $u_t$ 时如何演化的矩阵 $A$ 和 $B$ 。为了改进其模型，它必须通过施加各种控制来“探索”，包括一些对其当前任务而言并非严格最优的“摆动”或扰动。这种探索帮助它更快地学习其动态特性，从而在未来实现更好的控制。遗憾是在这个自我发现阶段产生的额外能量或误差，与一个天生就拥有完美自我知识的控制器相比[@problem_id:3121216]。

我们甚至可以将这个想法带入合成生物学领域。设计[最小基因组](@article_id:323653)的科学家在某种意义上正在玩一场高风险的老虎机游戏。每个“臂”都是一个潜在的基因删除。拉动一个“臂”意味着创造和测试一个新的生物体。“回报”是其生长速率。一次“糟糕”的拉动——删除了一个[必需基因](@article_id:379017)——导致零回报（一个无法存活的生物体）。在这里，遗憾最小化指导着实验策略，告诉科学家应该测试哪些删除，以便在最小化失败实验次数的同时，最大限度地了解基因组的基本结构。这本身就是对[科学方法](@article_id:303666)的一种形式化的、[算法](@article_id:331821)化的途径[@problem-id:2741561]。

### 重新定义问题：公平、社会与遗憾的本质

遗憾的概念足够灵活，甚至可以容纳更微妙的社会考量。如果“最佳”策略是一个不公平的策略呢？

考虑一个为学生选择教育内容的[自适应学习](@article_id:300382)平台。[最优策略](@article_id:298943)可能会最大化平均考试分数，但在此过程中，它可能会发现一种类型的内容对某个群体更有效，而另一种内容对另一个群体更有效。一个无约束的[算法](@article_id:331821)会很乐意造成这种差异。如果我们施加一个公平性约束——例如，要求两种内容变体在不同群体间以相等的比率分配——那么可实现的最大平均分数可能会下降。在这种情况下，正确的遗憾基准是什么？它不再是那个无约束的、“不公平”的预言机。相反，遗憾应该与最佳*可行*策略进行比较，即在*满足公平性约束的同时*实现最高可能回报的策略。这种“约束遗憾”的概念对于开发必须在性能与伦理价值之间取得平衡的负责任的 AI 系统至关重要[@problem_id:3169872]。

最后，遗憾的想法可以延伸到更接近“社会不满”的意义上。在经典的[稳定婚姻问题](@article_id:335453)中，我们希望根据一组男性和女性的偏好将他们进行匹配。如果没有任何一个男性和女性宁愿彼此在一起也不愿与他们被分配的伴侣在一起，那么这个匹配就是“稳定的”。可能存在许多稳定的匹配。哪一个最好？一种方法是将一个智能体的“遗憾”定义为其被分配伴侣在其偏好列表中的排名（第一选择，第二选择，等等）。然后我们可以寻找一个能最小化任何单个个体*最大遗憾*的[稳定匹配](@article_id:641545)。这是一个深刻的平等主义概念，旨在创造一个没有任何人对其结果感到特别不满的社会，这是优化、社会选择理论与遗憾这个抽象概念之间一个美妙的联系[@problem_id:3274042]。

从机器学习的冷酷计算到生物学的鲜活现实，再到伦理学的复杂权衡，遗憾分析证明了它不仅仅是一个公式。它是一种深刻而统一的思维方式，关乎在一个我们永远无法全知的世界里，如何做出明智的选择。