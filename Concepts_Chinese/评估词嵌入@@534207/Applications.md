## 应用与跨学科联系

现在，我们已经学会了如何成为意义的制图师。通过观察哪些词与其他词为伴，我们可以绘制一张地图——一个[向量空间](@article_id:297288)——其中，邻近性代表了语义相似性。这是一项了不起的成就。但一张地图的好坏取决于它能促成什么样的旅程。我们能用这些新的几何景观做些什么呢？

你可能会认为这是语言学家和处理文本的计算机科学家使用的工具。你说得对，但这只是我们旅程的第一站。[分布假说](@article_id:638229)真正深刻的洞见在于其普适性。“符号”（token）与其“上下文”（context）之间的关系是一种随处可见的模式。我们即将看到，这一个思想如何让我们驾驭金融市场，理解我们自身 DNA 的语言，教计算机在没有词典的情况下进行翻译，甚至在一个城市中找到方向。让我们开始探索之旅吧。

### 磨砺我们的语言工具

首先，让我们停留在熟悉的人类语言领域，看看向量[嵌入](@article_id:311541)如何彻底改变了我们处理语言的方式。

一个常见的任务不仅是理解一个词，而是理解整个句子或文档。最简单的方法是直接将句子中所有词的向量取平均。但思考一下像“The cat sat on the mat”（猫坐在垫子上）这样的句子。“the”和“on”是功能词；它们提供语法结构，但几乎没有独特的意义。“cat”和“mat”才承载着真正的语义权重。简单的[平均法](@article_id:328107)给予所有词同等的话语权，重要的语义内容可能会被常见、意义较小的词的合唱声所淹没。我们可以做得更好。通过借鉴信息检索中的一种技巧，即逆文档频率 (IDF)，我们可以给予罕见、[信息量](@article_id:333051)大的词更多权重，而给予常见词更少权重。这种简单的重新加权通常能产生一个更忠实地捕捉句子核心主题或意义的句子向量，而不是其句法风格 [@problem_id:3199997]。

这种表示意义的能力是[情感分析](@article_id:642014)等任务的基石。假设你想构建一个模型来判断一篇产品评论是正面的还是负面的。你可能需要费力地标注数千条评论并训练一个模型。但如果你有堆积如山的未标注评论和少量已标注评论呢？这时，[嵌入](@article_id:311541)就大放异彩了。我们可以首先在整个庞大的、*未标注*的语料库上训练[词嵌入](@article_id:638175)。模型仅通过观察共现模式，就会自然地学到一个几何结构，其中“amazing”、“love”和“excellent”等词聚集在空间的一个区域，而“terrible”、“disappointed”和“awful”则聚集在另一个区域。模型并不知道哪个是“正面”或“负面”；它只知道语言的形态。现在，我们只需要几个标注好的例子——也许只有一个——来插上一面旗帜。通过向模型展示一篇正面评论，我们告诉它：“地图上的这个区域是‘正面’的。” 模型随后可以将这个概念推广到该区域内的所有其他词和文档。这种强大的[半监督学习](@article_id:640715)策略让我们能够利用海量的未标注数据来理解问题的底层结构，只需要极少的监督来为我们的最终分类器定向 [@problem_id:3162602]。

当然，并非所有地图都是生而平等的。想象一下，你是一位[计算经济学](@article_id:301366)家，试图从公司新闻稿的文本中预测股市动向。金融语言高度专业化。一个在网页文本上训练的通用[嵌入](@article_id:311541)模型可能对“amortization”（摊销）没有好的表示，或者可能会混淆“interest”的金融含义（利息）和与爱好相关的含义（兴趣）。此外，像 Word2Vec 这样的简单模型为每个词提供一个静态的、单一的向量。但像 BERT 这样的模型会生成*上下文*[嵌入](@article_id:311541)——“interest”的向量会根据它所在的句子而改变。哪种方法最好？如果你有一个庞大的、特定领域的数​​据集，你可以从头开始训练一个大模型。但对于一个更现实的、较小的标注数据集，一个常见且高效的策略是，采用一个大型的、[预训练](@article_id:638349)的上下文模型（如 BERT），并将其用作一个“冻结的”[特征提取器](@article_id:641630)。你不需要更新 BERT 中数以百万计的参数，这样做有在小数据集上过拟合的风险；你只需将你的金融文本输入 BERT，以获得复杂的、上下文感知的文档[嵌入](@article_id:311541)。然后，你再在这些特征之上训练一个更简单、更小的分类器。这种混合方法让你两全其美：既获得了大型[预训练](@article_id:638349)模型的深度上下文理解能力，又拥有一个针对你特定问题和数据限制的稳健、高效的训练过程 [@problem_id:2387244]。

### 上下文的普遍语法

当我们意识到“词”和“上下文”是抽象概念时，真正的魔力才开始显现。任何出现在结构化上下文中的东西都可以被映射。[分布假说](@article_id:638229)不仅仅关乎人类语言；它是发现任何复杂系统“语言”的原则。

思考一下计算机编程的语言。编程语言的符号——`list`、`append`、`string`、`concat`、`len`、`size`——是我们的新词汇。我们可以训练一个 Word2Vec 模型，不是在英语句子上，而是在源代码片段上。会发生什么？模型发现“词”`len`和`size`在非常相似的上下文中使用（例如，获取集合的长度），因此它给它们分配了非常相似的向量，捕捉了它们的语义等价性。更引人注目的是，模型学会了著名的向量类比。就像我们可以计算 $v_{\text{king}} - v_{\text{man}} + v_{\text{woman}}$ 并发现结果接近 $v_{\text{queen}}$ 一样，我们也可以对代码做同样的事情。如果我们计算 $v_{\text{append}} - v_{\text{list}} + v_{\text{string}}$，得到的向量将与 $v_{\text{concat}}$ 的向量惊人地接近。模型在没有任何明确指令的情况下，学会了“append 是列表的操作，它类似于字符串的 concat 操作”。它捕捉了这些符号在各自系统中的抽象功能角色 [@problem_id:3200023]。

这个原则从符号语言延伸到了视觉世界。想象一张图片是“词”的集合，其中每个“词”是一个小的图像块。一个图像块的“上下文”是什么？当然是它周围的图像块！一块绿草的图像块经常出现在蓝天或棕色树皮的图像块附近。我们可以从数百万张图片中构建一个[共现矩阵](@article_id:639535)，计算“天空块”出现在“草地块”旁边的频率，依此类推。通过一个类似 GloVe 的[算法](@article_id:331821)处理这个矩阵，我们可以为每种类型的图像块学习一个[嵌入](@article_id:311541)。这些[嵌入](@article_id:311541)代表什么？具有相似视觉属性和相似上下文的图像块——换句话说，相似的纹理——最终会得到相似的向量。模型在从未被告知什么是“纹理”的情况下，学会了将它们分组，为图像识别和分析提供了强大的基础，这也是现代自监督计算机视觉的核心思想 [@problem_id:3130208]。

同样的逻辑也适用于社会结构。让我们把社交网络中的用户视为“词”。一次互动，比如一个用户向另一个用户发送消息，构成了一次“共现”。我们可以将 GloVe [算法](@article_id:331821)应用于一个巨大的用户-用户互动矩阵。由此产生的用户[嵌入](@article_id:311541)将代表什么？处于同一社群的用户倾向于与社群内成员互动更多，而不是与外部人员。他们共享相似的“上下文”——他们交谈的人群。因此，他们的[嵌入](@article_id:311541)将在[向量空间](@article_id:297288)中自然地聚集在一起。通过简单地观察这些学到的用户向量的几何结构，我们就可以识别社群，找到核心影响者，并理解网络隐藏的社会结构 [@problem_id:3130223]。

### 连接世界与扩展知识

这些[向量空间](@article_id:297288)地图不仅仅帮助我们分析单个系统；它们还允许我们在不同世界之间建立桥梁，并对我们从未见过的事物做出智能预测。

最深刻的应用之一是无监督机器翻译。每种语言都提供了一种谈论世界的方式。虽然词语不同，但底层的概念——`狗`、`猫`、`爱`、`正义`——在很大程度上是普适的。这意味着英语词云的语义“形状”应该与西班牙语词云的形状大致相同；一个只是另一个在高维空间中的旋转。令人惊讶的事实是，我们可以在*不使用任何词典*的情况下找到这种旋转。通过独立分析每个[嵌入空间](@article_id:641450)的统计特性（特别是其[协方差](@article_id:312296)的主轴），我们可以计算出将西班牙语词云对齐到英语词云上的最优[正交变换](@article_id:316060)矩阵 $M$。这使我们能够通过找到一个西班牙语词的向量，乘以 $M$，然后找到最近的英语词向量来翻译它。这就像对齐两个形状相同但在夜空中方向不同的星座 [@problem_id:3182927]。

这种在空间之间进行映射的能力也让我们能够解决“零样本”问题：如何处理训练数据中未出现的新项目或类别？想象一个[推荐系统](@article_id:351916)，它已经为数千部电影学习了[嵌入](@article_id:311541)。一部全新的电影上映了。你如何将它放置在地图上？答案是使用[元数据](@article_id:339193)。我们可以分别学习一个从文本[嵌入空间](@article_id:641450)到电影[嵌入空间](@article_id:641450)的映射。对于任何现有电影，我们都有其文本描述和其学习到的电影[嵌入](@article_id:311541)。我们可以训练一个线性模型，用前者预测后者。现在，当新电影上映时，我们只需取其文本简介，将其[嵌入](@article_id:311541)，然后使用我们学到的对齐图将其投影到电影[嵌入空间](@article_id:641450)中。我们就为一个前所未见的项目预测了一个位置，从而可以立即开始向喜欢该几何邻域内电影的用户推荐它 [@problem-id:3121759]。

也许最激动人心的前沿领域是在自然科学中。在[计算生物学](@article_id:307404)中，细胞中的蛋白质形成一个复杂的[蛋白质-蛋白质相互作用](@article_id:335218) (PPI) 网络。我们可以将蛋白质视为“词”，将其物理相互作用视为“共现”。利用[图神经网络](@article_id:297304)，我们可以根据这种网络结构为每个蛋白质学习一个向量[嵌入](@article_id:311541)。但这些[嵌入](@article_id:311541)在生物学上有意义吗？这是一个评估问题。我们可以“探测”这个学习到的空间，看看生物学知识是否已经浮现。例如，我们可以检查执行相同生物学功能（用相同的[基因本体论](@article_id:338364)术语标注）或位于相同细胞区室的蛋白质是否在[嵌入空间](@article_id:641450)中彼此靠近。我们可以使用多种技术来量化这一点：训练一个简单的 k-近邻分类器，从蛋白质的向量预测其功能；使用像调整[互信息](@article_id:299166)这样的聚类指标，查看这些向量是否形成与已知区室匹配的簇；或者将其构建为一个检索任务，来衡量一个蛋白质在[向量空间](@article_id:297288)中的邻居是否也是它在细胞中的伙伴。当这些测试成功时，就证实了我们的[嵌入](@article_id:311541)纯粹从相互作用图的连通性中捕捉到了深层的生物学原理 [@problem_id:2406450]。

最后，让我们进行最后一次抽象的飞跃。考虑一个城市地图或道路网络。地点（[交叉](@article_id:315017)口）是我们的“词”。它们之间的路线是我们的“上下文”。我们可以构建一个[共现矩阵](@article_id:639535)，其中一个条目反映了连接两个地点的多步路径的数量。由此，我们可以为地图上的每个位置推导出[嵌入](@article_id:311541)。这些[嵌入](@article_id:311541)捕捉到了什么？它们捕捉了一个地点在[网络拓扑](@article_id:301848)中的*功能角色*。一个中心枢纽，有许多路径贯穿其中，将拥有一组丰富多样的“上下文”，从而导致一个具有大模长的[嵌入](@article_id:311541)。一条死胡同，只与极少数其他地方相连，将有一个稀疏的上下文和一个小模长的[嵌入](@article_id:311541)。我们可以通过检查节点的[图论](@article_id:301242)度与其[嵌入](@article_id:311541)范数之间的相关性，或通过使用[嵌入](@article_id:311541)成功地将节点分类为“枢纽”或“死胡同”来验证这一点。[嵌入空间](@article_id:641450)的抽象几何结构反映了物理世界的具体拓扑结构 [@problem_id:3182914]。

从语言学到金融学，从[计算机视觉](@article_id:298749)到社会科学，从分子生物学到城市规划，从上下文中学习的原则提供了一个统一而强大的视角。通过将符号关系转化为几何关系，我们开启了一种新的思维、推理和发现我们周围世界结构的方式。这些意义地图已经将我们带到了不可思议的地方，而这段旅程还远未结束。