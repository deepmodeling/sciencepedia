## 引言
我们用来解释世界的每一种工具，从简单的透镜到复杂的统计模型，都有其固有的属性和偏好；没有哪个模型是真正的“一张白纸”。几十年来，科学家和工程师们已经理解并驾驭了这些局限，例如统计学中经典的[偏差-方差权衡](@article_id:299270)。但我们最先进、最灵活的工具——[深度神经网络](@article_id:640465)又如何呢？这些强大的[函数逼近](@article_id:301770)器似乎终于可以摆脱此类束缚。本文正是要探讨这个问题，并揭示它们同样受到一种深刻的内在偏好所支配，这种现象被称为**谱偏见**。本文将深入探讨这一迷人现象，探索神经网络为何以及如何在处理复杂细节之前倾向于学习简单、平滑的模式。我们将首先揭示什么是谱偏见、其在神经网络内部的运作机制以及控制它的巧妙策略。随后，我们将拓宽视野，将现代机器学习中的谱偏见与其在经典信号处理中的深厚根源联系起来，并展示其在计算物理学、神经科学等领域中的深远影响。

## 原理与机制

### 一个普遍真理：没有模型是一张白纸

想象一位有抱负的音乐家正在学习一首复杂的乐曲。他们首先掌握的是什么？几乎可以肯定，他们会先学会主旋律，即构成歌曲骨架的缓慢、基础的曲调。只有在后期经过大量练习后，他们才能掌握快速的颤音、急速的琶音以及那些错综复杂的高频细节。艺术家画肖像时也是如此——首先是面部的宽泛轮廓，然后是鼻子的曲线，直到最后才描绘睫毛的纤细线条和眼中的闪光。

这种从简单到复杂、从低频到高频的演进过程，并不仅仅是人类的特质。事实证明，这是我们如何建模世界的一个深刻而基本的原则。我们用来分析信号或数据集的任何方法，无论多么复杂，都带有其固有的“个性”，即其自身的偏好或偏见。它从来都不是一张真正的白纸。

几十年来，从事**信号处理**的工程师和科学家们对这些权衡已经非常熟悉。以 Welch 方法为例，这是一种用于观察信号“[频谱](@article_id:340514)”（即信号所包含的频率集合）的经典技术。为此，我们可以将信号切成许多短段，并对它们各自的[频谱](@article_id:340514)进行平均。这样做可以得到一幅非常平滑、稳定的图像，但却模糊了细节；我们可能会错过两个非常接近的频率。这是一种**高偏差、低方差**的估计。或者，我们可以分析一个单一的长段。这能提供一幅更清晰、分辨率更高的图像，可以区分相近的频率（低偏差），但它更容易受到噪声和随机波动的影响（高方差）([@problem_id:2428993])。这就是著名的**偏差-方差权衡**，[统计估计](@article_id:333732)的基石之一。你可以得到一幅模糊但稳定的图像，或是一幅清晰但充满噪声的图像。想同时获得两者从根本上是困难的。

此外，还有另一种权衡。当我们分析一个信号时，我们只能在有限的时间内通过一个“窗口”来观察它。一个简单、边缘锐利的矩形窗能提供最佳的[频率分辨率](@article_id:303675)，但它存在一个严重的问题，称为**[频谱泄漏](@article_id:300967)**：来自强频率的能量会“泄漏”出去，污染我们试图观察微弱频率的[频谱](@article_id:340514)部分。我们可以使用一个更平滑的窗口，比如一个能平缓淡入淡出的 Tukey 窗。这能显著减少泄漏，但代价是它会加宽主要的频率峰，从而降低我们的分辨率 ([@problem_id:2428977])。这就是**偏差-泄漏权衡**。

这些权衡并非缺陷，而是数学的基本属性。它们迫使我们做出选择。我们想要高分辨率还是低噪声？我们想要抑制泄漏还是保持锐度？在选择不同的[信号建模](@article_id:360856)[算法](@article_id:331821)时，例如用于[自回归模型](@article_id:368525)的 Yule-Walker 和 Burg 方法，也存在同样的困境。在处理短数据记录时，一种方法通常提供稳定但模糊（高偏差）的[频谱](@article_id:340514)，而另一种则提供清晰但可能充满噪声（高方差）的结果 ([@problem_id:2889645], [@problem_id:2853150])。

这引出了一个有趣的问题。现代[神经网络](@article_id:305336)——这些能够逼近几乎任何函数的、极其复杂和灵活的模型——最终摆脱了这条普遍规则吗？它们是终极的“白板”模型，没有任何内在偏好吗？答案，直到最近才被发现，是一个响亮的“不”。[神经网络](@article_id:305336)有其自身深刻而强烈的偏好，这种现象被称为**谱偏见**。

### 平滑的“阴谋”：[神经网络](@article_id:305336)中的谱偏见

那么，标准神经网络所具有的这种“偏好”究竟是什么呢？当使用常见的[梯度下降法](@article_id:302299)进行训练时，[神经网络](@article_id:305336)表现出一种强烈的倾向：**它学习简单、低频函数比学习复杂、高频函数要容易和快得多。**

让我们通过一个优美而清晰的实验来观察这一现象。假设我们有一个函数，它是两个[正弦波](@article_id:338691)的和：一个是缓慢、平缓的波动 $\sin(x)$，另一个是急速、高频的[抖动](@article_id:326537) $\sin(25x)$。组合后的函数 $u(x) = \sin(x) + \sin(25x)$ 看起来像一个低频波上叠加了快速的“嗡嗡声”。现在，我们不直接向神经网络展示这个函数，而是给它一个谜题。我们提供一个[微分方程](@article_id:327891)，而这个函数恰好是该方程的一个解。然后我们要求网络找出这个谜题的答案 ([@problem_id:2427229])。

当我们开始训练时，会发生什么？在初期，网络的输出几乎完美地匹配了[慢波](@article_id:355945) $\sin(x)$。它完全、甚至可以说是固执地忽略了高频的 $\sin(25x)$ 分量。尽管正确解同等程度地需要这两个部分，但网络固有的偏见使其走上了阻力最小的道路，而这条路就是最平滑、频率最低的那条。只有在它或多或少地完善了低频部分之后，它才会不情愿地开始学习高频细节。

这种偏见有时会如此强烈，以至于完全阻止网络找到正确答案。考虑描述波现象的[亥姆霍兹方程](@article_id:310396)。对于一个高波数 $k$，一个可能的解是像 $u(x) = \sin(kx)$ 这样的高频波。然而，另一个完全有效的数学解是[平凡解](@article_id:315573)：$u(x) = 0$。这是终极的低频函数——一条平坦的直线。当一个标准的 PINN（[物理信息神经网络](@article_id:305653)）被要求解决这个问题时，它的谱偏见是如此强大，以至于它会锁定在平凡解 $u(x)=0$ 上并停滞不前，无法发现那个[振荡](@article_id:331484)的、高频的真相 ([@problem_id:2411070])。这就像一个学生被问到一个难题时，发现说“什么都不知道”比构思复杂的答案更容易。

### 为何如此“懒惰”？偏见背后的机制

这种行为不是我们代码中的错误，也不是数学上的失误。它是我们用来构建网络的要素——平滑的激活函数和[基于梯度的优化](@article_id:348458)——所产生的一种深刻的、涌现的属性。

让我们打个比方。把网络想象成一个极其复杂的音频合成器，其数百万个参数（[权重和偏置](@article_id:639384)）就是那些旋钮和推子。网络内部的[激活函数](@article_id:302225)，通常是像[双曲正切函数](@article_id:638603)（$\tanh$）这样的平滑曲线，就像是基本的[振荡器](@article_id:329170)。稍微转动一个旋钮，往往会在最终的声音中产生非常平滑、宽泛的变化。你可以很容易地让整体音调升高或降低。但要创造一个非常尖锐、高频的尖叫声，一个“纯粹”的高音，你需要以高度协调、精确且常常是相互抵消的方式，调整大量的旋钮。

[梯度下降](@article_id:306363)的工作方式是同时对所有旋钮进行微小调整，朝着最快减少声音总“误差”的方向进行。因为对参数的微小调整自然会产生低频变化，所以最速下降的路径——减少误差的“最简单”方式——几乎总是先修复误差中的低频部分。创造高频分量需要一套更协调、更“昂贵”的参数变化，因此优化器会推迟这项任务。

这种直觉得到了理论家的形式化证明。对于非常宽的[神经网络](@article_id:305336)，训练过程可以用一种叫做**[神经正切核](@article_id:638783)（NTK）**的东西来描述。你可以把这个核看作是定义了网络的“学习规则”。事实证明，网络学习最快的函数对应于这个核的最大[特征值](@article_id:315305)。而对于标准的[网络架构](@article_id:332683)，这些占主导地位的、学习速度快的函数恰恰是低频函数 ([@problem_id:2886083])。所以，我们的观察不仅仅是一个经验上的奇特现象；它是网络基本结构的可预测结果。

### 驾驭偏见：教给旧网络新技巧

如果谱偏见是一种基本属性，这是否意味着我们在试图模拟高频现象时注定会失败？完全不是！既然我们了解了模型的“个性”，我们就可以成为操纵大师，利用巧妙的技巧来抵消这种偏见，或者完全改变游戏规则。

#### 策略1：改变目标
最简单的方法是迫使网络集中注意力。如果优化器因为高频误差对梯度的贡献较小而忽略它们，我们可以人为地放大这些误差。通过使用**频率加权[损失函数](@article_id:638865)**，我们可以对解的高频分量中的误差施加更大的惩罚。这就像告诉我们的音乐家：“我会*非常*仔细地听那些快速的音符，那里的每个错误都算双倍！” 这重新加权了优化景观，使得拟合高频的路径更具吸引力 ([@problem_id:2886083])。

#### 策略2：改变输入
这可能是最优雅、最强大的技巧。网络不擅长学习其输入变量（比如 $x$）的高频函数。问题不在于网络组合事物的能力，而在于它从简单的输入中*创造*高频[抖动](@article_id:326537)的能力。那么，如果我们免费给它这些[抖动](@article_id:326537)呢？

与其只给网络输入变量 $x$，我们可以先将 $x$ 通过一组正弦和余弦函数。我们给网络一整套[特征向量](@article_id:312227)，比如 $[x, \sin(x), \cos(x), \dots, \sin(Mx), \cos(Mx)]$。这被称为使用**傅里叶特征**或**[位置编码](@article_id:639065)**。现在，网络拥有了它可能需要的所有高频构建模块。它的任务不再是*创造*[抖动](@article_id:326537)，而仅仅是学习如何*组合*它们来形成最终的解。这种组合是这些新特征的一个更简单、更平滑的函数——这是一个低频任务，而网络天生就擅长这个！通过改变输入表示，我们将问题与网络的自然偏见对齐了 ([@problem_id:2886083], [@problem_id:2411070])。

#### 策略3：改变网络的“原子”
我们讨论的谱偏见源于使用像 $\tanh$ 这样的[激活函数](@article_id:302225)，其[导数](@article_id:318324)是局部的、“凸起”状的形状。如果我们用不同的“原子”来构建网络呢？我们可以设计一种使用周期性[激活函数](@article_id:302225)（如正弦函数本身）的架构。像这样的网络（例如，SIREN，即正弦表示网络）具有完全不同的[归纳偏置](@article_id:297870)。它们天生就适合表示复杂的[振荡函数](@article_id:318387)及其[导数](@article_id:318324)，有效地颠覆了谱偏见，使它们在学习高频细节方面表现出色 ([@problem_id:2411070])。

最终，谱偏见远非仅仅是一个麻烦。它是一个美丽的例子，说明了简单的局部规则——选择激活函数和[优化算法](@article_id:308254)——如何产生强大、全局性且可预测的[涌现行为](@article_id:298726)。它提醒我们，即使是我们最先进的工具也有其特性。[现代机器学习](@article_id:641462)的艺术和科学不在于寻找一个神话般的“通用”模型，而在于理解我们现有模型的个性，并学会如何与它们进行富有成效的对话。