## 引言
在任何人群中，未来发生不良事件的风险并非均等。无论是在医学还是公共卫生领域，一些个体面临的不良结局概率远高于其他人。面对这种异质性和资源有限的现实，“一刀切”的策略不仅效率低下，而且往往有害，它对健康者过度治疗，却对脆弱者服务不足。本文旨在探讨风险分层这一概念，以应对这一根本性挑战。风险分层是一种使干预强度与风险大小相匹配的系统性方法。在接下来的章节中，您将首先深入了解核心的“原理与机制”，以理解我们如何量化风险、构建预测模型并评估其性能。随后，“应用与跨学科联系”部分将阐明这一强大框架如何在广阔的领域中付诸实践，从指导医生的床边决策到管理整个人群的健康。

## 原理与机制

想象一下，你正站在一条繁忙的街道边，想要过马路。你会闭上眼睛就走吗？当然不会。你会左看看，右看看。你会估算汽车的速度、距离以及天气状况。在几分之一秒内，你的大脑进行了一次复杂的计算，并为过马路这一行为赋予了一个风险等级。你不会以同样的方式对待每一次过马路；一条安静的郊区小巷与高峰时段的六车道高速公路是不同的。你会本能地将你的谨慎程度——即你的干预措施——与风险水平相匹配。

这个简单而日常的行为，蕴含了**风险分层**的精髓。其核心在于，它是一个形式化的系统，用以实现我们凭直觉所做的事情：认识到世界并非整齐划一。在医学和公共卫生领域，人群具有奇妙的，有时也是危险的异质性 [@problem_id:4402518]。不同的人患病、对治疗产生反应或出现并发症的几率各不相同。鉴于我们的资源——医生时间、医院床位、药品、资金——总是有限的，“一刀切”的方法不仅效率低下，而且是灾难的根源。它意味着对健康者进行过度治疗，他们可能因不必要的干预而受到伤害；同时又对病重者治疗不足，他们无法获得急需的护理。

因此，风险分层是使干预强度与风险大小相匹配的艺术与科学。它致力于**相称性**原则。为此，我们必须首先学会看清我们周围那无形的风险图景。

### 未来的语言：绝对风险

为了超越简单的直觉，我们需要一种形式化的语言。风险的语言是概率。未来不良结局的风险，不过是一个条件概率：在已知关于某人的一系列事实或预测因子 $x$ 的情况下，事件 $Y=1$ 发生的可能性。我们将其正式写作 $r(x) = \mathbb{P}(Y=1 \mid x)$ [@problem_id:4402518] [@problem_id:5210086]。这个介于 $0$ 和 $1$ 之间的数值，就是我们试图估计的对象。它是任何风险评估工具的核心输出。

在这里，我们必须对两种经常被混淆的风险进行重要区分：**相对风险**和**绝对风险**。想象一下，你读到一个新闻标题，说吃某种食物会使你患上一种罕见病的“风险加倍”。这听起来很吓人！但这是一个关于相对风险的陈述。如果你原来的基线风险是百万分之一，那么加倍后的风险现在是百万分之二——仍然是微乎其微。虽然相对风险对寻找病因的科学家很有用，但对于个人或政策决策来说，它是一个糟糕的指南。

为此，我们需要**绝对风险**：事件发生在你或像你这样的人身上的实际概率。让我们来看一个关于青少年心理健康的困难但重要的思想实验 [@problem_id:5098395]。假设一家诊所发现，在一组具有三种特定社会心理风险因素的青少年中，未来一年内尝试自杀的绝对风险是 $6$ in $100$（即 $0.06$）。在另一组没有风险因素的青少年中，绝对风险是 $1.5$ in $100$（即 $0.015$）。第一组相对于第二组的相对风险是 $0.06 / 0.015 = 4.0$。他们尝试自杀的可能性是后者的四倍。但对于一家资源仅够帮助几十名青少年的诊所来说，真正重要的是什么？他们必须专注于*绝对风险*最高的人群。对高风险组的 $40$ 名青少年进行干预，其事件发生几率为 $6\%$，预计将比对低风险组的 $40$ 名青少年进行干预（其几率仅为 $1.5\%$）能预防更多的悲剧。当资源稀缺时，绝对风险是指向最大潜在益处的指南针。

### 构建水晶球

那么，我们如何估计这个至关重要的绝对风险呢？我们构建一个预后模型——一种统计学的水晶球。至关重要的是要理解，这些模型是**预测性的**，而非诊断性的 [@problem_id:4507604]。诊断性测试问的是：“你现在是否患有这种疾病？”而预后模型问的是：“在未来一段时间内，比如未来 $10$ 年，你出现特定结局的概率是多少？”它基于从成千上万人的大型、长期观察性研究中学习到的过去模式来预测未来。

这些模型存在一个复杂性谱系 [@problem_id:4737742]：

*   **加性评分：**最简单的方法是简单地计算风险因素的数量。例如，“你有高血压（1分），你吸烟（1分），你有家族史（1分），所以你的分数是3分。”这种方法透明且易于计算。但它带有一个巨大的、通常是错误的假设：每个风险因素对结果的贡献是相等且独立的。

*   **加权线性模型：**一种更复杂的方法，以逻辑回归等统计技术为代表，是让数据告诉我们每个因素的重要性。模型为每个预测因子学习“权重”。年龄可能会获得一个较大的权重，而另一个因素则获得一个较小的权重。这使得模型能更好地逼近真实风险，并且通常能带来更好的性能，前提是我们有足够的高[质量数](@entry_id:142580)据来可靠地学习这些权重。

*   **灵活的[机器学习模型](@entry_id:262335)：**处于前沿的是强大的[机器学习算法](@entry_id:751585)，如神经网络或随机森林。这些模型可以学习预测因子之间极其复杂、非线性的关系和相互作用，而这些是简单模型会错过的。它们可以达到惊人的预测准确性。但这种能力是有代价的：它们通常是“黑箱”，很难理解它们*为什么*会做出某个特定的预测，而且它们对数据有巨大的需求。如果没有仔细、严格的验证，它们极易发生**[过拟合](@entry_id:139093)**——本质上是“记住”了训练数据中的噪声，而不是学习到真正的潜在信号，这可能使它们对新人群的预测变得危险且不可靠。

### 水晶球是清晰还是模糊？

仅仅有一个能输出数字的模型是不够的。我们必须能够判断其质量。它是一扇通往未来的清晰窗户，还是一个扭曲、模糊的乱象？我们在评估预后模型时，会关注两个基本品质：**区分度**和**校准度**。

**区分度**是模型区分不同个体的能力。对于那些最终会出现不良结局的人，模型是否总能给予比那些不会出现的人更高的风险评分？这是一个排序能力的度量。最常用的指标是**受试者工作特征曲线下面积 (AUROC)**。[AUROC](@entry_id:636693)为 $1.0$ 表示完美的排序；[AUROC](@entry_id:636693)为 $0.5$ 则不比抛硬币好。

另一方面，**校准度**则关乎模型的诚实度。它的预测是否名副其实？如果模型对一群人预测有 $20\%$ 的风险，那么结果是否真的在大约 $20\%$ 的人身上发生？一个模型可以有很好的区分度，但校准度很差。例如，它可能完美地将每个人从最高风险排到最低风险，但它给出的概率值可能是系统性错误的——比如，它预测的80%风险实际上对应50%的事件率，而预测的40%风险对应20%的事件率。

哪种品质更重要？这完全取决于你想让模型做什么工作。考虑两种情景：

1.  一家医院每天早上只有有限数量的放射科医生可以阅览乳腺X光片。他们部署了一个人工智能模型，为每张X光片给出一个 $0$ 到 $1$ 之间的恶性概率评分。目标是创建一个工作列表，以便放射科医生首先阅览*最可疑*的病例，以最大化早期发现癌症的数量 [@problem_id:5210086]。对于这种**分诊**或**排序**任务，**区分度为王**。你需要AUROC最高的模型，因为它最擅长将真正高风险的病例排在列表的最前面。绝对概率值的重要性低于排序顺序。

2.  现在想象一个卫生系统想要识别阿片类药物过量的高风险患者，以便让他们参加一个强化的预防项目。他们有两个模型 [@problem_id:4553990]。模型A的AUROC非常出色，为 $0.86$，但校准度很差。模型B的[AUROC](@entry_id:636693)较低，为 $0.77$，但校准度完美。如果政策只是简单地将风险评分最高的前 $10\%$ 的患者纳入项目，那么这个任务同样是关于排序的。模型A凭借其卓越的区分度，是完成这项工作的更好工具，因为它能更准确地识别出未来过量病例最富集的队列，即使其概率数字并非字面意义上的真实值。

### 从数字到行动

一旦我们有了可靠的风险评分，我们就可以采取行动。第一步通常是将连续的风险评分转化为少数几个离散的类别或**分层**：低、中、高风险。但我们如何选择切点呢？这是一个至关重要的步骤，必须以科学的诚实性和透明度来完成。人们很容易进行“数据捞取”——在你的数据集上测试数千个不同的切点，然后只报告那些让你的模型看起来最好的切点。这会导致过于乐观的结果，这些结果在现实世界中是站不住脚的。最佳实践，如TRIPOD报告指南所述，是*事先*根据临床上有意义的、可能改变治疗决策的阈值来定义[切点](@entry_id:172885)，然后在一组完全独立的数据上验证这个固定的规则 [@problem_id:4558943]。

定义了有意义的分层后，我们就可以部署有针对性的、相称的干预措施。考虑一个针对慢性病的筛查项目 [@problem_id:4562520]。如果这种疾病在普通人群中很罕见（患病率低），那么即使是一个好的筛查测试，也会产生大量的[假阳性](@entry_id:635878)。每发现一个真实病例，就会有许多健康人被错误地标记，导致焦虑和不必要的、可能有害的后续检查。然而，如果我们首先对人群进行分层，只向疾病更为常见的高风险层提供筛查，那么情况就会发生巨大变化。**阳性预测值 (PPV)**——即阳性测试结果为[真阳性](@entry_id:637126)的概率——会大幅上升。该项目变得高效、具有成本效益且符合伦理。

风险的概念也可以是多维度的。“风险”并非一个单一、同质的实体。一个患者可能存在需要不同干预措施的不同类型风险 [@problem_id:4386133]。一个初级保健诊所可能会发现：
*   具有高**临床风险**（例如，严重、不稳定的糖尿病）的患者从临床护士的强化管理中获益最多。
*   具有高**医疗使用风险**（例如，因可管理病症频繁就诊急诊室）的患者从护理协调员那里获益，协调员可以确保他们获得及时的预约和随访。
*   具有高**社会风险**（例如，住房不稳定或食品不安全）的患者从社会工作者那里获益，社工可以帮助他们连接到社区资源。

一个成熟的系统不仅仅问“这个患者是高风险吗？”它会问：“这个患者有什么样的风险，针对这个特定工作，什么是正确的工具？”

### 风险是预测，而非个人

最后，我们必须以智慧和谦逊的态度来使用这个强大的工具。风险评分是一个预测，而不是一个永久的标签。它是关于一个可能的未来的陈述，而不是对一个人身份的定义。在像急性髓系[白血病](@entry_id:152725) (AML) 这类疾病的分类中，**诊断实体**（疾病*是什么*，基于其基础生物学和基因构成）和其**风险分层**（疾病可能*做什么*）之间存在着根本的区别 [@problem_id:4346716]。患者的诊断，比如说“带有*NPM1*突变的AML”，是一个稳定的、[分类学](@entry_id:172984)上的标签。然而，他们的风险类别是动态的。它可以根据具体情况而改变，例如存在其他突变或他们对治疗的反应。风险评分是在特定情境下疾病的*一个属性*；它并不重新定义疾病本身。

这种区分具有深远的伦理意义。风险模型是由人构建的，使用的数据来自于一个常常不公正的世界。如果我们不小心，这些模型可能会继承甚至放大社会偏见 [@problem_id:4404024]。例如，在评估医院绩效时，我们必须考虑到它们的**病例组合**——即一些医院照顾的病人病情更重、社会处境更不利。如果一个风险调整模型未能恰当地考虑贫困、无家可归和歧视对健康结果的影响，它可能会不公平地惩罚那些照顾最弱势群体的“安全网”医疗提供者。这会产生避免接收复杂患者的不良激励。一种更公平的方法不是“调整掉”社会风险并假装它不存在，而是**按社会风险分层**。这意味着对不同的社会群体分别报告绩效，使健康差异变得可见，并让整个系统为缩小这些差距负责。

因此，风险分层不仅仅是一项统计工作。它是一个思考不确定性、资源分配和公正的框架。当以科学的严谨性和深刻的伦理责任感来运用它时，它使我们能够将一个需求未分化的世界，转变为一个结构化的图景，在这个图景中，我们可以精确、有力且有目的地运用我们的知识和同情心。

