## 应用与跨学科联系

在了解了采样的基本原理之后，我们可能会感到一丝不安。[奈奎斯特-香农定理](@article_id:306486)如同一座宏伟的门槛，在沙地上划下了一条清晰的界线，告诉我们必须以多快的速度采样才能完美地捕捉信号。进行[欠采样](@article_id:336567)，即冒险低于该速率，似乎是在引向混乱——一种蓄意丢弃信息、并欢迎混叠幻影般失真的行为。然而，故事远比这更微妙和美好。事实证明，在现实世界中，[欠采样](@article_id:336567)不是一种破坏行为，而是一种蕴含深刻工程和科学智慧的行为。它是一种艺术形式。这门艺术在于精确地知道*哪些*信息你可以忽略，而这种知识使我们能够构建更快的技术，创建更稳健的科学模型，甚至回答关于地球生命史的深层问题。现在，让我们来探索这个广阔而令人惊讶的领域，在这里，少做往往能取得更多成就。

### 数字世界的效率：更快地看、听和计算

也许[欠采样](@article_id:336567)最无处不在的应用，就是你此刻正在使用的那个。你屏幕上绚丽的色彩、扬声器里清脆的声音、你互联网连接的速度——所有这些都得益于巧妙的[欠采样](@article_id:336567)。

想一想现代数字视频的奇迹。高清视频是数据的洪流。一张 4K 图像有超过八百万个像素，为了实现平滑的运动，每秒需要60张这样的图像。如果我们为每个像素存储完整的颜色信息——比如红、绿、蓝三个值——数据速率将是天文数字。驯服这头野兽的秘诀是一个将信号处理与人类生物学联系起来的美妙洞见：你的眼睛对不同信息并非一视同仁。人类[视觉系统](@article_id:311698)对亮度（luminance）的敏锐度远高于对色度（chrominance）的敏锐度。我们能出色地分辨灰度图像中的精细细节，但我们对颜色的感知则比较模糊。

视频工程师利用我们生物学上的这个“缺陷”，采用了一种名为**色度子采样**（chroma subsampling）的技术。他们不是为每个像素存储颜色信息，而是为（比如说）每 2x2 的像素块存储颜色信息。亮度信息仍然为每个像素保留，从而保留了我们眼睛渴望的锐利细节，但颜色信息被以4倍的系数进行了[欠采样](@article_id:336567)。结果如何？数据大小大幅减少，而质量几乎没有可感知的损失。你的大脑会愉快地填补空白，用低分辨率的颜色信息来“粉刷”高分辨率的亮度信号。这是空间域中[欠采样](@article_id:336567)的一次精湛应用，完美地契合了[人眼](@article_id:343903)这个非常特殊的“接收器” [@problem_id:1729772]。

类似的巧思也处于现代通信的核心。想象一下，你想收听一个在 100.7 MHz 广播的 FM 电台。朴素地应用奈奎斯特定理会意味着你需要一个每秒采样超过 2 亿次的采样器！这似乎要求高得令人难以置信。但我们知道，实际的信息——音乐和语音——只占据了带宽中一个很窄的片段，也许只有 200 kHz 宽，并以那个高载波频率为中心。两边广阔的[频谱](@article_id:340514)都是空的。我们为什么要浪费精力去采样那些空无一物的东西呢？

这就是**[带通采样](@article_id:336382)**（bandpass sampling）的用武之地。我们可以先使用一些模拟电子技巧，将信号的[频谱](@article_id:340514)从其高频位置下移到以零频率为中心（基带），而不是直接对原始信号进行采样。现在，信号处于一个低频带，我们可以用一个悠闲得多的速率对其进行采样，只要快到足以捕捉其实际信息内容即可。这在精神上是一种[欠采样](@article_id:336567)技术，因为最终的采样率远低于载波频率所暗示的速率。这是一个深刻的论断：我们不必以原始形式对世界进行采样；我们可以在测量之前，先将我们关心的部分转换成一种更方便的“语言” [@problem_id:1750404]。

这些效率的提升并不仅限于采样环节。一旦我们有了一个数字信号并希望降低其[采样率](@article_id:328591)（这个过程称为抽取，decimation），也有一些数学上很优雅的方法可以实现。一个常见的操作是先对信号进行滤波以防止混叠，*然后*通过丢弃样本来进行下采样。例如，要以降 $M$ 倍的系数进行下采样，你需要计算所有滤波后的样本，然后只保留每 $M$ 个样本中的一个。这似乎很浪费——你为保留的每一个样本都进行了 $M$ 次计算！但是，通过一种被称为**[多相分解](@article_id:332955)**（polyphase decomposition）的优美数学编排，我们可以重新[排列](@article_id:296886)方程。这使我们能够*先*进行[下采样](@article_id:329461)，然后在较低的速率下执行一组较小的滤波操作。最终结果在数学上是完全相同的，但[计算成本](@article_id:308397)却被削减了 $M$ 倍。这不是一个近似；这是对[算法](@article_id:331821)的[完美重构](@article_id:323998)，揭示了[多速率系统](@article_id:328689)数学中隐藏的深层效率 [@problem_id:2892166]。

这种由[欠采样](@article_id:336567)驱动的“由粗到精”策略，是计算领域一个反复出现的主题。例如，在[机械工程](@article_id:345308)和计算机视觉中，科学家使用**[数字图像相关](@article_id:378522)（DIC）**来测量材料在应力下的变形。他们拍摄变形前后的照片，然后用[算法](@article_id:331821)试图找出像素块是如何移动的。如果移动幅度很大，[局部搜索](@article_id:640744)[算法](@article_id:331821)很容易迷失方向。解决方案是什么？创建一个**图像金字塔**（image pyramid），即一堆越来越粗糙（下采样）的图像版本。[算法](@article_id:331821)在最粗糙的图像上开始搜索，在那里，大的物理位移表现为仅几个像素的微小、可管理的移动。在这个粗糙层级找到的近似解为下一个更精细的层级提供了一个绝佳的初始猜测，如此循环，直到以极高的精度找到全分辨率的位移。在这种情况下，[欠采样](@article_id:336567)将一个棘手的搜索问题转变为一系列简单的、级联的容易问题 [@problem_id:2630446]。

### 从稀缺中获得准确性：用子采样提升科学的锐度

到目前为止，我们已经将[欠采样](@article_id:336567)视为一种提高效率的工具。我们旅程的下一步更令人惊讶：在许多现代科学领域，刻意减少采样反而能得出更准确、更稳健、更公平的结论。这似乎完全是自相矛盾的，但却是现代[数据科学](@article_id:300658)中最强大的思想之一。

一个引人注目的例子来自机器学习和人工智能领域。最成功的[预测模型](@article_id:383073)之一是**[随机森林](@article_id:307083)**（Random Forest）。它是由许多单个决策树模型组成的“集成”模型。一个单一、复杂的决策树通常是一个糟糕的预测器，因为它会“过拟合”其训练数据；它学习了特定数据集的噪声和怪癖，而不是真实的潜在模式。[随机森林](@article_id:307083)如何解决这个问题？它在略有不同的、经过[欠采样](@article_id:336567)的数据版本上构建每一棵树。更重要的是，在每棵树的每个决策点，它只被允许考虑可用特征的一个随机子样本（例如，在医学背景下是基因）。

通过强迫每棵树在不完整的信息（包括数据点（行）和特征（列））上构建，我们确保了森林中的树是多样化的。它们各自会犯不同类型的错误。当我们对它们的预测进行平均时，这些个别错误往往会相互抵消，留下一个稳定、高度准确的预测，这种预测能更好地泛化到新的、未见过的数据上。在基因组学的高维世界里，我们可能有数千个相关的基因特征，这种[特征子采样](@article_id:304959)至关重要。它能防止所有的树都抓住同样几个明显的预测基因，迫使它们探索更广泛的基因组背景的预测能力，从而使整个模型更加稳健和富有洞察力 [@problem_id:2384471]。

这种利用子采样来提高稳健性和公平性的主题在整个科学界引起了强烈的共鸣。思考一下研究**[奥陶纪](@article_id:356405)生物大辐射事件**的[古生物学](@article_id:312102)家的困境，那是在 4.5 亿多年前地球生命多样性大爆发的时期。他们拥有来自不同地质阶段的化石收藏，但采样投入却极不均衡。一个阶段可能有 1200 个化石出现记录，而相邻的一个阶段只有 80 个。你怎么可能比较它们之间属的原始数量，来判断多样性是上升还是下降了呢？这就像比较在一个小时的后院观鸟中看到的鸟类物种数量，与在为期一个月的亚马逊探险中看到的数量一样。

答案是通过子采样进行[标准化](@article_id:310343)。使用像**稀疏化**（rarefaction）或**法定人数子采样**（quorum subsampling）这样的方法，科学家们在计算上对较大的集合进行下采样，直到它以某种[标准化](@article_id:310343)的方式与较小的集合相匹配——要么通过匹配样本数量，要么通过匹配“完整性”的统计度量。只有这样才能进行公平的比较。在这种情况下，丢弃数据是得出有意义的科学结论的唯一合法途径 [@problem_id:2616930]。完全相同的原理也适用于免疫学中比较不同[测序深度](@article_id:357491)的 T 细胞和 B 细胞受体库的多样性 [@problem_id:2886877]，或在生态学中比较两个不同栖息地的[生物多样性](@article_id:300365)。

此外，子采样已成为计算验证的基石。想象一下，你使用像 UMAP 这样的复杂[算法](@article_id:331821)，根据基因表达来可视化来自肿瘤的数万个单细胞。你看到一个小的、孤立的细胞簇。这是一个真实的、罕见的潜在耐药细胞亚群，还是仅仅是一个随机假象——[算法](@article_id:331821)创造的“海市蜃楼”？为了检验这一点，你可以重复对你的数据集进行子采样，每次随机抽取 80% 的细胞并重新运行分析。如果那个小簇是稳健的，它的成员将在大多数子样本中持续地聚集在一起。如果它是一个假象，它很可能会溶解和分散。这种类似[自助法](@article_id:299286)（bootstrap）的子采样使我们能够为我们的发现分配一个置信度分数，从而将真实的生物信号与[算法](@article_id:331821)噪声区分开来 [@problem_id:1428876] [@problem_id:2665323]。

### 新前沿：[压缩感知](@article_id:376711)

我们的最终目的地或许是所有这些中最具革命性的。这是一个从根本上改写了采样规则的领域：**[压缩感知](@article_id:376711)**（compressive sensing）。[奈奎斯特-香农定理](@article_id:306486)告诉我们，要完美重建一个信号，采样率必须至少是其最高频率的两倍。[压缩感知](@article_id:376711)提出了一个不同的问题：如果信号虽然可能含有高频分量，但同时也是*稀疏的*（sparse），那该怎么办？如果一个信号在某个基中可以用少数几个非零系数来表示，那么它就是稀疏的。一张照片可能在[小波基](@article_id:328903)中是稀疏的；一个和弦在[频域](@article_id:320474)中是稀疏的。

[压缩感知](@article_id:376711)的惊人发现是，如果一个信号已知是稀疏的，它就可以从远低于[奈奎斯特速率](@article_id:325827)的测量次数中被完美重建。这并非是智能地忽略信号的某些部分；而是让每一次测量都变得极其高效，以至于它能捕捉到关于*整个*信号的一点点信息。

关键在于放弃简单的点采样。取而代之的是，我们进行少量随机化的测量——比如[随机投影](@article_id:338386)或加扰的[傅里叶系数](@article_id:305311)。随机性至关重要。它确保了我们的测量方案与信号的稀疏基之间的“非相干性”，使得稀疏信号不可能“躲过”我们的测量。然后，一个巧妙的重建[算法](@article_id:331821)可以解决一个难题：找到与我们所做的少量测量相一致的最稀疏的可能信号。

这个将深奥数学与实际工程联系起来的想法，具有改变世界的应用。一个典型的例子是磁共振成像（MRI）。传统的 MRI 扫描可能是一个漫长而幽闭的过程，因为机器必须缓慢地获取病人身体“频率空间”中的数据。通过使用[压缩感知](@article_id:376711)，我们只需获取该数据的一个小的、随机的子集，仍然可以重建出高质量的图像。这使得扫描速度大大加快，对于儿童、重症患者以及任何觉得这种体验不舒服的人来说都是一个福音。这是我们主题的终极体现：用更少的投入做更多的事，将一个深奥的数学原理转化为切实的人类福祉 [@problem_id:2905658]。

从使你的数字生活成为可能，到提升科学的前沿锐度，再到彻底改变医学成像，[欠采样](@article_id:336567)的原理是一条金线。它教导我们，信息具有结构，通过理解这种结构，我们可以用一种起初看似不可能的优雅和效率来测量世界。它证明了这样一个问题的力量：我们不仅要问“我们*必须*以多快的速度采样？”，还要问“我们*可以*以多慢的速度采样？”。