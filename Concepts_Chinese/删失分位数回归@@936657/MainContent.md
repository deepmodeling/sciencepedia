## 引言
统计分析通常始于“平均值”或均值，这是一个用于概括数据的强大概念。然而，当数据存在偏态或包含极端离群值时，均值可能会产生误导，无法代表“典型”结果。一种更稳健的方法是使用[分位数](@entry_id:178417)（如[中位数](@entry_id:264877)），它可以描述从中心到尾部的整个数据分布。这使得我们能够更深入地理解变量之间的关系。但当我们的数据不完整时，情况又会如何呢？在医学或工程等领域，结果常常是“删失”的；例如，我们可能只知道一名患者存活了*至少*五年，而不知道其确切的生存时间。这种信息缺失对标准分析构成了重大挑战。

本文全面概述了删失[分位数回归](@entry_id:169107)，这是一种旨在解决上述问题的强大工具。在“原理与机制”部分，我们将探讨[分位数回归](@entry_id:169107)背后的精妙理论，理解其如何通过巧妙的估计策略处理删失观测值，并回顾支持它的现代统计学工具包。随后，“应用与跨学科联系”部分将展示该方法在真实场景中的应用，将其与其他统计技术进行比较，并展示其与机器学习前沿的融合。

## 原理与机制

### 超越平均值：[分位数](@entry_id:178417)的世界

在初次接触数据时，我们学到了一个强大而优美简洁的概念：平均值。如果我们想概括一组数字——比如班级里学生的身高、一次考试的成绩——我们会将它们相加然后做除法。平均值，即**均值**，就像是数据的天然[重心](@entry_id:273519)。在统计学中，我们通过称均值为最小化平方差之和的值来将其形式化。这一**最小二乘**原则是许多[统计模型](@entry_id:755400)的基础。

但“平均值”总是我们想要讲述的故事吗？想象一下，你是一家医院的管理者，试图根据患者入院时的化验值来预测其住院时长。你收集数据后发现，大多数患者住院几天，但少数极其复杂的病例会导致住院数周甚至数月。如果你计算平均住院时长，这几个极端的离群值会把这个数字拉高，使你得到一个对大多数患者而言并非“典型”的值。在这种情况下，均值感觉不像是数据的重心，更像一个被极端值操纵的木偶 [@problem_id:4579939]。

这时，一个更稳健的概念应运而生：**[中位数](@entry_id:264877)**。中位数就是中间值——第 50 百分位数。它是一条分界线，恰好一半患者的住院时间比它短，另一半则比它长。与均值不同，中位数不关心住院时间最长的那个病人在医院待了多久，只关心他是否属于前半部分。这使得中位数在处理[偏态](@entry_id:178163)数据时成为一个更稳定、通常也更具代表性的“中心”度量。

但我们为何要止步于中心呢？[中位数](@entry_id:264877)只是一个庞大而强大的家族——**分位数**——中的一员。第 $\tau$ 分位数是一个值，群体中比例为 $\tau$ 的部分位于该值之下。中位数是 $0.5$-分位数。$0.1$-[分位数](@entry_id:178417)是区分最低 10% 观测值与其余部分的点。$0.95$-分位数则标记了前 5% 的阈值。[分位数](@entry_id:178417)不像单一数字那样只给我们一个点，而是为我们提供了分布的全景——它的中心、它的尾部、它的整个轮廓。医生可能不仅关心药物对血压的典型反应，还关心最坏情况下的反应（比如第 95 百分位数），以管理毒性风险 [@problem_id:4981880]。

### 检验[损失函数](@entry_id:136784)的形状：如何找到[分位数](@entry_id:178417)

我们有最小二乘法这个优美的原则来寻找均值。那么，是否存在一个同样精妙的原则来寻找分位数呢？答案是肯定的，而且它简洁得令人惊叹。

回想一下，对于[中位数](@entry_id:264877)，我们可以最小化*绝对*差之和，而不是平方差之和。这赋予了离群值较小的权重。要得到任何其他[分位数](@entry_id:178417)，我们只需倾斜这个[损失函数](@entry_id:136784)。想象一个跷跷板。为了找到中位数（$\tau=0.5$），我们给两边的点相同的权重。为了找到 $0.9$-分位数，我们需要告诉我们的估计过程，低估分位数比高估它要糟糕得多。我们需要在跷跷板的一侧放一个更重的人。

这正是**检验损失**（check loss），有时也称为**弹球损失**（pinball loss）所做的事情 [@problem_id:4831944]。该函数，记为 $\rho_\tau(u)$，是一个非常简单的 V 形函数，其中“V”的两臂斜率不同。
$$ \rho_\tau(u) = u(\tau - \mathbf{1}\{u0\}) $$
这里，$u$ 是残差（实际值与我们估计值之间的差异），$\mathbf{1}\{u0\}$ 是一个指示函数，当残差为负时取 1，否则取 0。

让我们来剖析一下。如果我们低估了（$u > 0$），损失是 $\tau \cdot u$。如果我们高估了（$u  0$），损失是 $(\tau-1) \cdot u = (1-\tau) \cdot |u|$。对于 $0.9$-[分位数](@entry_id:178417)，我们以 $0.9$ 的斜率惩罚低估，以 $0.1$ 的斜率惩罚高估。为了最小化所有数据点的总损失，我们的估计值必须策略性地将自己定位在这样一个点上：其下方点的加权拉力与上方点的加权拉力[相平衡](@entry_id:136822)。要达到这种平衡，唯一的办法就是让估计值恰好位于 90% 的数据在其之下、10% 的数据在其之上的那个点。[损失函数](@entry_id:136784)简单的非对称“检验”形状迫使解成为该[分位数](@entry_id:178417)。这是一个几何形状与一个基本统计思想之间深刻而优美的联系。

### 穿过[分位数](@entry_id:178417)的线：回归思想

现在，让我们迈出合乎逻辑的下一步。我们不只想知道所有人的空腹血糖的第 90 百[分位数](@entry_id:178417)；我们想知道第 90 百分位数如何随患者的年龄、性别和治疗方法而变化 [@problem_id:4981810]。这就引出了**[分位数回归](@entry_id:169107)**。我们为条件[分位数](@entry_id:178417)提出了一个模型，通常是线性的：
$$ Q_Y(\tau \mid X) = \beta_0(\tau) + \beta_1(\tau)X_1 + \beta_2(\tau)X_2 + \dots = X^\top\beta(\tau) $$
注意这个关键的符号：系数，即 $\beta$，本身就是 $\tau$ 的函数。这是[分位数回归](@entry_id:169107)的超能力。系数 $\beta_{\text{age}}(0.9)$ 告诉我们，年龄每增加一岁，结果的第 90 百[分位数](@entry_id:178417)会变化多少。这可能与 $\beta_{\text{age}}(0.5)$（即对[中位数](@entry_id:264877)的影响）截然不同。一种新疗法可能在降低生物标志物的危险高值方面有显著效果（对第 90 或 95 百分位数有很大的负向影响），但对其典型值（[中位数](@entry_id:264877)）的影响可以忽略不计。[均值回归](@entry_id:164380)会产生一个单一的年龄系数，将这些效应平均掉，从而错过了故事中最重要的部分。[分位数回归](@entry_id:169107)使我们能够对整个[条件分布](@entry_id:138367)进行建模，揭示出数据关系中更丰富、更细致的画面。

这些模型中的系数具有既直观又精妙的特性。例如，如果你改变一个协变量的单位，比如从以年为单位的年龄变为以十年为单位的年龄（除以 10），相应的系数就会简单地乘以 10，以保持预测不变。解释是不变的。平移一个协变量（例如，围绕其均值进行中心化）会改变截距，截距现在方便地代表了“平均”受试者的条件[分位数](@entry_id:178417)，但斜率系数保持不变 [@problem_id:4831944]。这些等变和不变性质不仅仅是数学上的奇特性质；它们确保了我们的科学解释是稳定的、一致的，无论我们如何编码变量。

当然，拟合这些模型需要计算机。虽然检验[损失函数](@entry_id:136784)是[凸函数](@entry_id:143075)，确保可以找到唯一的最小值，但它在零点处的尖锐“[拐点](@entry_id:144929)”意味着我们不能使用简单的导数微积分。取而代之的是，我们采用线性规划和[数值优化](@entry_id:138060)中的强大算法。在这里我们再次看到了一个实际的联系：像对协变量进行中心化和缩放这样为[可解释性](@entry_id:637759)而做的简单变换，也极大地提高了这些算法的[数值稳定性](@entry_id:146550)，使它们能够更快、更可靠地收敛 [@problem_id:4981862]。

### 被遮蔽的真相：处理删失数据

我们现在来到了本文的核心。在许多真实世界的研究中，尤其是在医学领域，我们并不总能观察到我们关心的结果。想象一项临床试验，研究一种新疗法后癌症进展的时间。该研究持续五年。一些患者的癌症会在研究期间进展，我们记录了他们确切的进展时间。但对于在研究结束时仍然无进展的患者该怎么办呢？我们不知道他们真正的进展时间。我们只知道这个时间*至少*是五年。这被称为**右删失**。真实值被研究的结束或患者的退出所遮蔽，对我们来说是隐藏的 [@problem_id:4981842]。

这是一个深刻的挑战。如果我们简单地忽略被删失的患者，我们的分析就会有偏差。我们将会丢掉那些结果最好的人，从而系统地低估了治疗的真实有效时间。如果我们把删失时间当作事件时间，我们又是在系统地缩短真实时间，同样会导致有偏差的结果。我们试图为一个我们无法完全看到的变量 $T$ 的[分位数](@entry_id:178417)建模。我们怎么可能瞄准一个隐藏的目标呢？

这正是现代统计学真正独创性的闪光之处。主要有两条思路，两者都同样精彩。

#### 策略一：调整模型

第一种方法由 James Powell 首创，它提出了一个聪明的问题：如果我们无法对真实的、潜在的事件时间 $T$ 的分位数进行建模，我们能否转而为*观测*时间 $Y = \min(T, C)$ 的[分位数](@entry_id:178417)找到一个模型，其中 $C$ 是删失时间？

让我们来思考一下。$Y$ 的分位数必须同时依赖于患者的协变量 $X$ 和他们特定的删失时间 $C$。如果事件时间的真实 $\tau$-分位数 $Q_T(\tau \mid X)$ 小于删失时间 $C$，那么事件很可能在删失前发生，观测时间的[分位数](@entry_id:178417)就将是真实时间的[分位数](@entry_id:178417)。但是，如果真实分位数*大于*删失时间，事件则很可能在删失后发生。在这种情况下，观测时间的第 $\tau$ 分位数会被“卡”在删失时间 $C$ 上。

将这些综合起来，便得出了一个非凡的结果。观测时间的条件 $\tau$-分位数就是真实条件[分位数](@entry_id:178417)和删失时间的*最小值*：
$$ Q_Y(\tau \mid X, C) = \min\big(X^\top\beta(\tau), C\big) $$
这是一个突破！我们找到了我们想要的隐藏量（$X^\top\beta(\tau)$）和我们拥有的数据（$Y$ 和 $C$）之间的明确数学关系。我们现在可以使用检验损失来建立我们的最小化问题，但不是对 $Y$ 拟合一个线性模型，而是拟合这个新的、非线性的“最小值”模型。这就是 **Powell 删失[分位数回归](@entry_id:169107)估计量**的精髓。通过改变*模型的形式*以反映删失机制，我们可以恢复对真实、未删失世界参数的无偏估计 [@problem_id:4981842]。

#### 策略二：重新加权证据

第二种方法在哲学上完全不同。它不是调整模型，而是调整数据。这种方法被称为**逆删失概率加权 (IPCW)**。

其核心思想是，未删失的患者集合——即我们能看到真实事件时间的那些患者——是一个有偏的样本。真实事件时间较长的患者有更长的时间面临被删失的风险，因此他们在我们的完整案例数据中代表性不足。IPCW 方法通过对未删失的观测值进行上调权重来纠正这一点，让它们“代表”那些相似但因被删失而从完整分析中丢失的对应个体。

我们应该重新加权多少呢？一个事件时间为 $t$ 的未删失患者被观察到，仅仅是因为他们的删失时间 $C$ 大于或等于 $t$。这种情况发生的概率是删失过程的生存概率，$G(t \mid X) = P(C \ge t \mid X)$。为了校正[选择偏差](@entry_id:172119)，我们用这个概率的倒数 $1/G(t \mid X)$ 来加权这位患者对分析的贡献。一个很可能被删失但最终没有被删失的观测值会获得很大的权重，因为它代表了许多没有那么幸运的其他人。

我们首先从数据中估计删失分布（将删失视为“事件”），然后运行一个标准的[分位数回归](@entry_id:169107)，但其中每个未删失患者的贡献都按其被观察到的概率的倒数进行加权。这就好像我们创建了一个“伪群体”，在这个群体中，通过仔细的重新加权，删失偏差已被统计学上消除 [@problem_id:4981820]。

### 当遮蔽不再随机：统计学的坦诚

Powell 估计量和 IPCW 都依赖于一个关键假设：删失是“非信息性的”。这意味着，在给定协变量的情况下，导致删失的机制与我们正在研究的结果无关。但如果这个假设是错误的呢？如果那些感觉治疗无效（因此进展时间会更短）的患者更有可能退出研究怎么办？这就是**信息性删失**，它破坏了我们标准方法的有效性。

一切都完了吗？不。这正是统计学展示其对知识坦诚承诺的地方。如果我们不能做出足够强的假设来获得单一的[点估计](@entry_id:174544)，我们仍然可以确定真实答案必须位于的一个范围或**界限**之内。

逻辑简单而强大。让我们看一下[累积分布函数 (CDF)](@entry_id:264700)，$F_Y(y) = P(Y \le y)$。真实的 CDF 必须至少与我们仅使用未删失事件计算出的 CDF 一样大（因为我们遗漏了一些事件）。同时，真实的 CDF 不能大于未删失事件的 CDF *加上*到那个时间点已被删失的所有个体的比例（这是最极端的情况，即所有被删失的个体都会立即发生事件）。这个逻辑为真实的、未知的 CDF 提供了一个严格的下界和上界。通过对这些界限求逆，我们可以得到任何给定分位数的一系列可[能值](@entry_id:187992)。这个范围被称为**识别区间** [@problem_id:4831922]。它诚实地告诉我们数据能说什么，不能说什么。这个区间可能很宽，但它是在没有不合理假设的情况下对我们知识的严谨陈述。

### 更广阔的视角：现代[分位数回归](@entry_id:169107)工具箱

删失[分位数回归](@entry_id:169107)不是一个孤立的技巧，而是现代统计思维广阔互联网络的一部分。它与其他强大的思想无缝集成：

*   **推断**：得到 $\beta(\tau)$ 的估计是一回事，但我们对此有多大把握？为了构建[置信区间](@entry_id:138194)，我们可以使用**[自助法](@entry_id:139281)**（bootstrap），这是一种巧妙的重抽样技术，我们通过从自己的数据中抽样来创建新的数据集。通过在数千个这样的自助数据集上拟合模型，我们可以看到我们的估计值变化了多少，从而直接度量它们的[统计不确定性](@entry_id:267672)。关键是，像 xy-pair bootstrap 这样的方法即使在[分位数回归](@entry_id:169107)旨在处理的复杂的、异方差的误差结构下也有效 [@problem_id:4981880]。

*   **[高维数据](@entry_id:138874)**：如果我们有成百上千个潜在的预测变量，比如一整套基因标记，该怎么办？我们可以将[分位数回归](@entry_id:169107)与**[组套索](@entry_id:170889)**（Group [LASSO](@entry_id:751223)）等[正则化技术](@entry_id:261393)相结合。这使得模型能够自动选择与特定[分位数](@entry_id:178417)相关的整个预测变量组，同时将不相关组的影响精确地压缩到零。我们可能会发现，一组生物标志物预测了住院时间的[中位数](@entry_id:264877)，而另一组完全不同的生物标志物预测了第 90 百[分位数](@entry_id:178417)，从而提供了宝贵的科学见解 [@problem_id:4981878]。

*   **贝叶斯视角**：我们也可以从贝叶斯的角度来处理整个问题。在这里，我们使用非对称[拉普拉斯分布](@entry_id:266437)作为形式上的似然函数，它在数学上等价于检验损失。这使我们能够融入先验知识——例如，来自临床专家的知识，他们对治疗效果对上尾[分位数](@entry_id:178417)的可能大小有自己的信念。我们可以将这种信念编码为 $\beta(\tau)$ 系数上的先验分布。然后，贝叶斯推断将这种先验信念与来自数据的证据相结合，产生我们更新后信念的完整后验分布 [@problem_id:4981879]。

从一个简单直观的[损失函数](@entry_id:136784)到一个用于处理删失数据、高维预测变量和不同哲学框架的复杂工具，删失[分位数回归](@entry_id:169107)体现了现代统计学的力量与美。这是一段始于一个简单问题——“如果平均值还不够怎么办？”——并最终获得一种深刻能力的旅程，即观察和模拟这个世界所有丰富多样的细节。

