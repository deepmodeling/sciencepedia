## 引言
机器学习与物理学这两个曾被视为截然不同的领域，如今正以一种伙伴关系交汇融合，重塑着科学发现的图景。当我们构建能够从数据中“学习”的机器时，一个引人-入胜的问题随之出现：在物理学家眼中，这个学习过程是怎样的？答案是，神经网络内部的抽象运算，实际上遵循着我们所熟悉的能量、对称性和运动等原理。这种融合带来了双向的机遇：物理学为理解和改进机器学习提供了强大的框架，而机器学习则为解决物理学中一些最复杂的问题提供了前所未有的工具包。

本文深入探讨了这种深刻的跨学科联系，试图弥合抽象[算法](@article_id:331821)与可感知的物理现实之间的鸿沟。我们将探索物理学的语言如何揭开机器学习内部工作的神秘面纱，以及这些智能[算法](@article_id:331821)又是如何反过来成为科学探究不可或缺的工具。在接下来的章节中，你将发现这两个领域之间深刻的相似之处。我们首先将踏上“原理与机制”的旅程，揭示支撑学习模型训练与架构的、那些令人惊讶的物理定律。随后，在“应用与跨学科联系”部分，我们将看到这些概念的实际应用，探索这种协同作用如何在全球的实验室和天文台中加速科学发现。

## 原理与机制

“机器学习”这个概念很奇特。我们给机器输入数据，它就“学会”执行一项任务——对星系进行分类、预测天气或发现新药物。我们使用的语言借鉴于自身：我们谈论“训练”、“经验”和“[神经网络](@article_id:305336)”。但那个硅盒内部究竟发生了什么？如果你用物理学家的眼光仔细观察，你会发现一些惊人的事情。抽象的学习过程终究没有那么抽象。它是一场由我们从物理世界中熟知的原理——运动、能量、对称性和守恒——所支配的数字之舞。在本章中，我们将揭开幕布，看一看当我们教导一台机器时，我们实际上是在引导一个宇宙的诞生，一个拥有其自身物理定律的宇宙。

### 跨界的共同语言

在探索这个新宇宙之前，我们需要学习它的语言。幸运的是，这门语言物理学家已经使用了几个世纪。其核心是**[张量](@article_id:321604)** (tensor)，你可能以为它只是矩阵或向量，但实际上它远比这更深刻。可以把标量看作一个点（0阶），向量看作一条带方向的线（1阶），矩阵看作一个平面内的变换（2阶）。[张量](@article_id:321604)就是这个概念在任意维度上的推广。它们是描述物理定律的基础构件，同时也是神经网络内部运算的基础构件。

[张量](@article_id:321604)并非孤立存在，它们会相互作用。其相互作用的方式是通过一种称为**[张量缩并](@article_id:323965)**(contraction)的运算，这是对一种结构化乘法和求和运算的别称。想象有两个[张量](@article_id:321604)，我们称之为 $A_{ij}$ 和 $B_{kl}$。你可以通过对一个共享的指标求和来将它们“粘合”在一起。例如，在表达式 $S = \sum_{i,j,k,l} A_{ij} B_{kl} \delta_{jk} \delta_{li}$ 中，克罗内克(Kronecker)delta符号 $\delta$ 充当了胶水，它强制某些指标相等，从而将四个[指标缩并](@article_id:359812)成一个单一的数字，即一个标量 [@problem_id:1543553]。被求和的指标称为**闭合**或**内部**指标——它们是维系整个结构的纽带。剩下的指标则是**开放**或**外部**指标，它们定义了新的组合[张量](@article_id:321604)的形状 [@problem_id:1543573]。一个简单的[矩阵乘法](@article_id:316443) $C_{ik} = \sum_j A_{ij} B_{jk}$，其实就是一种[张量缩并](@article_id:323965)，其中内部指标 $j$ 是闭合的，而外部指标 $i$ 和 $k$ 保持开放，形成了新的矩阵 $C$。

这种[张量](@article_id:321604)语言与几何学语法并行使用。在物理学和机器学习中，我们都热衷于变换——旋转物体、改变[坐标系](@article_id:316753)，或为数据找到最自然的“视角”。最“纯净”的变换是那些不拉伸或扭曲空间的变换，它们由**[标准正交矩阵](@article_id:348450)** (orthonormal matrices)表示。这些矩阵（我们称之为 $Q$）的列向量是相互垂直的单位向量。它们代表了纯粹的旋转（和反射）。这类矩阵的一个显著特性是其转置等于其逆，即乘积 $Q^T Q$ 恒为[单位矩阵](@article_id:317130) $I$ [@problem_id:1375841]。这一完美的数学性质确保了当我们旋转视角时，系统的基本几何关系得以保持——这一原则对于绘制[卫星轨道](@article_id:353829)和分析高维数据集同样至关重要。

### 作为物理过程的优化

机器“学习”的真正含义是什么？在大多数情况下，这意味着找到一组能够最小化**损失函数**(loss function)的内部参数（或称“权重”）。这个函数是衡量机器预测与真实值之间差距的数学度量。损失越低，模型越好。因此，训练模型的整个宏大工程就是一个优化问题：在某个高维参数空间中，找到使损失函数达到绝对最小值的那个点。

这正是与物理学的第一个美妙类比出现的地方。寻找一个函数的最小值问题，等同于寻找一个地形景观中的最低点。在物理学中，我们称这个景观为**[势能面](@article_id:307856)**(potential energy surface)。让我们具体一点。假设你想求解一个简单的线性方程组 $Ax=b$。这是一个标准的代数问题。但你可以完全换一种方式来想它。这个代数问题完全等同于寻找一个二次函数（一个由 $f(x) = \frac{1}{2}x^{\mathsf{T}}Ax - b^{\mathsf{T}}x$ 给出的多维碗状[曲面](@article_id:331153)）的唯一最小值 [@problem_id:2211040]。那个碗的底部就是解 $x$。求解方程这个抽象问题，变成了一个寻找最低势能点的物理问题。

我们如何找到那个最低点呢？我们可以尝试用解析方法求解，但对于现代神经网络极其复杂的景观而言，这是不可能的。所以，我们采用自然界的方式：我们下山。想象在这个景观上放一个弹珠，它会沿着最陡峭的方向滚下去。这个方向由景观梯度的负值给出，即 $-\nabla U$。这就是**梯度下降**(gradient descent)背后的思想，它是机器学习的主力[算法](@article_id:331821)。我们从参数景观中的某个随机点 $\mathbf{y}_0$ 开始，向下走一小步：
$$
\mathbf{y}_{1} = \mathbf{y}_{0} - \eta \nabla U(\mathbf{y}_{0})
$$
这里，$\eta$ 是一个称为**学习率**(learning rate)的小数，它控制着我们的步长。我们重复这个过程，一步一步地，我们的参数“滚”下山，朝着损失函数的某个最小值前进。

现在是关键所在。这个迭代式的离散[算法](@article_id:331821)是物理学家们数百年来所熟知的东西。它不过是对一个连续物理过程的最简单的[数值模拟](@article_id:297538)。描述一个粒子沿[势能面](@article_id:307856)滑下（在所谓的“[过阻尼](@article_id:347221)”极限下，即摩擦力占主导地位）的方程是一个**[梯度流](@article_id:640260)**(gradient flow)：
$$
\frac{d\mathbf{y}}{dt} = -\nabla U(\mathbf{y})
$$
梯度下降[算法](@article_id:331821)其实就是求解该[微分方程](@article_id:327891)的**前向欧拉法**(forward Euler method)，其中学习率 $\eta$ 就是我们的时间步长 $h$ [@problem_id:2172192]。学习*就是*一个物理过程。[损失函数](@article_id:638865)是势能，模型参数是粒子的位置，而训练就是模拟这个粒子在寻求最低能量状态时的运动。

这个类比不仅仅是一种诗意的奇想，它具有深远的实际意义。如果我们的时间步长 $\eta$ 太大，会发生什么？在任何物理模拟中，采用过大的时间步长都会导致数值不稳定——模拟结果可能会剧烈[振荡](@article_id:331484)甚至发散。机器学习也是如此。选择过高的学习率，训练过程就会发散。[算法](@article_id:331821)所采用的离散路径可能会与潜在[梯度流](@article_id:640260)的真实、平滑轨迹发生巨大偏离 [@problem_id:2373875]。因此，调整[学习率](@article_id:300654)等超参数的艺术，被重新定义为确保数值模拟稳定性的科学。

### 运用更丰富的物理学实现更智能的学习

我们那个弹珠滚下山的简单模型是个好的开始，但它描述的是一个处于绝对[零度](@article_id:316692)下的系统。它自身没有能量，所以会陷入它找到的第一个山谷——我们称之为**局部最小值**(local minimum)。但[深度学习](@article_id:302462)的景观是崎岖多山的，充满了无数的山谷。我们如何才能找到那些真正深邃的山谷，即**全局最小值**(global minima)？

让我们问问物理学家：给一个系统加热会发生什么？系统内的粒子会开始晃动和[振荡](@article_id:331484)。这种随机的热运动使它们能够越过能量壁垒。我们也可以对学习过程做同样的事。我们不再是纯粹地向下走，而是在每一步都加上一个小的、随机的“扰动”。这就是**[朗之万动力学](@article_id:302745)**(Langevin dynamics)的核心思想，这是一种模拟有限温度效应的方法。我们的粒子（模型的参数）现在不仅会滚下山，还会随机扩散。这种热扰动使它能够“跳出”浅的局部最小值，继续探索景观中更深的山谷 [@problem_id:2417103]。

这个过程会产生一个显著的后果。经过很长时间后，系统不只是找到一个最小值。它会探索整个景观，并最终达到一个由**玻尔兹曼分布**(Boltzmann distribution) $P(\boldsymbol{\theta}) \propto \exp(-U(\boldsymbol{\theta}) / (k_B T))$ 所描述的[统计平衡](@article_id:323751)。这意味着它大部[分时](@article_id:338112)间会停留在低能量状态，但它也揭示了一种微妙的偏好：系统更青睐*更宽*、更平坦的最小值，而不是狭窄、尖锐的最小值。为什么？因为宽阔的山谷里有更多的“空间”——它具有更高的熵。在机器学习中，这些更宽的最小值通常对应着更稳健、对未见新数据泛化能力更好的模型。[深度学习](@article_id:302462)中最常用的优化器——**[随机梯度下降](@article_id:299582)**(Stochastic Gradient Descent, SGD)——的带噪声更新，可以被解释为对这种有限温度过程的模拟。

当然，理想的情况是景观中没有局部最小值可以陷入——只有一个宏伟的大峡谷。这种理想的景观被称为**凸**(convex)的。在凸[曲面](@article_id:331153)上，*任何*下坡路径都保证能通向唯一的[全局最小值](@article_id:345300) [@problem_id:2164017]。尽管[深度学习](@article_id:302462)的[损失景观](@article_id:639867)很少是凸的，但科学和工程中的许多关键问题都可以被表述为[凸优化](@article_id:297892)问题，这使得我们能够自信地找到唯一的最佳解 [@problem_id:1615207]。

### 拥有物理灵魂的架构

到目前为止，我们一直将物理学作为一种强大的类比和改进训练过程的工具来源。但我们可以将这种结合推向其最终的形态。我们不再仅仅用物理学来指导优化，而是可以将物理定律直接构建到我们学习机器的架构中。我们可以赋予它们一个物理的灵魂。

第一步是教它们关于**对称性**(symmetry)。物理定律是对称的。例如，两个原子之间的力取决于它们之间的距离，而不是它们在空间中的绝对位置或整个系统的朝向。这是一种基本的**[归纳偏置](@article_id:297870)**(inductive bias)——一种关于世界的先验知识。如果我们正在构建一个模型来预测分子系统中的力，它就应该尊重这种对称性。如果我们旋转整个分子，每个原子上的力矢量也应该以完全相同的方式旋转。这种性质称为**[等变性](@article_id:640964)**(equivariance)。一个满足 $\mathcal{F}(\{Q \mathbf{r}_i + \mathbf{t}\}) = \{Q \mathbf{F}_i\}$（其中 $Q$ 是旋转，$\mathbf{t}$ 是平移）的模型，就将这种对称性内建其中了 [@problem_id:2838022]。它不需要浪费时间和数据去学习物理定律在颠倒过来时同样适用。它已经知道了。

我们可以更进一步。一些物理定律不仅仅是对称性，它们是绝对的守恒定律。在任何封闭的物理过程中，能量、动量和[电荷](@article_id:339187)都是守恒的。我们能否构建一台机器，其本质就决定了它必须遵守这些定律？答案是肯定的。我们可以设计一种[神经网络](@article_id:305336)，其内部结构模仿物理学的数学框架，而不是使用能够逼近任何函数的通用[神经网络](@article_id:305336)。例如，**[哈密顿神经网络](@article_id:301139)** (Hamiltonian Neural Network, HNN) 并非直接被训练来预测力，而是被训练来学习一个单一的标量函数——哈密顿量（即总能量），然后它利用[哈密顿运动方程](@article_id:355931)来推导动力学。由于这种结构，它所学习到的能量在其预测的任何轨迹上都*必然*是守恒的。同样，一个基于成对、反对称力（牛顿第三定律）原理构建的相互作用网络，根据其构造，将永远保持总动量守恒 [@problem_id:2410539]。

这是一种[范式](@article_id:329204)转换。我们从使用通用的“黑箱”模型并[期望](@article_id:311378)它们能学会物理，转向设计预先包含了宇宙基本原理的“白箱”模型。它们不仅仅是从数据中学习，它们是从物理定律出发进行推理。它们不仅仅是强大的近似器；它们是微型的、被学习出来的宇宙，等待我们去探索它们的发现。