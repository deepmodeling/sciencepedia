## 引言
现代人工智能系统已展现出从数据中学习的卓越能力，在从医疗诊断到科学发现的各项任务中取得了超越人类的表现。然而，这一成功背后隐藏着一个微妙但关键的漏洞。AI 模型在不懈追求准确率的过程中，会学会通过寻找最简单的路径来“作弊”以获得正确答案。这种现象被称为**捷径学习**（shortcut learning），即模型抓住训练数据中肤浅的、伪造的相关性，而不是掌握问题更深层次的因果原理。这会造成一种危险的能力错觉，构建出的模型在根本上是脆弱且不可信的。

本文旨在解决模型书面性能与其在现实世界中可靠性之间的关键知识鸿沟。文章剖析了捷径学习问题，为理解和识别这种常见的 AI 失效模式提供了一个框架。在接下来的章节中，您将对这一挑战有清晰的认识。第一章“原理与机制”将揭示模型为何以及如何走上这些捷径，探讨[经验风险最小化](@entry_id:633880)、相关性与因果性混淆等核心概念。随后的“应用与跨学科联系”章节将展示在医学、生物学和物理学等高风险领域中捷径学习的真实案例，揭示这一问题的普遍性以及为解决它而正在开发的巧妙策略。

## 原理与机制

假设您正在教一台机器识别苹果的图片。您向它展示了数千张图片，并对每一张都告诉它：“这是苹果”或“这不是苹果”。经过大量训练后，这台机器变得异常准确。您感到自豪。但随后您发现了一个秘密：您几乎所有的苹果照片都是在户外拍摄的，背景中有绿叶，而非苹果的照片则是在室内拍摄的。您的机器并没有学会识别苹果的圆形、微红的形状。它学会了一个更简单、更懒惰的规则：“如果我看到绿叶，那就是苹果。”

这本质上就是**捷径学习**这个美丽而又危险的问题。机器在它的训练考试中找到了一个通往正确答案的聪明、简单的路径，但它完全没有抓住要点。它学会了相关性，而非其背后的概念。这种未能掌握“是什么”背后的“为什么”的失败，是构建我们可信赖的智能系统，尤其是在医学和科学等高风险领域中所面临的最重大挑战之一。

### 学习的 deceptively simple 目标

大多数现代机器学习的核心都遵循一个名为**[经验风险最小化](@entry_id:633880)（ERM）**的原则。这听起来很复杂，但其思想却异常简单。模型的目标是最小化其在给定训练数据（“经验”证据）上的误差（其“风险”）。这就像一个学生为了考试而死记硬背练习题的答案，试图通过任何必要手段获得满分。模型通过一步步调整其内部参数，使其越来越接近预测与训练集中的标签相匹配的状态。

但这个过程对于*如何*得到正确答案是不可知的。它没有内置的对世界的理解，对因果关系的理解，或者对哪些特征真正有意义的理解。如果一个微不足道的、非本质的特征恰好在训练数据中是答案的强预测因子，模型就会以无情的效率抓住它。

考虑用 AI 设计新颖的[基因回路](@entry_id:201900) [@problem_id:2018104]。如果我们仅通过展示成功构建并正常运作的回路示例来训练模型，模型可能会学到一个危险而乐观的教训：“所有提出的设计都是功能性的。”它没有关于什么构成*失败*设计的信息。没有看到负面例子，它就无法学到关键的**[决策边界](@entry_id:146073)**——即区分成功与失败的细微界线。要学习一个概念，模型需要对比。它需要看到硬币的两面。

### 捷径的剖析：相关性 vs. 因果性

最常见、最微妙的捷径源于相关性与因果性之间的混淆。我们的世界是一个由相互关联的事件构成的网络，通常很难从那些恰好同时发生的事件中理清哪个事件导致了另一个。ERM 本质上是关联大师，但在因果推理方面却是个新手。

让我们回到医学界，在这个领域，这个问题不仅是学术性的，更是生死攸关的。一个 AI 系统被训练用于从胸部X光片中检测肺炎。在训练所在的医院里，碰巧使用便携式X光机拍摄的患者（通常在急诊室或隔离病房）更可能患有严重肺炎。我们还假设，这台便携式机器会在图像上留下一个小的金属标记或特定的纹理伪影 [@problem_id:4422557]。

一个经过ERM训练的模型，在筛选了数千张图像后，可以做出一个强大的发现：标记的存在是肺炎的绝佳预测因子。既然可以只寻找那个明亮、显眼的标记，为什么还要学习肺部浑浊的复杂而微妙的模式呢？模型学会了捷径。

当我们绘制一个简单的因果关系图，即**有向无环图（DAG）**时，这个错误就变得清晰了。

$$
\text{医疗环境} (C) \longrightarrow \text{肺炎} (Y)
$$
$$
\text{医疗环境} (C) \longrightarrow \text{标记} (S)
$$

`医疗环境`（$C$）——身处急诊室——是更高概率患有`肺炎`（$Y$）和使用会留下`标记`（$S$）的机器的共同原因，或称**混淆因子**。从 $S$ 到 $Y$ 没有直接的因果箭头；标记不会导致疾病。然而，由于它们共享一个共同原因，它们在统计上变得相关。这就创建了一条“后门路径”（$S \leftarrow C \rightarrow Y$），模型乐于利用这条路径。它学会了[伪相关](@entry_id:755254)，而不是真正的因果路径，后者涉及疾病本身在肺部引起可见模式：$Y \to \text{图像} (X)$。

### 当世界改变时：捷径的脆弱性

所以，模型在训练测试中取得了优异成绩。问题何在？问题在于当世界发生变化时——而世界总是在变化。依赖脆弱捷径的模型是脆弱的。它的知识建立在沙子之上。

当模型被部署到一个不使用便携式X光机标记的新医院时会发生什么？或者当原来的医院更新了设备时？捷径消失了。突然之间，模型的性能灾难性地崩溃了 [@problem_id:4422557]。这就是**[分布偏移](@entry_id:638064)**问题：部署时的数据分布与训练分布不同。

这是一种普遍现象。它可能是X光片上的侧向标记（“L”或“R”），碰巧与某种分诊协议相关，或者是因医院而异的扫描仪特有的图像伪影，如黑色边框 [@problem_id:4405478]。它甚至可以出现在意想不到的地方，比如计算生物学。当训练一个网络来分析不同长度的[蛋白质序列](@entry_id:184994)时，一个常见的做法是用[零填充](@entry_id:637925)较短的序列，使它们长度一致以便高效处理。如果在训练数据中，蛋白质长度恰好与其功能相关，模型可能不会学习决定功能的复杂[序列基序](@entry_id:177422)。相反，它可能会学会简单地检测蛋白质结束和[零填充](@entry_id:637925)开始的“边缘”——一个完全由程序员创造的人工特征 [@problem_id:2373405]。

[鲁棒机器学习](@entry_id:635133)的理论为我们提供了描述这一现象的语言。一个学习了真正、**不变的因果特征**（$Z$）——比如实际的肺部模式，其与疾病的关系 $P(Y|Z)$ 在所有医院中都是稳定的——将具有较低的**鲁棒风险**（$R_{\mathrm{rob}}$）。它泛化得很好。而一个学习了特定环境的**伪特征**（$S$）——比如那个标记，其与疾病的关系 $P(Y|S)$ 因医院而异——将具有很高的鲁棒风险，意味着它在最坏情况下的新环境中的表现将非常糟糕 [@problem_id:4405478]。

### 揭开欺骗的面纱：我们如何找到捷径？

如果我们的模型是如此聪明的骗子，我们如何才能成为同样聪明的侦探？我们需要方法来探测模型的“心智”并揭露其隐藏的依赖关系。

最直接的方法之一是进行**[对照实验](@entry_id:144738)**。在训练模型后，我们可以在一个特制的[验证集](@entry_id:636445)上测试它，在这个验证集中我们故意打破了[伪相关](@entry_id:755254)性 [@problem_id:3135726]。对于我们的肺炎模型，我们可以取验证集中的所有图像，并以数字方式移除这些标记。如果模型的准确率从一个高的“捷径可见”时的准确率（$a_\mathrm{vis}$）骤降到一个低的“捷径被抑制”时的准确率（$a_\mathrm{sup}$），我们就抓住了模型的现行。它一直依赖于那个标记。

我们可以更加精细，使用因果推断的语言。我们可以问一个**反事实问题**：“对于这位来自B医院的健康患者，*如果*他们的X光片上包含了A医院的水印，这个模型会做出什么预测？”这等同于执行一个被称为**do算子**的数学干预。通过以数字方式向图像添加水印——通过 $\mathrm{do}(\text{Token}=1)$ 进行干预——并观察到模型预测的系统性变化，我们可以证明模型对水印存在非因果依赖关系，而这与患者的实际疾病状态无关 [@problem_id:4411379]。

第三种极富创造性的方法是利用**对抗攻击**的工具，不是为了破坏，而是为了理解。对抗攻击通常涉及对图像进行微小、人类无法察觉的更改以欺骗模型。我们可以调整这种方法用于诊断。想象一张用于诊断癌症的组织学图像。病理学家可以标记出具有诊断意义的区域（例如，细胞核）。然后我们可以问模型：“我能否*仅仅*通过在病理学家认为不相关的背景区域进行微小扰动，就迫使你将诊断从‘健康’改为‘癌症’？”[@problem_id:2373351]。如果答案是肯定的，这就是无可否认的证据，证明模型正依赖于脆弱、不鲁棒且非医学的特征。它在噪声中找到了捷径。

### 内部运作：更深层次的机制

捷径学习现象的影响深远，与机器学习流程的每个部分交织在一起，甚至深入到[优化算法](@entry_id:147840)的比特和字节层面。

我们已经看到像**[零填充](@entry_id:637925)**这样简单的预处理步骤是如何创造出可供模型学习的人工边缘的 [@problem_id:2373405]。但问题可能更加微妙。**优化器的选择**本身也可能加剧这个问题。例如，使用**动量**的优化器旨在通过累积过去的梯度来加速训练，就像一个重球滚下山坡。如果一个伪特征提供了一个强大而一致的梯度信号，动量可能导致模型对这个捷径“加倍下注”，在参数空间的错误方向上积累速度，从而更难学习那些更弱但更有意义的因果特征 [@problem_id:3154032]。

也许捷径学习最令人不安的方面是它与 AI 安全的交集。捷径不仅仅是偶然发生的；它们创造了可被恶意利用的漏洞。想象一个对手想要在医疗 AI 中植入一个“后门”。他们可以用一些包含特定、微妙触发模式（比如角落里一个微小的彩色方块）的样本来对训练数据投毒，并将它们全部标记为“癌症”。一个过[参数化](@entry_id:265163)的模型有足够的能力学习这个恶意规则。但如果对手设计的触发器与模型已经倾向于学习的*现有捷径*（比如某种类型的扫描仪噪声）相吻合，那么攻击将变得更加阴险 [@problem_id:4401108]。那些驱动捷径学习的机制——对简单相关性的不懈寻找以及[验证集](@entry_id:636445)对这些缺陷的盲目性——成为了[定向攻击](@entry_id:266897)的敞开大门。

理解这些原理和机制是迈向解决方案的第一步。它迫使我们超越简单地问“模型准确吗？”，而开始问更重要的问题：“模型为什么准确？”以及“那个‘为什么’明天还会成立吗？”

