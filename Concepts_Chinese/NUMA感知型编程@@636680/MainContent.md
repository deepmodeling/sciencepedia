## 引言
现代计算机，从强大的服务器到超级计算机，其巨大的能力源于多个处理器协同工作。然而，仅仅增加处理器数量并不能保证性能的提升。一个关键却常被忽视的挑战在于这些处理器如何访问内存。当软件不了解硬件的物理布局时，可能会无意中造成数据交通拥堵，从而严重影响性能，使机器的潜力远未得到发挥。本文旨在弥合这一知识鸿沟，揭示主导这些强[大系统](@entry_id:166848)中内存访问的架构的神秘面纱。

本文探讨了[非统一内存访问](@entry_id:752608)（NUMA）的概念以及编写与硬件协同工作的“NUMA感知”程序的艺术。首先，在“原理与机制”一章中，我们将深入研究现代硬件的地理布局，解释为什么内存访问不是统一的、远程数据访问的性能成本，以及[操作系统](@entry_id:752937)“首次接触”策略的关键作用。随后，“应用与跨学科联系”一章将展示这些原理如何应用于解决[计算流体力学](@entry_id:747620)和[分子动力学](@entry_id:147283)等领域的复杂问题，并将局部性概念扩展到GPU和高级网络通信。通过理解这一领域，您可以将隐藏的性能瓶颈转化为重大优化的机会。

## 原理与机制

要理解如何编写在现代计算机上快速运行的程序，我们必须从一点地理学知识开始。不是山川河流的地理学，而是计算机内部的地理学——具体来说，是其处理器和内存的布局方式。

### 两种内存的故事：NUMA的诞生

想象一台多年前的简单计算机。它有一个中央处理单元（CPU）和一个内存库。从CPU的角度来看，内存的每个字节都同样容易访问。从第一个地址获取数据所需的时间与从最后一个地址获取数据所需的时间相同。这个田园诗般简单的世界被称为**统一内存访问**（**Uniform Memory Access**，简称**UMA**）。

现在，让我们来构建一台现代超级计算机，或者仅仅是一台强大的服务器。我们不只想要一个CPU；我们想要几十个，甚至几百个CPU，共同解决一个问题。挑战在于，如何将所有这些处理器连接到内存？如果你试图构建一个所有CPU都能平等访问的巨大内存库，就会造成交通拥堵。通往内存的路径成为瓶颈，增加更多的处理器并不会让机器变得更快，只会让等待队列变得更长。

工程师们想出了一个巧妙而实用的解决方案，这与我们组织大型社区的方式类似。我们不是为整个国家设立一个中央图书馆，而是在每个城镇都设有地方图书馆。同样的原则被应用于计算机架构。现代[高性能计算](@entry_id:169980)机由多个“节点”（通常对应主板上的物理处理器插槽）构成。每个节点都有自己的一组处理器核心和自己直接连接的内存库。这种设计被称为**[非统一内存访问](@entry_id:752608)**（**Non-Uniform Memory Access**，简称**NUMA**）。

这个名字说明了一切。内存访问不再是统一的。对于节点0上的一个[CPU核心](@entry_id:748005)来说，访问物理上连接到节点0的内存是一个“本地”操作。它快速而高效。但如果同一个核心需要访问存储在节点1内存中的数据，它必须通过连接这些节点的特殊高速互连发送请求。这是一个“远程”访问。它能工作，但速度没那么快。

这不是设计缺陷，而是一个绝妙的折衷方案。这是扩展计算能力的物理现实。**NUMA感知型编程**的艺术与科学在于理解这种地理布局，并相应地安排你的计算。目标简单而深刻：让工作和数据待在一起。

### 距离的代价：[延迟与带宽](@entry_id:178179)

当我们说远程访问“更慢”时，到底是什么意思？成本有两种：**延迟**（**latency**）和**带宽**（**bandwidth**）。

**延迟**是获取首字节的时间：一个请求传播到远程节点并让第一片数据返回的往返时间。可以把它想象成步行去另一个城镇的图书馆所需的时间。

**带宽**是建立连接后数据流的传输速率。可以把它想象成每次往返可以带回多少本书。

在许多科学和数据密集型应用中，你不仅仅是获取一个字节，而是在处理数GB的数据流。在这些受内存限制的场景中，带宽为王。而本地访问和远程访问之间的带宽差异是惊人的。

让我们想象一台典型的双插槽服务器。一个线程从其本地内存可以获得的持续带宽 $B_{\text{local}}$ 可能约为 $20 \text{ GiB/s}$。而它从远程节点内存可以获得的带宽 $B_{\text{remote}}$，受限于互连，可能只有 $10 \text{ GiB/s}$ [@problem_id:3687004]。现在，假设一个线程需要处理一个 4 GiB 的[数据块](@entry_id:748187)。

如果数据是本地的，所需时间为：
$$ t_{\text{local}} = \frac{4 \text{ GiB}}{20 \text{ GiB/s}} = 0.2 \text{ seconds} $$

如果数据是远程的，所需时间为：
$$ t_{\text{remote}} = \frac{4 \text{ GiB}}{10 \text{ GiB/s}} = 0.4 \text{ seconds} $$

仅仅因为数据放错了位置，处理它就需要两倍的时间！这不是一个微不足道的优化细节，这是一个直接的性能减半因素。现在想象一下，你程序中一半的线程都处于这种情况。你那价值数十亿美元的超级计算机正以其潜力的一半速度运行，而这一切都源于内存的地理布局。

### 看不见的手：[操作系统](@entry_id:752937)与首次接触策略

那么，是谁决定数据存放在哪里呢？你可能认为，当你的程序分配一个大数组时，计算机会随便找一个空闲的内存块并分配给你。现实情况更为微妙，并为掌握NUMA提供了关键。

当你的程序请求内存时，[操作系统](@entry_id:752937)（OS）会给它一个*虚拟*地址范围。这些只是占位符。此时还没有分配任何物理内存。[操作系统](@entry_id:752937)会等到你的程序实际尝试访问该范围内的某个位置——对内存页的第一次读取或写入。这个事件被称为**[缺页中断](@entry_id:753072)（page fault）**，它会触发[操作系统](@entry_id:752937)最终将一个物理[RAM](@entry_id:173159)页映射到那个虚拟地址。

在[NUMA系统](@entry_id:752769)上，[操作系统](@entry_id:752937)使用一个极其简单而有效的[启发式方法](@entry_id:637904)来决定将这个物理页放在*哪里*：**首次接触策略**。页面被分配在触发缺页中断的[CPU核心](@entry_id:748005)所在的NUMA节点上[@problem_id:2422586]。逻辑很简单：最先需要数据的代码很可能就是使用它最多的代码，所以让我们把数据放在它附近。

这个策略是一个强大的工具，但对于不经意的程序员来说，也是一个极其有效的陷阱。考虑这个常见的场景：一个程序员编写了一个带有大型共享数组的并行程序。在开始主要的[并行计算](@entry_id:139241)之前，他们在主线程上写了一个简单的循环来将数组初始化为零。
```c
// Deceptively simple code with huge performance implications
double* data = (double*)malloc(HUGE_SIZE);
// Single-threaded initialization
for (size_t i = 0; i  HUGE_SIZE / sizeof(double); ++i) {
    data[i] = 0.0;
}
// Now, start many threads to work on 'data' in parallel...
#pragma omp parallel for
for (size_t i = 0; i  ...; ++i) {
    // ... do work on data[i]
}
```
这里发生了什么？运行在（比如说）节点0上某个核心的单个主线程“首次接触”了数组的每一个页面。由于首次接触策略，整个数组现在都位于节点0的物理内存中。

现在，并行计算开始了。运行在节点0上的32个线程很高兴；它们的数据是本地的。但运行在节点1上的32个线程将面临漫长的一天。它们对数组中自己那部分数据的每一次内存访问都必须跨越互连。它们永久地受限于较低的远程[内存带宽](@entry_id:751847)。系统的总带宽不是两个本地带宽之和（$B_{\text{local}} + B_{\text{local}}$），而是由一个本地和一个远程通道组成的受限总和（$B_{\text{local}} + B_{\text{remote}}$）[@problem_id:2422586]。你不经意间创造了一个系统，其中一半的处理器沦为二等公民。

### 驯服野兽：程序员的策略

一旦你看到了这个陷阱，解决方案就变得异常清晰。NUMA优化的核心原则是确保数据物理上位于将要处理它的线程所在的同一节点上。

**并行初始化：** 最关键的技术是用**并行首次接触**取代顺序初始化。稍后将对某块数据进行计算的那些线程，也应该是由它们来初始化这块数据。
```c
// The NUMA-aware way
double* data = (double*)malloc(HUGE_SIZE);
// Parallel initialization
#pragma omp parallel for
for (size_t i = 0; i  HUGE_SIZE / sizeof(double); ++i) {
    data[i] = 0.0;
}
// Now, start the same threads to work on the data...
```
在这个版本中，节点0上的线程首次接触它们那半边的数组，将其放置在本地内存中。节点1上的线程首次接触*它们*的那半边，将其放置在*它们*的本地内存中。当主计算开始时，所有线程都发现它们的数据就在本地等待着。现在，两个节点都可以以其全部本地带宽运行，系统的总吞吐量也随之飙升。[@problem_id:3208187] [@problem_id:2422586]

**线程亲和性：** 为了使这个策略稳健，你不能让[操作系统](@entry_id:752937)随意移动你的线程。你需要强制执行你的地理规划。这通过设置**线程亲和性**（thread affinity），或将线程“绑定”（pinning）到特定的处理器核心或整个节点上来实现。通过将处理前半部分数据的线程绑定到节点0的核心上，并将其他线程绑定到节点1的核上，你可以保证你精心布局的[数据局部性](@entry_id:638066)在整个程序执行期间都得以维持[@problem_id:3516586]。

这引出了一个在多插槽节点上实现高性能的经典方案：划分数据，将线程组绑定到每个插槽，并执行并行首次接触。这一简单的步骤组合使计算工作与内存地理布局保持一致，将NUMA挑战转化为性能上的胜利。

### [超越数](@entry_id:154911)组：无处不在的NUMA感知

局部性原则并不仅限于大型、密集的数组。它是现代系统的一个普遍真理，其印记随处可见。

**[内存映射](@entry_id:175224)文件：** 许多应用程序通过将文件“映射”到内存来访问磁盘上的文件。在幕后，[操作系统](@entry_id:752937)使用其**[页缓存](@entry_id:753070)**来管理这一切。这些缓存的页面，就像匿名内存一样，也有一个由首次接触策略决定的NUMA位置！如果单个线程为了“预热”缓存而读取一个大文件，它可能会无意中将所有缓存页面强制分配到单个节点上，为之后访问该文件的其他线程制造完全相同的瓶颈。解决方案同样是[分布](@entry_id:182848)式首次接触，即每个工作线程负责将其将要处理的文件部分调入内存[@problem_id:3687004]。

**同步：** 即使是像获取锁这样微小的操作，也可能引发NUMA级别的大麻烦。一个简单的**[自旋锁](@entry_id:755228)**，即等待的线程反复尝试写入一个锁变量，在[NUMA系统](@entry_id:752769)上是一场灾难。在高争用情况下，来自所有节点的线程都会尝试写入同一个内存位置。这导致持有锁的单个缓存行在缓慢的互连上疯狂地来回传递，造成一场“一致性风暴”，使总线饱和，并使进度陷入停滞[@problem_id:3684244]。

更智能的设计，如**票据锁**（ticket locks），可以缓解这个问题。一个线程获取一个“票据”（一次原子写入），然后通过在一个不同的变量上自旋来等待，这个变量它可以从本地缓存中读取而无需产生流量。锁以公平的、先进先出的方式移交。虽然这是一个巨大的改进，但即使是标准的票据锁也并非完全NUMA感知的。它基于到达时间而非地理位置来强制执行公平性。它可能会将锁授予一个远程节点上的线程，即使本地节点上也有线程在等待，从而迫使受保护的数据跨越互连进行迁移[@problem_id:3684244]。

**小对象分配：** 世界并非全是大数组。许多程序，如Web服务器或数据库，会分配和释放数百万个小的、固定大小的对象。在这里，NUMA也很重要。一个`[slab分配器](@entry_id:635042)`通常会为每个节点维护一个空闲对象的缓存以提高局部性。但如果节点1上的一个线程需要释放一个其“家”在节点0上的对象会发生什么？这是一个远程释放操作，会增加流量和争用。一个真正聪明的分配器可能会实现一种**延迟释放**（deferred free）机制。节点1上的线程不会立即释放对象，而是将其添加到一个小的私有列表中。它在做一个赌注：“我可能很快会迁移回节点0，如果我这样做了，我就可以将其作为一次廉价的本地释放来执行。”这是一种漂亮的、概率性的方法来最小化远程流量，平衡了迁移的可能性与及时将内存返回给系统的需求[@problem_id:3683594]。

### 宏观视角：作为指挥家的[操作系统](@entry_id:752937)

虽然程序员可以做很多事情，但[操作系统](@entry_id:752937)可以扮演一个总指挥的角色，试图自动化这种优化。[操作系统](@entry_id:752937)中复杂的NUMA调度器不仅仅依赖于静态的首次接触策略。它可以主动监控每个线程的内存访问模式。它可以建立一个复杂的[成本矩阵](@entry_id:634848)，估算当前布局的“痛苦程度”：“节点0上的线程12花了80%的时间访问节点3上的内存。成本很高。”[@problem_id:3661575]。

基于这种全局视图，[操作系统](@entry_id:752937)可以做出动态决策。它可能会迁移一个线程，使其更接近它正在访问的数据。或者，如果一个内存页面被另一个节点上的线程大量使用，它可能会迁移页面本身。为了防止不稳定性——即不断地来回移动线程和页面，这种现象被称为“颠簸”（thrashing）——它使用**滞后效应**（hysteresis），只有当新的安排承诺带来显著且稳定的性能提升时才会采取行动[@problem_id:3661575]。

从硅的物理布局到[操作系统](@entry_id:752937)的抽象策略，NUMA的故事是一个关于地理的故事。通过理解我们机器的地理景观，我们可以超越仅仅编写正确的代码，转而创作出与硬件和谐共存、而非与之对抗的优雅高效的计算。

