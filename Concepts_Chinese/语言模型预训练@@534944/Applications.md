## 应用与跨学科联系：数据的通用语法

我们的旅程已经深入探讨了[预训练](@article_id:638349)的核心原理，见证了一台机器如何通过简单的、自监督的填空行为来学习人类语言复杂的语法和语义。但是，科学中一个真正深刻的思想很少会局限于它的发源地。就像支配着下落的苹果和环绕的行星的运动定律一样，一个强大的原理通常通过其普适性来展现其真正的力量。

因此我们必须追问：这种从数据内在结构中学习的思想，其意义是否超越了人类语言？在世界的其他地方，是否存在一种“语言”——拥有自己的语法、句法和意义？如果存在，那么通过学习说这些语言，我们又能探索哪些新的前沿领域？这不仅仅是异想天开，更是解锁现代机器学习最激动人心、最多样化应用的关键。

### 超越人类语言：科学的方言

[预训练](@article_id:638349)[范式](@article_id:329204)的真正美妙之处在于其与具体领域无关的特性。其核心机制并不知道也不关心它的输入是一句英语。它只看到一个标记序列，并学习它们之间的统计关系。这为解读和建模任何可以表示为序列的领域打开了大门。

#### 生命的语言：[基因组学](@article_id:298572)与蛋白质组学

也许与人类语言最自然、最深刻的相似之处在于生命本身的语言。由仅四个字母（$A, C, G, T$）组成的DNA长序列，以及它们编码的、由二十种氨基酸组成的[蛋白质序列](@article_id:364232)，是生物学的基础文本。这些语言的“语法”并非人类约定的产物，而是由无情的物理定律所规定，并经过数十亿年演化精炼而成。

当我们将一个掩码语言模型应用于海量的蛋白质序列语料库时，我们实际上是在要求它学习这种演化语法。为了准确预测一个被掩码的氨基酸，模型必须隐式地学习它可能与哪些其他[残基](@article_id:348682)相互作用。值得注意的是，这迫使模型揭示了深层的生物学原理。它学到，在线性序列中相距甚远的[残基](@article_id:348682)，可能在最终的三维折叠结构中彼此靠近，它们之间的协同变异是物理接触的明显标志。它学会了定义酶的[活性位点](@article_id:296930)或结构蛋白核心的微妙模式 [@problem_id:2749082]。

其结果是一组“[嵌入](@article_id:311541)”（embeddings）——数值表示——这些表示充满了丰富的生物学意义，而这一切都是在没有任何生物学家提供标签的情况下学到的。这些表示随后可以用于一系列令人眼花缭乱的下游任务。只需少量标记数据，我们就能构建出高精度的预测器，用于识别如[启动子](@article_id:316909)（[@problem_id:2429075]）等关键基因组区域，甚至可以将分析转向设计。通过在这些强大的[嵌入](@article_id:311541)之上拟合一个像[高斯过程](@article_id:323592)（Gaussian Process）这样的统计模型，我们可以执行[贝叶斯优化](@article_id:323401)，以智能地搜索广阔的可能蛋白质空间，从而以前所未有的效率设计出新颖的酶或治疗药物 [@problem_id:2749082]。

#### 逻辑的语言：源代码

从生物学的自然语言，我们可以转向一种人类创造的[形式语言](@article_id:328817)：计算机代码。像自然语言一样，代码有严格的语法和深层、依赖上下文的语义。通过在巨大的开源代码仓库上进行[预训练](@article_id:638349)，模型可以学会“理解”编程。

在这里，我们甚至可以优化[预训练目标](@article_id:638546)，以专注于重要的事情。例如，在像Python这样的语言中，类型注解对于代码的鲁棒性至关重要，但往往缺失。我们可以通过在训练过程中简单地给这些预测任务赋予更高的权重，来教会模型特别擅长预测这些缺失的类型（[@problem_id:3164788]）。这个小小的调整展示了[预训练](@article_id:638349)框架的灵活性，允许我们根据希望解决问题的特定“方言”对其进行调整。

#### 时间的语言：传感器数据

“序列”的概念可以进一步延伸，超越离散的符号，延伸到时间的连续流动本身。来自传感器的数据流——无论是音频信号、股价图表还是[心电图](@article_id:313490)——都是一个序列。通过将此信号[离散化](@article_id:305437)为一系列标记，我们可以应用我们用于文本的完全相同的[Transformer架构](@article_id:639494)。

在这里，[Transformer](@article_id:334261)独特的架构真正大放异彩。像[门控循环单元](@article_id:641035)（Gated Recurrent Units, GRUs）这样的旧模型是逐步处理序列的，这使得信息难以跨越长时间跨度传播。然而，Transformer的[自注意力机制](@article_id:642355)允许时间中的任何一点直接与任何其他点通信，为建模[长程依赖](@article_id:361092)关系创建了直接的路径。虽然这带来了更高的[计算成本](@article_id:308397)——与序列长度成二次方关系，而GRU则为线性关系——但正是这种能力使得Transformer特别适合处理遥远事件之间联系至关重要的任务 [@problem_id:3102446]。

### 迁移的艺术与科学：从演化到物理

既然我们已经确定可以在多种多样的“语言”上[预训练](@article_id:638349)模型，下一个问题是如何最好地利用这些知识。将[预训练](@article_id:638349)模型适应于一个新的、特定任务的过程称为[迁移学习](@article_id:357432)，它在其他科学学科中找到了优美而富有洞察力的类比。

#### 作为生物学[功能变异](@article_id:350010)的[迁移学习](@article_id:357432)

在演化生物学中，“[功能变异](@article_id:350010)”（exaptation）描述了一个为某一目的演化出的性状被挪用于新目的的过程。羽毛最初可能是为了[体温调节](@article_id:307751)而演化，后来才被挪用于飞行。这是对[迁移学习](@article_id:357432)一个绝妙的类比 [@problem_id:2373328]。

在海量的通用数据集上进行[预训练](@article_id:638349)，就像最初的演化过程，创造了一个复杂且高度结构化的系统——模型的参数，其中编码了对一个领域的通用理解。当我们用少量数据去处理一个新任务时，我们面临一个选择。我们可以尝试从零开始演化一个解决方案，但在数据有限的情况下，这就像要求演化在一夜之间造出翅膀——一个几乎不可能完成的任务，很可能会导致一个功能失调的混乱结果（在机器学习中，这被称为“[过拟合](@article_id:299541)”）。或者，我们可以利用预先存在的、功能性的结构并对其进行*适应*。这就是“微调”：我们从[预训练](@article_id:638349)的权重开始，并利用新数据温和地调整它们，允许整个结构为新目的而被修改。这种方法，与[功能变异](@article_id:350010)如出一辙，始终是利用[预训练](@article_id:638349)知识最有效的方式。

#### 物理学家眼中的迁移：保存知识

这个类比可以变得更加严谨，带上物理学的色彩。当我们将模型适应于一个新任务时，我们如何防止它完全忘记从第一个任务中学到的关键通用知识？这个问题被称为“[灾难性遗忘](@article_id:640592)”。

考虑一个来自[量子化学](@article_id:300637)的实际案例：我们有一个[预训练](@article_id:638349)的神经网络，用于预测各种有机分子的能量，我们想将它微调于一个特定的家族，比如取代苯 [@problem_id:2903813]。当我们在新分子上训练时，模型的参数会发生偏移。为了防止它们偏移得太多以至于模型忘记如何处理其他分子，我们可以采用一种名为弹性权重巩固（Elastic Weight Consolidation, EWC）的技术。

这个想法既优美又有效。想象一下，在模型的每个参数上放置一个微小的“虚拟弹簧”。对于那些对原始任务非常重要的参数，我们使用非常硬的弹簧。对于那些不太重要的参数，我们使用松散的弹簧。现在，当新训练数据试图将参数拉向一个新的最优解时，这些弹簧会向后拉，抵抗对原始知识最关键部分的改变。这些弹簧的“刚度”由一个称为费雪信息（Fisher Information）的量来确定，它在数学上衡量了每个参数的重要性。这个基于贝叶斯统计的优雅方法，为平衡新知识的获取与旧智慧的保存提供了一种有原则的方式。

#### 衡量可迁移性：对齐进步的梯度

我们如何能知道一个领域（比如计算机代码）的知识对另一个领域（比如自然语言）是否有用？我们可以在学习的几何学中找到答案。任何任务的“改进方向”可以被看作是模型参数高维空间中的一个向量——梯度。如果我们计算代码预测任务的[梯度向量](@article_id:301622)和文本预测任务的[梯度向量](@article_id:301622)，我们就可以测量它们之间的夹角 [@problem_id:3164788]。

如果向量指向相似的方向（正的[余弦相似度](@article_id:639253)），这意味着有助于模型在代码上做得更好的东西，同样有助于它在文本上做得更好。这预示着正向迁移的巨大潜力；这两个领域共享一个潜在的抽象结构。如果它们指向相反的方向，则一个任务会损害另一个任务——这是负向迁移的明确警告。这为不同领域的兼容性提供了一个形式化的、几何的直觉。

### 建立新联系：从多语言到元科学

[预训练](@article_id:638349)[范式](@article_id:329204)不限于单个目标或单一语言。我们可以组合多个目标来构建具有更丰富功能的模型。

一个强有力的例子来自构建跨语言模型。我们如何教一个模型理解语言之间的关系？我们可以同时在两个目标上训练它 [@problem_id:3164805]。第一个是熟悉的[掩码语言建模](@article_id:641899)，应用于英语和法语文本，这教会模型每种语言的内部语法。第二个是“对比”目标，模型被给予一个英语句子及其法语翻译，并被教导这两个句子的数值表示应该非常接近，同时与其他的、不相关的句子的表示相距甚远。通过同时学习“说”两种语言并“对齐”它们的意义，模型发展出一个共享的概念空间，一种“中间语言”（interlingua），它使得卓越的零样本翻译和跨语言理解成为可能。

此外，训练这些巨型模型的过程本身已经成为一门科学。鉴于[预训练](@article_id:638349)可能需要数月时间并花费数百万美元，我们如何知道何时停止？研究人员已经发现了“缩放定律”，它将[预训练](@article_id:638349)任务的性能（以[困惑度](@article_id:333750)perplexity衡量）与下游应用的最终性能联系起来。通过仔细追踪这两条曲线，我们可以观察到收益递减的点，并就何时停止进一步训练做出有原则的、数据驱动的决策，因为再训练下去已不值得成本 [@problem_id:3115529]。

### 看不见的基础：底层的经典机制

在这些关于学习、演化和信息的高层次讨论中，我们很容易忘记这项事业的纯粹物理现实。这些模型并非飘渺的心智，而是运行在物理硬件上的具体工程产物。其规模几乎不可思议。一个大型[预训练](@article_id:638349)语料库可达数TB之巨。

考虑[预训练](@article_id:638349)的第一步：构建词汇表。这涉及到在一个数TB的数据集中计算每个唯一标记的数量。这个数据集远大于任何一台计算机的主内存。这个问题是如何解决的？通过一种作为经典计算机科学基石的[算法](@article_id:331821)：[外排序](@article_id:639351) [@problem_id:3232906]。数据从磁盘分块读入内存，每个块在内部排序，然后将排好序的块（称为“顺串”）写回磁盘。接着，在随后的一系列遍中，这些顺串被合并，直到形成一个单一的、完全排序的文件。这个过程是由I/O、内存和磁盘块大小的限制所决定的精心安排的舞蹈。它有力地提醒我们，我们这个时代最先进的人工智能，是建立在永恒的、基础的[算法](@article_id:331821)之上的。

### 结论：一种新的发现[范式](@article_id:329204)

我们的探索已远不止于预测句子中单词的最初问题。我们见证了[自监督学习](@article_id:352490)的核心思想绽放成为一种通用工具，能够破译生命、逻辑和物理的语言。我们在机器中的知识迁移过程与宏大的演化过程以及贝叶斯推断的严谨框架之间，找到了深刻而富有启发性的联系。我们也为构成这场革命看不见的基石的巨大工程而感到谦卑。

学习任何数据领域的内在语法的能力，不仅仅是创造了强大的技术。它为科学提供了一种新的[范式](@article_id:329204)。在一个充斥着未标记数据的世界里，从扫描宇宙的望远镜到绘制生物[圈图](@article_id:309706)谱的测序仪，我们现在有了一种将原始信息转化为结构化知识的方法。[预训练](@article_id:638349)模型正在成为我们发现之旅中的伙伴，帮助我们看到隐藏的模式，学习宇宙的复杂语言。