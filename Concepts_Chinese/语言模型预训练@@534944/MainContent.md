## 引言
在现代人工智能领域，大型语言模型代表了一次巨大的飞跃，展示了其在理解、生成和推理人类语言方面的卓越能力。但这些系统是如何从原始、未标记的文本中获得如此复杂技能的呢？答案在于强大的[预训练](@article_id:638349)[范式](@article_id:329204)，这是一种[自监督学习](@article_id:352490)形式，模型通过学习数据本身的内在结构来深入理解世界。这种方法避免了对大规模、人工标注数据集的需求，而是从其消耗的文本中自行创建学习信号。本文深入探讨了使这一革命性学习过程成为可能的核心思想。

本文的探索分为两个主要部分。在第一章“原理与机制”中，我们将剖析[预训练](@article_id:638349)的基础理念。我们将对比[自回归模型](@article_id:368525)的“讲故事者”方法与掩码语言模型的“侦探”工作，探索设计有效训练目标背后的艺术与科学，并考察用于将这些强大的通用模型应用于特定实际应用的严谨方法。随后，“应用与跨学科联系”一章将拓宽我们的视野，揭示[预训练](@article_id:638349)的原理并不仅限于人类语言。我们将探索生物学、逻辑学乃至时间序列数据的“语言”，揭示这一通用[范式](@article_id:329204)如何跨越不同科学学科建立新的联系，并成为一项基本的发现工具。

## 原理与机制

现代人工智能的核心在于一个极其简单却又异常强大的思想：要了解世界，你并不总是需要一位老师来为你标注所有东西。想象一下学习一门语言的过程。你不会只去学习词汇表和语法规则，而是会[沉浸](@article_id:320671)其中——阅读书籍、聆听对话，并逐渐培养出对词语如何组合的直觉。这便是**[自监督学习](@article_id:352490)**的精髓，也是驱动[语言模型预训练](@article_id:640698)的引擎。模型被给予一个巨大的文本库——基本上就是整个互联网——以及一个简单的游戏：预测文本的缺失部分。通过数十亿次地玩这个游戏，模型被迫学习语法、句法、关于世界的事实，甚至初级的推理能力。这个游戏的具体规则，即[预训练目标](@article_id:638546)，塑造了模型的“思维”，并决定了其最终的能力。

### 两大理念：讲故事者与侦探

让我们从考察机器从文本中学习的两种基本方式开始我们的旅程。可以把它们想象成两种不同类型的思考者：讲故事者和侦探。

**自回归**（AR）模型是讲故事者。它严格地从左到右读取文本，就像我们一样。它的游戏很简单：给定一个词语序列，预测紧随其后的下一个词。这就像一位小说家写了半个句子，现在必须仅根据已经写下的内容来决定下一个词是什么。这个过程本质上是定向和序列化的。模型一步步地建立起它的理解，创造出连贯的叙事流。这使得像GPT系列这样的[AR模型](@article_id:368525)天生擅长生成任务——写故事、补全你的句子或撰写电子邮件。

另一方面，**掩码语言模型**（MLM）是侦探。它不受单向时间流的限制。相反，它得到的是一段其中一些词被涂掉——或称**掩码**——的文本。它的游戏是填补空白。为此，侦探可以查看来自左边（空白之前的内容）和右边（空白之后的内容）的线索。这种**双向**上下文对于理解句子的*含义*是一个巨大的优势。思考这个句子：“The man went to the ___ to deposit his check.”（那个男人去___存支票。）一个AR讲故事者可能会猜测“商店”或“银行”。但一个MLM侦探，看到句子后面“存入支票”的线索，就能以高得多的确定性知道空白处必须是“银行”。这就是像BERT这类模型背后的核心思想。

哪种方法更好？这并非优劣之分，而是适用性的问题。我们可以通过一个思想实验来具体说明。假设我们用指定一个序列所需的总信息量（以熵的比特数计）来衡量其“难度”。对于AR讲故事者来说，这是逐个预测每个词的难度之和。对于理想化的MLM侦探（可以并行处理所有上下文），难度则由给定所有其他词的情况下最难填补的那个空白决定。在一个存在[长程依赖](@article_id:361092)的场景中，比如一段代码块，末尾的右括号`}`必须与开头的左括号`{`相匹配，MLM侦探就具有巨大优势。它可以同时看到开头和结尾来解析中间部分，而AR讲故事者则必须一步步地将该信息贯穿始终。这揭示了一个根本性的权衡：AR用于[序列生成](@article_id:639866)，MLM用于深层上下文理解 [@problem_id:3153625]。

### 掩码的艺术：优化游戏

一旦我们接受了侦探填补空白的方法，一个充满问题的新世界便展现在眼前。我们到底应该如何制造这些空白？事实证明，魔鬼在细节之中，而优化这个游戏是一门具有深远影响的艺术。

一个看似微不足道的细节是，我们*如何*掩码一个词。最初的BERT论文引入了一个特殊的`[MASK]`标记。因此，“The dog chased the cat”（狗追猫）可能变成“The dog chased the `[MASK]`”。模型学习到，当它看到`[MASK]`时，就应该尝试预测被隐藏的词。但这里有一个问题：模型在充满这些`[MASK]`标记的世界中进行训练，但在推理时——当我们用它来执行实际任务时——它面对的是没有任何`[MASK]`的干净句子！这在训练和测试之间造成了**分布不匹配**。研究人员甚至可以使用像[Jensen-Shannon散度](@article_id:296946)这样的信息论工具来量化这种不匹配。为了缓解这个问题，一个聪明的技巧被引入：有时，不把一个词替换为`[MASK]`，而是用词汇表中的另一个随机词替换它。这迫使模型学习纠正文本中的错误，而不仅仅是填补明确的空白，从而使训练游戏更接近现实世界 [@problem_id:3147306]。

除了单个标记，我们可以问一个更深层次的问题：我们应该掩码单个词还是整个短语？语言中的许多思想并非局限于单个词，而是词的跨度。考虑一个抽取式问答任务，答案是段落中一个连续的文本跨度。如果我们希望模型擅长这个任务，或许它的训练游戏也应该包含预测跨度。这就催生了**跨度掩码**，即我们不再掩码单个标记，而是掩码一整个序列。我们甚至可以为这些被掩码的跨度建立理想长度的模型。通过设计一个其结构与下游任务“对齐”的[预训练](@article_id:638349)任务——例如，通过使被掩码跨度的长度分布与问答任务中答案长度的分布相似——我们可以显著提升性能。这就像为了考试而练习与真实考试格式相同的题目一样 [@problem_id:3102524]。

这种将训练游戏与现实世界挑战对齐的原则也延伸到了处理含噪声的文本上。用户经常会打错字。一个标准模型可能会被一个拼写错误的单词搞糊涂，将其分解成奇怪的子词标记。我们如何让模型更具鲁棒性？答案是一种形式的疫苗接种：以可控的方式让模型接触“病原体”。通过在[预训练](@article_id:638349)期间故意用字符级噪声破坏单词，我们创建了一个看起来更像它稍后会遇到的含噪声用户文本的训练分布。这种[数据增强](@article_id:329733)迫使模型学习单词的潜在含义，即使它们有轻微的拼写错误，从而极大地提高了其在现实世界中的鲁棒性 [@problem_id:3102531]。

### 教会模型认识世界

[预训练](@article_id:638349)的目标不仅仅是学习语法，更是吸收编码在文本中浩瀚的人类世界知识与结构。我们设计的[预训练目标](@article_id:638546)就是这一宏伟教育的课程。

早期的双向模型如BERT不仅在MLM上进行训练，还在一个名为**下一句预测**（NSP）的任务上进行训练。模型被给予两个句子A和B，并且必须预测B是否是原文中紧跟A的下一句，或者只是一个从别处随机抽取的句子。人们希望这能教会模型关于语篇以及句子之间关系的知识。然而，研究人员发现了一个缺陷。这个任务太容易“作弊”了。随机选择的句子几乎总是关于不同的话题。因此，模型学会了一个简单的启发式方法：如果句子A和B关于同一个主题，它们很可能是连续的。它没有学到[连贯性](@article_id:332655)的微妙逻辑，只学了主题匹配。这导致了**句子顺序预测**（SOP）的发展。在SOP中，模型被给予两个*总是*连续且来自同一文档的句子。它的任务仅仅是判断它们是按正确顺序[排列](@article_id:296886)，还是被调换了位置。通过消除主题变化的信号，SOP迫使模型学习文本实际的逻辑和因果流程，从而对语篇有更深入的理解 [@problem_id:3102444]。

这引出了一个有趣的问题：我们如何知道模型*真正*在学什么？这些复杂的架构仅仅是在记忆统计模式，还是在发展语言概念的真实表征？为了找出答案，研究人员使用一种称为**探查**（probing）的技术。在模型[预训练](@article_id:638349)后，其内部机制被冻结。然后，一个简单的[线性分类器](@article_id:641846)——即探针——在模型的内部表征之上进行训练，以查看它们是否能预测某些语言属性。例如，在句子“The chef cuts the bread”（厨师切面包）中，我们能否从模型的内部状态预测出“chef”是施事者（AGENT），而“bread”是受事者（PATIENT）？实验表明，MLM提供的上下文至关重要。动词（“cuts”）是一个强有力的线索。如果我们掩码动词，探针识别角色的难度会增加——但并非不可能——这表明名词本身已经学到了一些“类施事者”或“类受事者”的属性。探查为我们提供了一个窥视黑箱的窗口，使我们能够科学地研究从这些简单的预测游戏中涌现出的抽象知识结构 [@problem_id:3147302]。

### 为新技能演化目标

随着我们对语言模型[期望](@article_id:311378)的增长，其训练游戏的复杂性也在增加。一个引人入胜的最新发展是**中间填充**（Fill-in-the-Middle, FIM）目标，它特别为代码生成等任务设计。程序员不仅仅是从头到尾写代码；他们经常跳到函数中间添加新功能或修复错误。一个理想的编码助手应该既能从某个点开始继续写代码（像AR讲故事者），也能填补中间的缺失部分（像MLM侦探）。

FIM通过巧妙地重新[排列](@article_id:296886)文本来实现这一点。一个序列被分成前缀、中间[部分和](@article_id:322480)后缀。然后，模型被训练在给定前缀和后缀的情况下预测中间部分，接着在给定前缀和已填充的中间部分的情况下预测后缀。这种混合方法使得模型能够同时学习用于填充的双向上下文和从左到右的生成能力。值得注意的是，这种重构并非任意为之；它是对模型注意力的一种有原则的重新分配。详细分析表明，虽然标准AR和FIM模型能够建立的注意力连接总数是守恒的，但FIM将后缀查看中间部分的能力所对应的连接“拿走”，并将其“给予”中间部分，使其能够看到后缀。这种重新分配在不改变底层架构的情况下，赋予了模型一项多功能的新技能 [@problem_id:3164789]。

### 回报：从[预训练](@article_id:638349)到实践

我们之所以花费如此大的精力去设计复杂的[预训练目标](@article_id:638546)，只有一个原因：创造出强大、通用的模型，以便能高效地适应并解决新问题。然而，这第二个适应阶段本身也伴随着一系列有趣的原则和权衡。

首先是资源问题。[预训练](@article_id:638349)和微调都消耗巨大的计算预算。给定固定的预算，我们应该如何在两个阶段之间分配？多亏了**缩放定律**（scaling laws）的发现——这些经验性的[幂律](@article_id:320566)关系表明，模型损失会随着更多数据或计算量而可预测地减少——我们可以用微积分来回答这个问题。[预训练](@article_id:638349)和微调都表现出收益递减的现象。通过将损失建模为两个[幂律](@article_id:320566)项之和，一个用于[预训练](@article_id:638349)，一个用于微调，我们可以使用约束优化来找到我们词元预算的确切[最优分配](@article_id:639438)，以最小化最终损失。这是一个展示了深层工程原理如何能在一个复杂领域指导我们的绝佳例子 [@problem_id:3195240]。

一旦我们有了[预训练](@article_id:638349)模型，我们在适应新任务时面临一个战略选择。如果我们的新任务只有很少的标记样本（比如少于十几个），最好的方法通常是**情境学习**（ICL），即我们只需在提示中向模型展示这些样本，并要求它为新输入完成模式。如果我们有成百上千个样本，那么进行**微调**（fine-tuning）会更好，即我们在新数据上继续训练过程，更新模型的权重。这两种方法遵循不同的[学习曲线](@article_id:640568)。ICL的性能随样本增加而提高，但很快达到平台期。微调开始时表现较差，但其性能上限要高得多。通过对这些曲线进行建模，我们可以计算出一个精确的**[范式](@article_id:329204)转换点**——即样本数量$k^{\star}$，在该点上微调的性能预计将超过ICL。这为我们提供了一种有原则的方法来为任务选择正确的工具 [@problem_id:3195216]。

最后，在我们推动这门科学的前沿时，我们必须保持严谨的标准。评估这些巨型模型时一个潜藏的危险是**数据污染**。如果我们用来对模型进行基准测试的测试数据，无意中被包含在其庞大的[预训练](@article_id:638349)语料库中，那该怎么办？模型可能表现优异，不是因为它聪明，而是因为它记住了答案。为了防范这种情况，研究人员采用了复杂的协议。他们可能会使用一个**影子[测试集](@article_id:641838)**——这些数据保证是在[预训练](@article_id:638349)完成*之后*创建的。通过比较模型在可能受污染的测试集和干净的影子测试集上的性能，并对一个控制模型做同样的操作，他们可以使用像[双重差分法](@article_id:640588)（difference-in-differences）这样的统计技术来分离出记忆的迹象。这种对严谨[实验设计](@article_id:302887)的执着，将该领域从单纯的工程学提升为一门真正的科学，确保我们的发现之旅建立在真理的基石之上 [@problem_id:3195241]。

