## 原理与机制

想象你是一位雕塑家，但你的创作媒介不是大理石或黏土，而是概率本身。你从一个简单、不起眼的土块开始：一个完美的概率球体，假设在球内任何位置找到一个点的可能性都均等。这就是我们的**基准分布**，一种极其简单的东西，比如标准高斯分布，通常被称为“[钟形曲线](@article_id:311235)”。它易于理解，而且至关重要的是，易于从中抽取样本——就像从土块中[取出点](@article_id:333502)一样。我们的目标是将这个简单的土块塑造成一个错综复杂的雕塑，完美地模仿我们真实世界数据的形状——无论是照片中人脸的分布、天空中星星的模式，还是分子的结构。这种转变的行为就是**[标准化](@article_id:310343)流**的精髓。

### 概率守恒定律

我们如何执行这种变换？我们应用一个数学函数，一个映射 $x = g(z)$，它将我们简单基准分布中的一个点 $z$ 移动到数据空间中的一个新位置 $x$。为了使其成为一个“流”，函数 $g$ 必须是一种特殊的函数：它必须是可逆的。正如我们可以将 $z$ 映射到 $x$，我们也必须能够通过一个逆函数 $z = f(x) = g^{-1}(x)$ 将 $x$ 唯一地映射回 $z$。为什么这如此重要？因为它确保了没有[信息丢失](@article_id:335658)；我们总是可以追溯一个点回到其起点的路径。

但是我们必须遵守一个基本定律，即“[概率守恒](@article_id:310055)”。总概率必须始终加起来（或积分为）一。如果我们取基准空间中的一个小区域，并在变换过程中将其拉伸，那么该区域的概率必须分散开来，密度变得更低。相反，如果我们压缩一个区域，概率必须变得更集中，密度必须增加。

这个概念被数学中最优雅的规则之一所捕捉：**[变量替换公式](@article_id:300139)**。它精确地告诉我们概率密度如何变化。如果我们有一个基准密度 $p_Z(z)$，并通过 $z = f(x)$ 对其进行变换，数据空间中的新密度 $p_X(x)$ 为：

$$
p_X(x) = p_Z(f(x)) \cdot \left| \det J_f(x) \right|
$$

让我们来分解一下。第一项 $p_Z(f(x))$ 很简单：我们取数据点 $x$，将其映射回其在基准空间中的原点 $z=f(x)$，然后检查那里的密度。第二项 $\left| \det J_f(x) \right|$ 则是神奇的成分。$J_f(x)$ 是变换 $f$ 的**雅可比矩阵**，它只是 $f$ 的输出相对于其输入的所有[偏导数](@article_id:306700)的集合。这个矩阵的**[行列式](@article_id:303413)**告诉我们 $x$ 周围一个无穷小区域的体积在变换中被拉伸或压缩了多少。它是一个局部的“[缩放因子](@article_id:337434)”，确保了概率守恒。

在计算中，我们几乎总是使用对数，将乘积转化为和，并提高数值稳定性。一个数据点 $x$ 的对数概率，或称**[对数似然](@article_id:337478)**，则为：

$$
\log p_X(x) = \log p_Z(f(x)) + \log \left| \det J_f(x) \right|
$$

这个方程是[标准化](@article_id:310343)流的绝对核心。训练一个流模型意味着找到变换 $f$ 的参数，以最大化我们观察到的真实数据的[对数似然](@article_id:337478) [@problem_id:3282824]。我们可以通过简单、精确的变换来验证这个基本定律，例如将一个相关的高斯分布变为标准高斯分布的仿射“白化”，或者基于[概率积分变换](@article_id:326507)的映射，并发现该公式能以完美的数值精度成立 [@problem_id:3166224]。

### 工程师的困境与巧妙的解决方案

在很长一段时间里，这个优美的公式在机器学习中主要只是教科书上的一个奇闻。原因在于雅可比行列式。对于一个具有 $d$ 维的一般高维变换（比如一个高分辨率图像，其中 $d$ 可以是数百万），计算一个 $d \times d$ [行列式](@article_id:303413)的成本大约是 $\mathcal{O}(d^3)$。这在计算上是毁灭性的。对一张图像进行一次计算可能比宇宙的年龄还要长！

标准化流的现代革命源于一系列极其简单的架构技巧，旨在绕过这个“[行列式](@article_id:303413)瓶颈”。其关键洞见在于设计变换 $f$，使其雅可比矩阵具有一种特殊的结构，从而使其[行列式](@article_id:303413)的计算变得微不足道。

#### 架构一：自回归流

一种方法是逐个维度地顺序构建变换。这被称为**自回归**变换。其思想是，变换后变量的第一个分量 $z_1$ 仅依赖于输入的第一个分量 $x_1$。第二个分量 $z_2$ 依赖于 $x_1$ 和 $x_2$，以此类推。总的来说，$z_i$ 仅依赖于 $x_1, \dots, x_i$。

这对雅可比矩阵 $J_f(x)$ 有何影响？其元素是 $\frac{\partial z_i}{\partial x_j}$。因为 $z_i$ 不依赖于任何 $j > i$ 的 $x_j$，所以所有 $j > i$ 的[偏导数](@article_id:306700) $\frac{\partial z_i}{\partial x_j}$ 都为零。这意味着雅可比矩阵是**[下三角矩阵](@article_id:638550)**！[三角矩阵的行列式](@article_id:310254)就是其对角线元素的乘积。残酷的 $\mathcal{O}(d^3)$ 成本骤降至轻松的 $\mathcal{O}(d)$ [@problem_id:3282824]。我们只需要计算 $d$ 个[导数](@article_id:318324)，而不是一个巨大的[行列式](@article_id:303413)。

这个想法揭示了不同类型生成模型之间美妙的统一性。像 PixelCNN 这样的[自回归模型](@article_id:368525)，根据先前的像素逐个生成图像像素，是基于概率[链式法则](@article_id:307837)：$p(x) = \prod_i p(x_i | x_{'}

