## 引言
物理定律拥有一个深刻而优雅的特性：它们是对称的，无论观察者的位置或朝向如何，都保持不变。当我们构建物理世界的[计算模型](@entry_id:152639)时，我们面临一个关键选择——要么希望模型能从海量数据中学习到这些对称性，要么将它们直接嵌入到模型的设计中。等变[图神经网络](@entry_id:136853)（GNNs）采用了后者，这种更强大的方法，创造出能用与自然本身相同的几何语言“思考”的模型。本文旨在解决构建尊重基本物理原理的人工智能这一挑战，从而实现更高的效率和准确性。在接下来的章节中，我们将首先探讨[等变性](@entry_id:636671)的核心“原理与机制”，区分其与[不变性](@entry_id:140168)，并概述构建对称性感知的 GNN 的关键策略。然后，在“应用与跨学科联系”部分，我们将见证这些有原则的模型如何正在彻底改变从化学、[材料科学](@entry_id:152226)到[高能物理](@entry_id:181260)和[机器人学](@entry_id:150623)的各个领域。

## 原理与机制

假如你正试图描述自然法则，你会很快发现一个深刻而美丽的真理：这些法则不依赖于你的视角。无论你是在伦敦还是东京，在今天还是明天，面朝北方还是南方进行实验，其底层的物理学都保持不变。这个宏伟的思想，即对称性原理，不仅是一种哲学上的雅趣，更是我们构建对宇宙理解的深刻而有力的指南。

当我们为物理系统（无论是一个分子、一个星系，还是天气）构建[计算模型](@entry_id:152639)时，我们有一个选择。我们可以要么期望我们的模型从海量数据中学习到这些基本对称性，要么将对称性直接构建到模型本身的结构中。后一种方法，即**[等变性](@entry_id:636671)**（equivariance）之路，不仅更优雅，而且功能更强大、效率也高得多。它是一种物理原理在架构上的体现。

### 对称性的语言：[不变性](@entry_id:140168)与[等变性](@entry_id:636671)

让我们首先明确我们的术语。当我们谈论三维空间的对称性时，我们指的是**欧几里得群 E(3) **，它包含了所有可能的刚体运动：平移（移动而不转动）、旋转（转动而不移动）和反射（照镜子）。一个尊重这些对称性的模型必须表现出以下两种行为之一：

- **[不变性](@entry_id:140168)**（Invariance）：当输入被变换时，模型的输出完全不发生改变。想象一个水分子的总[势能](@entry_id:748988)。它是一个单一的数字，一个标量。这个能量取决于氢原子和氧原子的相对[排列](@entry_id:136432)，但与分子是在你的实验室里还是在月球上无关，也与它指向哪个方向无关。能量在平移和旋转下是**不变的**。

- **[等变性](@entry_id:636671)**（Equivariance）：当输入发生变换时，模型的输出会以一种可预测的方式相应地变换。思考作用于同一个水分子中各个原子的力。每个力都是一个向量，既有大小也有方向。如果你旋转分子，你会期望力向量也随之旋转。从分子的几何结构到其受力的映射是**等变的**。输出与输入和谐共舞。

因此，我们的挑战就是构建一个能够内在地理解这种舞蹈的学习机器——一个图神经网络（GNN）。

### 从[置换](@entry_id:136432)到几何：一个热身

在处理完整的三维空间几何之前，让我们先考虑一个更简单的对称性。想象一下你正在分析欧洲[核子](@entry_id:158389)研究中心（CERN）探测器记录的一次粒子碰撞事件[@problem_id:3510650]。这个事件只是一堆或一组粒子。你列出它们的顺序是完全任意的。一个物理结论，比如碰撞的总能量，不应依赖于这种任意的标签。这就是**[置换不变性](@entry_id:753356)**（permutation invariance）原理。

我们如何构建一个自动具备[置换不变性](@entry_id:753356)的[神经网](@entry_id:276355)络呢？一个优美而简单的方案是像 Deep Sets 这类模型的基础：

1.  取集合中的每一项（每个粒子的[特征向量](@entry_id:151813)）。
2.  将每一项输入到一个相同的[神经网](@entry_id:276355)络中，我们称之为 $\phi$。
3.  使用一个不关心顺序的操作（如求和或平均）来聚合结果。
4.  将这个单一的聚合结果输入到最后的[神经网](@entry_id:276355)络 $\rho$ 中，得到你的答案。

这个结构，$f(X) = \rho(\sum_{i} \phi(x_i))$，通过其设计本身就保证了[置换不变性](@entry_id:753356)。打乱输入只会打乱求和项的顺序，但这并不会改变结果。

这是一个深刻的架构思想。我们不仅仅是训练了一个在所见过的数据上*恰好*表现出[不变性](@entry_id:140168)的模型；我们构建了一个对于任何可能的输入都*不可能不是*不变的模型。这就是等变工程的精神。[图神经网络](@entry_id:136853)自然地扩展了这一思想，以产生逐粒子的输出，其中对称聚合发生在[消息传递](@entry_id:751915)步骤中，确保粒子 `i` 的输出对应于粒子 `i` 的输入，这一属性被称为**[置换](@entry_id:136432)[等变性](@entry_id:636671)**（permutation equivariance）[@problem_id:3510650]。

### 几何和谐的两大策略

现在，让我们回到完整的[三维几何](@entry_id:176328)。我们的 GNN 节点不再是抽象的粒子，而是空间中具有位置 $\mathbf{r}_i$ 的原子。我们如何尊重 E(3) 群呢？第一个对称性，平移，处理起来出奇地简单。在空旷空间中的物理相互作用取决于相对位置，而非绝对位置。因此，我们设计的 GNN 只会看到**相对位置向量** $\mathbf{r}_{ij} = \mathbf{r}_i - \mathbf{r}_j$。由于全局平移会给 $\mathbf{r}_i$ 和 $\mathbf{r}_j$ 加上相同的向量，它们的差值保持不变，我们的模型因此自动变得平移不变 [@problem_id:2648604] [@problem_id:2760132]。

真正的挑战在于旋转。于此，两种优雅的策略应运而生。

#### 不变性之路：一个没有方向的世界

一种方法是从一开始就构建一个对方向“视而不见”的模型。我们可以构建本身就具有[旋转不变性](@entry_id:137644)的特征，并将它们输入到一个标准的 GNN 中。这些特征是什么呢？
- 两个原子间的距离，$\|\mathbf{r}_{ij}\|$。
- 两个化学键之间的夹角，$\theta_{ijk}$，可以通过[点积](@entry_id:149019) $\cos \theta_{ijk} = \hat{\mathbf{r}}_{ji} \cdot \hat{\mathbf{r}}_{jk}$ 求得 [@problem_id:2395405]。
- 两个原子平面之间的[二面角](@entry_id:185221)，可以通过相对位置向量的[点积](@entry_id:149019)和[叉积](@entry_id:156672)求得 [@problem_id:2395405]。

所有这些——距离、角度、二面角——都是**[标量不变量](@entry_id:193787)**。它们是在旋转下值保持不变的数字。一个只看到这些不变特征的模型，就像许多经典[势场](@entry_id:143025)和一些[机器学习模型](@entry_id:262335)（如基于 SOAP 的[高斯近似势](@entry_id:749744)）一样，会自然地产生一个不变的输出，非常适合用来预测总能量 $E$ [@problem_id:3464197]。

但是力 $\mathbf{F}_i$ 怎么办呢？它们必须是等变向量！这是否意味着不变性之路走到了死胡同？这其中蕴含着一个数学魔术般的时刻。矢量微积分的一个基本定理指出：**一个不变[标量场的梯度](@entry_id:270765)是一个等变向量场**。这意味着，如果我们有一个模型能够正确地将总能量 $E$ 预测为原子位置的[可微函数](@entry_id:144590)，我们就可以通过取负梯度来计算力，即 $\mathbf{F}_i = -\nabla_{\mathbf{r}_i} E$。根据这一定理，得到的力向量*必然*是完全 E(3) 等变的 [@problem_id:3422804] [@problem_id:2648604] [@problem_id:2760132] [@problem_id:2760146]。那些看似被丢弃的方向信息，通过[微分](@entry_id:158718)这一行为被奇迹般地恢复了。

#### [等变性](@entry_id:636671)之路：教向量学会舞蹈

另一种策略更为直接。我们不再让模型对方向“视而不见”，而是教它几何规则。我们构建一个网络，其中的特征本身不仅仅是数字，而是懂得如何旋转的几何对象。

在 E(3) 等变 GNN 中，与一个原子相关联的特征可能是一组标量（0 型，不变）、向量（1 型，会旋转），甚至更高阶的张量（2 型等，具有更复杂的旋转规则）[@problem_id:3571840]。GNN 的消息传递层就是根据物理和群论的法则来组合这些几何对象的操作。

例如，要用来自邻居 `j` 的信息来更新原子 `i` 上的特征，我们不能简单地将它们的笛卡尔向量分量拼接起来，然后输入到一个标准的多层感知机（MLP）中。MLP 将其输入视为一个简单的数字列表；对向量的 $x$、$y$ 和 $z$ 分量独立应用像 ReLU 这样的[非线性](@entry_id:637147)函数，会破坏其几何特性，从而打破[等变性](@entry_id:636671) [@problem_id:2760146]。

相反，我们必须使用尊重几何的操作。我们可以通过两个向量的**[点积](@entry_id:149019)**来形成新的标量（[不变量](@entry_id:148850)）。我们可以通过**叉积**来形成新的向量。构建等变更新的一个强大而通用的方法是，将新向量构造为现有等变[基向量](@entry_id:199546)的线性组合，其中系数本身是由 MLP 计算出的不变标量 [@problem_id:3401636]。例如，从 $j$ 到 $i$ 的一条消息可能是一个如下的向量：
$$ \mathbf{m}_{ij} = \alpha_1(\text{invariants}) \, \mathbf{u}_i + \alpha_2(\text{invariants}) \, \mathbf{u}_j + \alpha_3(\text{invariants}) \, \hat{\mathbf{r}}_{ij} $$
在这里，$\mathbf{u}_i$ 和 $\mathbf{u}_j$ 是已有的向量特征，$\hat{\mathbf{r}}_{ij}$ 是它们之间的方向向量，而标量系数 $\alpha_k$ 是关于距离和[点积](@entry_id:149019)等[不变量](@entry_id:148850)的学习函数。这种构造保证了 $\mathbf{m}_{ij}$ 会正确地旋转。

这些模型中最复杂的版本，应用于[理论化学](@entry_id:199050)和物理学中，使用量子力学的语言将其形式化。它们将特征表示为旋转群的**[不可约表示](@entry_id:263310)**（或称“irreps”），由角动量数 $l$ 索引。然后，它们使用**[张量积](@entry_id:140694)**和 **Clebsch-Gordan 系数**来组合这些特征——这正是用于将在原子中电子的[角动量相加](@entry_id:145967)的同一套数学工具 [@problem_id:2648604] [@problem_id:3571840]。最终的能量（一个 $l=0$ 的标量）和力（$l=1$ 向量的集合）随后通过将最终的丰富几何特征投影到所需的输出类型上来读出 [@problem_id:2760132]。

### 回报：为何这套精美的机制至关重要

将这种对称性构建到网络的架构中不仅仅是一种审美上的追求；它具有巨大的实际意义。

- **数据效率**：一个等变模型不需要学习什么是旋转。它已经知道了。当它从单一的分子构型中学习时，它自动理解了该构型的所有无限多个[旋转和平移](@entry_id:175994)副本的物理性质。与非对称模型相比，这极大地减少了达到高精度所需的训练数据量 [@problem_id:3464197]。

- **物理一致性**：通过构造，模型的预测保证遵守物理学的基本对称性。你永远不会得到旋转一个分子会改变其预测能量或导致力不能正确旋转这样的非物理结果。

- **更智能的科学发现**：在**主动学习**等前沿应用中，算法必须智能地决定要运行哪些新的模拟，此时[等变性](@entry_id:636671)是一种超能力。一个等变模型的[不确定性估计](@entry_id:191096)也是不变的。它认识到先前见过的结构的旋转版本并非“新”的，并且不会浪费昂贵的计算资源来重新计算它已经知道的东西 [@problem_id:3394205] [@problem_id:2760132] [@problem_id:2760146]。它将搜索集中在广阔化学空间中真正新颖且信息丰富的区域，从而加速发现。

从本质上讲，通过将深刻的对称性原理直接嵌入到我们[神经网](@entry_id:276355)络的结构中，我们创造出的模型不仅更准确、更高效，而且还能用与自然本身相同的几何语言“思考”。

