## 应用与跨学科联系

我们花了一些时间来理解什么是[伪随机性](@article_id:326976)——这个奇特而美丽的思想，即简单的确定性规则可以产生在所有实际应用中都与真正偶然性无法区分的序列。这似乎仅仅是数学家和计算机科学家的一个好奇心或玩物。但事实远非如此。事实证明，世界充满了这种“被驯服的随机性”不仅有用、甚至是必不可少的地方。在掌握了原理之后，我们现在准备好看到它们的实际应用。我们会发现，这一个思想提供了一条统一的线索，贯穿我们计算机中的电路、我们用以探寻真理的统计方法，以及自然世界的根本结构——从一只卑微昆虫的行为到生命本身的基础。

### 工程师的策略：利用随机性实现有序与完美

让我们从一些坚实而实际的东西开始：工程学。假设你构建了一个复杂的集成电路，一个由数百万个晶体管组成的微型宇宙。你如何知道它能完美工作？你必须测试它。你可以尝试输入所有可能的输入模式，但对于一个有（比如说）64条输入线的芯片，模式的数量是 $2^{64}$——这个数字如此巨大，以至于测试所有模式所需的时间比宇宙的年龄还要长。一个更简单的方法是使用一个确定性序列，比如一个只向上计数的[二进制计数器](@article_id:354133)：`0000`、`0001`、`0010` 等等。这很有序且可预测，但却是测试电路的一种糟糕方式。为什么？因为它的可预测性正是其弱点。这些模式高度相关；在很长的片段里，每次只有一个比特在变化。这种温和的、逐步的探测不太可能发现微妙、复杂的缺陷，比如一个“[串扰](@article_id:296749)”故障（一根导线的变化信号不当地影响了邻近导线），或是一个仅在信号以非常特定、快速的序列到达时才出现的“延迟”故障。

一个好得多的策略是剧烈且不可预测地“摇晃”电路。这就是[伪随机性](@article_id:326976)发挥作用的地方。工程师们不使用简单的计数器，而是使用一种称为[线性反馈移位寄存器](@article_id:314936)（LFSR）的设备。正如我们所见，LFSR生成一个看起来混沌且不相关的序列。当输入到电路中时，这些伪随机模式以各种狂野的组合切换输入。这种激进的、类随机的测试在激发和揭示那些简单计数器会错过的棘手、时序相关的故障方面要有效得多 [@problem_id:1917393]。这就像是检查一座桥的稳定性，让士兵们齐步走过（有引发灾难性共振的风险）与让一群无序的人群在上面又跳又跑（对其真实世界韧性进行更稳健的测试）之间的区别。当然，工程师们很聪明，他们也设计了混合方案，在伪随机模式的彻底性与其他目标（如最小化测试期间的功耗）之间取得平衡 [@problem_id:1917397]。

这个想法——即一点随机性可以实现纯粹有序所不能实现的目标——在信号处理领域以一种更令人惊讶的形式出现，这是一种称为**[压缩感知](@article_id:376711)**的技术。想象一下你想用相机拍照。传统的方法是测量照射到传感器上数百万个像素中每一个像素的光线。但如果你试图捕捉的信号是“稀疏”的，意味着它的大部分信息都集中在少数几个关键分量中（就像一个由几个纯音组成的简单声音），那该怎么办？[压缩感知](@article_id:376711)告诉我们，你只需进行远少于像素数量的测量，就可以完美地重建*整个*信号！

这种魔法是如何实现的？诀窍不是按有序的网格测量相邻的像素，而是测量信号的一组*[随机投影](@article_id:338386)*。例如，人们可以使用像傅里叶变换这样计算效率高的快速、结构化变换，但将其与对其输出的随机选择相结合，甚至将信号与一个随机向量进行卷积。这些混合设计，融合了确定性结构和有意的随机性，能够保证——以非常高的概率——捕捉到所有必要的信息，只要随机测量的数量略大于信号的内在稀疏度。一个纯粹确定性的采样模式将不可避免地存在盲点，使得一些稀疏信号完全不被检测到。通过使用随机性，我们确保无论信号是什么，我们都极不可能错过它。这是一个深刻的权衡：我们牺牲了对特定、已知测量方案的100%保证，换来了一个概率性的、但可能性极大的保证，这个保证对*所有*稀疏信号都有效 [@problem_id:2905658]。

### 科学家的盾牌：运用随机性对抗假象

从工程师的工具箱，我们现在转向科学家对知识的探索。在科学领域，我们最大的挑战之一就是不被愚弄——不被巧合愚弄，不被我们自己的偏见愚弄，也不被数据的纯粹复杂性愚弄。在这里，[伪随机性](@article_id:326976)再次成为一个不可或缺的盾牌。

考虑一位现代生物学家，他试图从庞大的基因表达谱数据集中预测癌症亚型；或是一位材料化学家，他在巨大的[材料属性](@article_id:307141)数据库中寻找新化合物。他们使用强大的机器学习[算法](@article_id:331821)来寻找模式。但是，当[算法](@article_id:331821)呈现出一个模式时——比如说，一簇似乎共享某种特殊属性的材料——我们如何知道这是一个真正的发现，而不仅仅是[算法](@article_id:331821)的产物或是数据中的统计侥幸？

最重要的验证技术之一是使用随机性作为探针。许多这些复杂的[算法](@article_id:331821)，如 [t-SNE](@article_id:340240) 或 UMAP（它们可以为[高维数据](@article_id:299322)创建漂亮的二维“地图”），都带有一个随机元素；它们使用一个随机的起点进行搜索。一个真实的、稳健的材料簇应该在无论[算法](@article_id:331821)以何种随机种子开始时都一致地出现。如果一个簇在一次运行中出现，但在下一次运行时消失，那它很可能是一个假象——一个由[算法](@article_id:331821)内部机制创造的幻影 [@problem_id:2479748]。

一个更基础的技术是**[交叉验证](@article_id:323045)**。要测试一个[预测模型](@article_id:383073)是否好，我们不能只用构建它时所用的数据来测试它。这就像让学生自己出考卷。相反，我们隐藏一部分数据（“[测试集](@article_id:641838)”）。但隐藏哪一部分呢？如果我们只选一次，我们的结果可能会因一次“幸运”或“不幸运”的划分而产生偏差。一个更稳健的方法是多次重复这个过程，每次都*随机地*将数据划分为训练集和测试集。通过对多次不同随机划分的模型性能进行平均，我们消除了任何单一特殊划分的影响。这降低了我们性能估计的方差，并为我们提供了一个更稳定、更可靠的图像，展示我们的模型在新的、未见过的数据上表现如何。这种[随机抽样](@article_id:354218)的使用是现代统计学和机器学习的基石，它让科学家们能够在面对不确定性时做出可靠的声明 [@problem_id:2383411]。

在这两种情况下，原理是相同的。我们使用受控的伪[随机过程](@article_id:333307)来挑战我们自己的发现。通过证明我们的结果对随机扰动——[算法](@article_id:331821)的扰动、数据划分的扰动——是稳定的，我们获得了信心，相信我们发现了真实的东西，而不仅仅是机器中的幽灵。

### 自然的逻辑：随机性作为生命与宇宙的策略

到目前为止，我们已经看到人类在*使用*[伪随机性](@article_id:326976)。但最深刻的应用并非我们自己创造的。自然界以其无穷的智慧，在数十亿年前就发现了随机性的力量。

看一个简单的气体盒子。任何单个原子的运动都由确定性的牛顿定律支配。然而，由于有如此多的原子不断碰撞，它们的路径变得异常复杂。在所有实际应用中，任何一个原子在碰撞前的速度与其碰撞伙伴的速度完全不相关。这个假设，即*Stosszahlansatz*或“[分子混沌假设](@article_id:314943)”，是[统计力](@article_id:373880)学的基石。正是这种微观层面上的有效随机性，催生了宏观层面上钢铁般确定性的热力学定律——这些定律支配着从我们的引擎到恒星的一切，并赋予了我们“时间之箭”。这种涌现出的[伪随机性](@article_id:326976)不是单个粒子的特征，而是整个系统的特征。相比之下，晶体固体缺乏这种性质；每个原子都永远锁定在其固定邻居的相互关联的舞蹈中，[分子混沌](@article_id:312505)的假设完全失效 [@problem_id:1950515]。

这种“看似随机”的行为不仅仅是无生命物质的特征；它也是生物的一种强大策略。考虑一只微小的等足目动物，或称潮虫，在一个一边干燥一边潮湿的室中。你可能会[期望](@article_id:311378)它能“感知”到湿度并直接走向它。但它做的事情要简单得多，并且在某种程度上更聪明。它以看似随机的路径徘徊，但遵循一个简单的规则：在不适的干燥区域快速移动并频繁转向，在舒适的潮湿区域则放慢速度并迂回前行。结果如何？这只潮虫不需要复杂的大脑或导航系统。它简单的、无[方向性](@article_id:329799)的、由动性驱动的行为不可避免地导致它在潮湿区域花费更多时间，并在那里与同伴聚集。这是一个美丽的例子，展示了一个简单的、局部[随机化](@article_id:376988)的搜索策略如何能解决一个复杂的优化问题：找到一个好的居住地 [@problem_id:2278653]。

最后，让我们思考生命密码本身：一个DNA序列。基因组是数十亿年进化的产物，这个过程由随机突变驱动。鉴于这种随机输入，产生的基因组本身是[算法](@article_id:331821)随机的吗——也就是说，是一个不可压缩的信息串？答案是响亮的“不”。自然选择作为一个强大的过滤器，一个确定性的[算法](@article_id:331821)，无情地偏爱那些能创造功能性、有组织结构的序列。结果是，基因组是高度结构化的，充满了模式、重复和复杂的规则。它绝非随机；它是一条高度压缩的信息，一份构建一个有机体的食谱。进化是一个宏伟的过程，它以随机噪声为原材料，经过亿万年的时间，产生了深刻而复杂的秩序 [@problem_id:1630666]。这让我们回到了原点。一个序列可以看起来很复杂，并且是[随机过程](@article_id:333307)的结果，但却包含着深刻的、可压缩的结构。而在另一个极端，就像沙堆实验中一粒一粒地添加沙子一样，一个具有简单规则的完全[确定性系统](@article_id:353602)可以产生行为——[雪崩](@article_id:317970)的大小——这种行为极其不可预测，在世人看来完全是随机的 [@problem_id:2441709]。

确定性与随机性之间的舞蹈是所有科学中最深刻、最富有成果的主题之一。从测试我们自己的创造物到理解宇宙及我们在其中的位置，[伪随机性](@article_id:326976)的概念不是一个狭隘的话题。它是一个基本的透镜，通过它，我们可以看到连接计算机逻辑、科学家方法以及自然本身智慧的隐藏统一性。