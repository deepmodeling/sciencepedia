## 引言
在构建能够理解世界的模型的探索中，我们常常使用概率语言来描述数据中隐藏的结构。最终目标是计算后验分布——即在看到数据后我们应该相信什么——但这在计算上往往是难以处理的，尤其是在当今这个拥有海量数据的时代。这就产生了一个巨大的鸿沟：我们如何才能为复杂的大规模问题释放[贝叶斯推理](@article_id:344945)的力量？本文通过介绍一种强大且可扩展的近似技术——[随机变分推断](@article_id:640207)（SVI），来应对这一挑战。我们将首先深入探讨其核心的“原理与机制”，探索它如何通过最大化[证据下界](@article_id:638406)（ELBO）和利用随机梯度来提速，从而将一个不可能的积分问题转化为一个可处理的优化问题。随后，“应用与跨学科联系”部分将展示SVI如何作为发现的引擎，彻底改变从机器学习本身到基因组学、天体物理学和金融学等多个领域。

## 原理与机制

在我们构建能真正理解世界的模型的征途中，我们常常发现自己陷入一种奇特的困境。我们可以写下对现实世界精美而复杂的描述——基因间微妙的相互作用、庞大的客户偏好网络，或是一个图书馆中所有文本的隐藏主题。我们可以将这些想法形式化为概率模型，这些模型通常涉及捕捉我们所认为存在的底层结构的隐藏（或称**潜**）变量$z$。我们所追求的目标是**[后验分布](@article_id:306029)**$p(z|x)$，它告诉我们在观测到数据$x$之后，我们应该对这些隐藏的成因$z$持有什么样的信念。这个[后验分布](@article_id:306029)是所有知识的关键，是我们科学探问中“为什么”的答案。

然而，它几乎总是被锁在我们无法触及的地方。要使用[贝叶斯法则](@article_id:338863) $p(z|x) = p(x|z)p(z) / p(x)$ 来计算它，我们必须计算分母 $p(x) = \int p(x|z)p(z) dz$。这个看起来无害的积分，即边缘似然或**证据**，是现代统计学的“阿喀琉斯之踵”。对于任何具有一定复杂性的模型，它都涉及到对天文数字般多的可能性进行求和，这项任务可能需要宇宙的年龄那么长的时间才能完成。宝藏被锁在保险库中，其密码组合之长令人望而却步。那么，我们该怎么办呢？我们选择成为聪明的谈判者。

### 一项有原则的交易：[证据下界](@article_id:638406)

如果我们无法找到精确的后验分布，或许我们可以找到一个好的近似。这是**[变分推断](@article_id:638571)**的核心思想。我们提出一个更简单、可处理的分布族，称之为 $q_{\phi}(z|x)$，由一些参数 $\phi$ 控制。可以想象这是一族简单的形状，比如高斯分布，我们可以通过调整 $\phi$ 来拉伸和移动它们。我们的目标是找到这个族中与真实、复杂的后验分布 $p(z|x)$ 最接近的那个成员。

我们如何衡量“接近程度”？我们使用信息论中的一个概念，称为**Kullback-Leibler (KL) 散度**，它量化了一个[概率分布](@article_id:306824)与另一个[概率分布](@article_id:306824)的差异程度。我们的目标是最小化 $\mathrm{KL}(q_{\phi}(z|x) \| p(z|x))$。神奇之处在于，当我们写出这个定义时，经过一番代数[重排](@article_id:369331)，我们会发现一个优美的恒等式 [@problem_id:3184459]：

$$
\log p(x) = \mathcal{L}(\phi) + \mathrm{KL}(q_{\phi}(z|x) \| p(z|x))
$$

这里，$\log p(x)$ 是我们想要计算的那个难以处理的证据的对数。KL散度是我们的近似与真实情况之间的“距离”。而 $\mathcal{L}(\phi)$ 是一个新量，我们称之为**[证据下界](@article_id:638406)（Evidence Lower Bound, ELBO）**。

这个方程是现代机器学习中最重要的方程之一。由于KL散度永远不会是负数，它告诉我们 $\log p(x) \ge \mathcal{L}(\phi)$。ELBO，顾名思义，永远是证据对数的一个下界。这意味着，如果我们最大化ELBO，我们实际上就在隐式地最小化KL散度，从而推动我们的近似 $q_{\phi}$ 尽可能地接近真实的[后验分布](@article_id:306029) $p(z|x)$！真实证据对数与我们通过ELBO能达到的值之间的“差距”，正是这个KL散度 [@problem_id:3110823]。如果我们的近似族 $q$ 足够灵活，能够包含真实的[后验分布](@article_id:306029)，我们就可以让这个差距完全消失 [@problem_id:3184459]。

我们已经将一个难以处理的积分问题转化为了一个优化问题。这是一笔极好的交易。但是，我们正在最大化的这个ELBO究竟是什么样的呢？它分解为两个非常直观的项 [@problem_id:3110823]：

$$
\mathcal{L}(\phi) = \mathbb{E}_{z \sim q_{\phi}(z|x)}[\log p(x|z)] - \mathrm{KL}(q_{\phi}(z|x) \| p(z))
$$

第一项，$\mathbb{E}_{z \sim q_{\phi}(z|x)}[\log p(x|z)]$，是**重构似然**。它表达的意思是：“让我们从当前最好的猜测 $q_{\phi}$ 中抽取一些假设的[潜变量](@article_id:304202) $z$。平均而言，这些[潜变量](@article_id:304202)能在多大程度上解释我们观测到的真实数据 $x$？”这一项促使我们的模型能够很好地重构数据。

第二项，$-\mathrm{KL}(q_{\phi}(z|x) \| p(z))$，充当一个**正则化项**。它惩罚我们的近似 $q_{\phi}$ 过多地偏离**先验**分布 $p(z)$，这个[先验分布](@article_id:301817)代表了我们*在看到任何数据之前*对[潜变量](@article_id:304202)的信念。它使我们的近似保持“诚实”，防止它为了仅仅拟合当前的数据点而以奇异的方式扭曲自己。

因此，最大化ELBO是一个优美的平衡艺术：找到能够很好地拟合数据的[潜变量](@article_id:304202)解释，但同时又不能过分偏离你的先验信念。

### 攀登高峰：从批量到随机

这是一个出色的框架，但我们很快就撞上了下一堵墙：大数据。要为一个大型数据集计算ELBO及其梯度，我们必须对每一个数据点的贡献进行求和。对于一个拥有数百万分离株的[细菌基因表达](@article_id:359779)模型，或一个涵盖整个互联网的主题模型，这根本是不可行的 [@problem_id:2479917]。我们优化的每一步都将是极其缓慢的。

这正是**[随机变分推断](@article_id:640207)（SVI）**中“随机”二字的用武之地。这是一个借鉴自[深度学习](@article_id:302462)领域的、 brilliantly simple yet profound 的想法。我们不再使用整个数据集来计算ELBO的梯度，而是使用一个随机选择的小**批量（minibatch）**数据来估计它。

让我们想象真实的梯度是一百万个数据点的平均值。我们可以通过仅采样（比如说）100个数据点并计算它们的平均值来得到这个平均值的一个相当好的无偏估计 [@problem_id:3192033]。这个估计会有噪声——它会在真实值周围[抖动](@article_id:326537)。但它的计算速度会快上*数千倍*。我们可以在计算一个巨大、精确的步长所需的时间内，朝着正确的方向迈出数千个微小但带噪声的步伐。这种权衡——用计算速度来换取梯度方差——正是让我们能够将这些复杂模型应用于海量数据集的引擎。

### 驯服梯度的[抖动](@article_id:326537)

这种新获得的速度是有代价的：噪声。在优化过程中，我们的参数所走的路径可能看起来不像是在坚定地登山，更像是一个醉汉的蹒跚。SVI的艺术在于驯服这种噪声，将蹒跚的步伐转变为优雅而迅速的攀登。

一个我们可以调节的明显旋钮是小批量的大小 $b$。我们[梯度估计](@article_id:343928)器的方差通常与 $1/b$ 成正比。更大的批量会产生更稳定的估计，但速度更慢。这催生了巧妙的**自适应[批量大小](@article_id:353338)**策略 [@problem_id:3192033]。当我们远离最优解时，真实梯度很大，一个粗略、带噪声的估计就足以指引我们朝大致正确的方向前进。当我们接近顶峰时，地形变得平坦，真实梯度变小。这时，噪声可能会淹没信号，所以我们需要一个更精确的方向。在这个阶段，我们可以增加[批量大小](@article_id:353338)以减少噪声，从而小心地导航至顶峰。

另一个源于经典力学的强大思想是**动量（momentum）** [@problem_id:3149972]。运动中的物体倾向于保持运动。我们不再让参数更新完全由当前（带噪声的）梯度决定，而是加上一部分前一次更新的方向。这就像惯性一样。动量项有助于平均掉[梯度噪声](@article_id:345219)的高频[抖动](@article_id:326537)，使轨迹变得平滑。它使得优化过程能在持续的下坡方向上累积速度，并抑制在狭窄峡谷间的[振荡](@article_id:331484)，通常能极大地加快[收敛速度](@article_id:641166)。

最后，我们可以在一个更根本的层面上解决方差问题。梯度本身通常是一个[期望值](@article_id:313620)，通过使用**[重参数化技巧](@article_id:641279)**进行采样来估计。例如，要从高斯分布 $z \sim \mathcal{N}(\mu, \sigma^2)$ 中采样，我们可以转而从 $\epsilon \sim \mathcal{N}(0, 1)$ 中采样，然后计算 $z = \mu + \sigma \epsilon$。这将随机性与参数分离开来，实现了低方差的[梯度估计](@article_id:343928)。但即便如此，其中仍有我们可以减少的方差。通过使用一种称为**控制变量**的统计技术，我们可以减去一个与我们的估计器相关且已知均值为零的量 [@problem_id:3166728]。这就像试图测量一个在充气城堡上的人的身高。测量结果充满噪声。但如果我们能同时测量城堡的运动并将其减去，我们就能得到一个关于此人真实身高更稳定的估计。通过精心设计这些控制变量，我们可以从[梯度估计](@article_id:343928)器中精准地移除方差源，从而实现更稳定、更快速的训练。

### 回报：快速、摊销的推断

在经历了从有原则的近似到[随机优化](@article_id:323527)和[方差缩减](@article_id:305920)的整个旅程之后，我们构建了什么？我们创造了一台高度可扩展的机器，用于执行近似贝叶斯推断。但真正的神来之笔是所谓的**摊销推断** [@problem_id:3184459]。

在许多应用中，我们使用一个[神经网络](@article_id:305336)（一个“推断网络”）来为任何给定的输入 $x$ 输出我们的近似 $q_{\phi}(z|x)$ 的参数 $\phi$。通过训练这个网络，我们不仅仅是为单个、固定的数据集找到了[后验分布](@article_id:306029)。我们学到了一个*函数*，这个函数可以立即将任何*新*的数据点映射到其近似后验的参数。

这是一个颠覆性的改变。像[马尔可夫链](@article_id:311246)蒙特卡洛（MCMC）这样的“黄金标准”方法虽然强大，但对于我们想要获得后验的每一个测试点，都必须运行一个新的、通常很漫长的模拟过程 [@problem_id:2479917]。而一个用SVI训练的VAE则在训练期间完成了所有繁重的工作。在测试时，寻找近似后验就像通过[神经网络](@article_id:305336)进行一次[前向传播](@article_id:372045)一样快。对于需要实时分析数千个输入的应用——如从图像进行医学诊断或内容审核——这种摊销的、即时的推断不仅仅是一个优势；它是一项赋能技术。

归根结底，[随机变分推断](@article_id:640207)是一个关于优美妥协的故事。它始于用一个可处理的下界换取无法获得的精确后验。然后，它用批量梯度的穷尽确定性换取随机梯度的惊人速度。最后，它利用一整套统计和优化技巧来驯服由此产生的噪声。其结果是现代科学家武器库中最强大、最通用的推断引擎之一，证明了有时通往解决方案最巧妙的路径并非最直接的那条。

