## 引言
在探索和改造我们周围世界的过程中，我们依赖于两种强大的工具：永恒的物理定律和现代机器学习的能力。传统上，这两者在各自独立的领域中运作。[物理模拟](@article_id:304746)虽然严谨，但常受制于不完美的模型；而机器学习虽具灵活性，却可能需要大量数据，并且未能遵循基本原理。可微物理作为一种革命性的[范式](@article_id:329204)应运而生，它融合了这两个世界，创造出能够在科学基石原理的指导下从数据中学习的智能计算系统。这种方法不仅解决了我们如何求解方程的问题，更关键的是，它触及了我们是否在求解正确的方程这一根本性差距。

本文全面概述了可微物理的核心概念及其变革性潜力。首先，在“原理与机制”一节中，我们将深入探讨其基本思想，探索物理定律如何被编码为可微约束，以及[自动微分](@article_id:304940)如何充当学习和发现的引擎。我们将解析物理信息神经网络（PINN）、混合损失函数以及[归纳偏置](@article_id:297870)的关键作用等概念。随后，在“应用与跨学科联系”一节中，我们将遍览一系列实际应用，从揭示隐藏结构的[逆问题](@article_id:303564)求解，到设计新型材料和蛋白质的生成模型，展示这种方法如何重塑科学探究和工程设计。

## 原理与机制

要真正领会可微物理的力量，我们必须深入其内部。这不是魔法，而是[经典物理学](@article_id:310812)、现代机器学习以及一种巧妙地将它们联系在一起的计算技巧的美妙结合。想象你是一名试图破案的侦探。你有一些零散的线索——一些确凿的证据——但你同时也对人性和逻辑有深刻的理解，这告诉你事件*必然*是如何联系起来的。可微物理的运作方式与此相同：它将稀疏的数据“线索”与物理定律的普适“逻辑”相结合。

### 两个差距：模型与现实

在任何模拟现实世界的尝试中，我们都面临着不止一个，而是两个根本性的差距。理解这一区别是我们征途的第一步。让我们考虑一个情景，工程师们正在为一个通道内的热流建模 [@problem_id:2370228]。

首先是**[模型误差](@article_id:354816)**。这是宇宙的*真实物理过程*与我们选择用来表示它的*数学方程*之间的差距。也许我们的热流方程假设流体是静止的（纯扩散），但实际上，流体在运动，并随之带走热量（平流）。无论我们多么完美地求解简化的方程，我们的答案都会是错误的，因为模型本身就有缺陷。这是一个验证（validation）问题：我们求解的方程正确吗？

其次是**离散误差**。这是我们所选方程的*精确理论解*与我们计算机计算出的*近似解*之间的差距。计算机无法处理无限连续的[时空](@article_id:370647)，因此它们将其切割成有限的片段（网格）并求解一个近似值。传统的[数值分析](@article_id:303075)在过去几十年里发展了许多卓越的方法来估计和最小化这种误差。这是一个校验（verification）问题：我们正确地求解方程了吗？

巨大的挑战在于，一个标准的误差估计器可能会告诉我们，我们的离散误差非常小——即我们的计算机已经以极高的精度求解了方程——但最终答案可能与实验测量值大相径庭。当[模型误差](@article_id:354816)是导致失败的主要来源时，就会发生这种情况。可微物理提供了一个革命性的框架，通过让我们的方程直接面对真实世界的数据，来解决第一个差距，即[模型误差](@article_id:354816)。

### 数据与理论的结合

我们如何修复一个有缺陷的模型？我们需要让数据来引导我们。但我们不希望在这个过程中抛弃几个世纪以来积累的物理知识。可微物理的核心机制是创造一种两者的和谐融合。这通常通过一个复合**[损失函数](@article_id:638865)**来实现，它是一个衡量我们模型“错误程度”的数学度量，我们随后会尝试将其最小化。

想象一个简单的力学系统，比如一个摆动的钟摆或一个弹簧上的[振动](@article_id:331484)质量块。我们知道它的运动由一个二阶微分方程控制，也许是类似 $\ddot{x}(t) + p_1 \dot{x}(t) + p_2 x(t) = 0$ 的形式，其中 $p_1$ 和 $p_2$ 代表诸如阻尼和刚度之类的物理属性 [@problem_id:1595359]。如果我们不知道 $p_1$ 和 $p_2$ 的确切值怎么办？

策略如下：我们使用一个神经网络，称之为 $\hat{x}(t)$，作为我们解的候选。这个网络输入时间 $t$，输出一个位置 $\hat{x}$。为了训练这个网络并发现未知参数，我们构建一个包含两部分的[损失函数](@article_id:638865) $\mathcal{L}$：

$\mathcal{L} = \mathcal{L}_{data} + \lambda \mathcal{L}_{physics}$

1.  **数据损失 ($\mathcal{L}_{data}$):** 这一项将我们的模型锚定于现实。假设我们有几个[振荡器](@article_id:329170)位置的真实测量值，记为 $(t_d, x_d)$。数据损失可以是平方差 $(\hat{x}(t_d) - x_d)^2$。这一项简单地表示：“无论你做什么，你的解都必须穿过这些测量点。”

2.  **物理损失 ($\mathcal{L}_{physics}$):** 这一项在所有其他地方强制执行物理定律。我们无法测量*每一个*时刻的位置，但我们知道物理定律在任何地方都成立。我们定义一组**配置点**——即在我们感兴趣的域内[均匀分布](@article_id:325445)的精细时间点网格。在每个这样的点 $t_c$ 上，我们计算[微分方程](@article_id:327891)的**[残差](@article_id:348682)**。[残差](@article_id:348682)是将网络解代入控制方程后得到的结果：$\mathcal{R}(t_c) = \ddot{\hat{x}}(t_c) + p_1 \dot{\hat{x}}(t_c) + p_2 \hat{x}(t_c)$。如果网络的解是完美的，[残差](@article_id:348682)将为零。物理损失就是所有配置点上[残差](@article_id:348682)平方的总和，例如 $\mathcal{R}(t_c)^2$。这一项表示：“你的解必须在所有这些其他点上遵守物理定律。”

项 $\lambda$ 是一个超参数，用于平衡拟合数据与遵守物理定律之间的重要性。通过最小化这个组合[损失函数](@article_id:638865)，优化过程必须找到一个函数 $\hat{x}(t)$ 以及参数 $p_1, p_2$，使得它既能尊重稀疏的测量值，*又*能在整个域内满足底层的物理定律。这就是我们执行**参数反演**并让数据“修正”我们模型的方式。

### 发现的引擎：[可微性](@article_id:301306)

这听起来很棒，但计算机究竟是如何最小化这个[损失函数](@article_id:638865)的呢？最常用的方法是**[梯度下降](@article_id:306363)**。想象一下，损失函数是一个广阔的、丘陵起伏的地形，其中海拔代表误差。我们的目标是找到最低的谷底。我们从某个随机点开始，在每一步中，计算最陡峭的下坡方向——即负梯度——然后朝那个方向迈出一小步。

要做到这一点，我们需要计算损失 $\mathcal{L}$ 相对于我们所有想学习的东西的[导数](@article_id:318324)：神经网络的权重，以及关键的物理参数如 $p_1$ 和 $p_2$ [@problem_id:1595359]。这就是奇迹发生的地方。物理[残差](@article_id:348682)本身包含了网络输出的[导数](@article_id:318324)，如 $\dot{\hat{x}}(t)$ 和 $\ddot{\hat{x}}(t)$。因此，为了得到损失的梯度，我们需要对一个本身就包含[导数](@article_id:318324)的函数进行微分！

这通过一种称为**[自动微分](@article_id:304940)（AD）**的计算技术得以实现。与[数值微分](@article_id:304880)（近似且不稳定）或[符号微分](@article_id:356163)（可能导致表达式冗长笨重）不同，AD 是一种巧妙的方法，用于计算作为计算机程序实现的复杂函数的精确[导数](@article_id:318324)。可以把你的模拟想象成由基本的乐高积木——加法、乘法、正弦函数等——构建而成。AD 为每一块积木赋予了根据[链式法则](@article_id:307837)传播[导数](@article_id:318324)的知识。当你构建一个大型结构（你的模拟）时，整个组件都知道如何对其最终输出相对于其任何输入或内部参数进行微分。

这就是可微物理中“可微”的含义。它允许我们将整个模拟视为一个巨大的、可[微分](@article_id:319122)的函数，从而能够通过[基于梯度的优化](@article_id:348458)来发现未知参数、学习隐藏的物理规律，甚至设计新颖的设备。

### 编码知识：硬约束与软约束

我们讨论的混合损失函数是一种**软约束**。它通过在模型不遵守物理定律时施加惩罚来*鼓励*模型遵守。如果惩罚权重 $\lambda$ 太小，模型可能会学会忽略物理定律以拟合数据。

但是，如果我们知道某些物理原理是不可协商的呢？例如，在流经圆形管道的流体中，我们确信管道壁上的速度必须为零（“无滑移”条件）。与其惩罚模型在壁上具有非零速度，我们可以构建一个通过其架构本身就*保证*满足此条件的模型。这是一种**硬约束**。

考虑对半径为 $R$ 的管道中的[速度剖面](@article_id:330108) $u(r)$ 进行建模，其中 $r$ 是距中心的径向距离 [@problem_id:2411045]。我们可以将模型设计成以下形式：

$u_{\theta}(r) = \left(1 - \left(\frac{r}{R}\right)^2\right) \times N_{\theta}(r)$

在这里，$N_{\theta}(r)$ 是一个[神经网络](@article_id:305336)。请注意这个巧妙的乘法因子 $(1 - (r/R)^2)$。当 $r=R$ 时，该因子变为零，迫使 $u_{\theta}(R)$ 的整个表达式为零，无论[神经网络](@article_id:305336)输出什么！边界条件通过构造得到满足。现在，优化器无需再担心边界问题，可以将其全部精力集中在使模型满足管道内部的控制物理（Navier-Stokes 方程）上。这种将已知物理直接[嵌入](@article_id:311541)模型架构的方法是一种极其优雅和强大的技术。

### 泛化的秘诀：良好的[归纳偏置](@article_id:297870)

为什么对于科学问题，一个[物理信息](@article_id:312969)模型会比一个通用的机器学习模型有效得多？答案在于[学习理论](@article_id:639048)中的一个概念，称为**[归纳偏置](@article_id:297870)**。[归纳偏置](@article_id:297870)是模型为了从有限的训练数据泛化到未见情况而做出的一组假设。

*   一个通用的三次多项式模型具有一个[归纳偏置](@article_id:297870)，即世界最好由三阶多项式描述 [@problem_id:3130045]。
*   一个通用的[神经网络](@article_id:305336)具有非常弱的[归纳偏置](@article_id:297870)；它假设底层函数是复杂且高度非线性的，但除此之外别无他设。这使其成为“通用近似器”，但也使其需要大量数据，并且在进行[外推](@article_id:354951)时容易出现怪异行为。
*   一个**[物理信息神经网络](@article_id:305653)（PINN）**具有强大且高度相关的[归纳偏置](@article_id:297870)：它假设解符合物理定律。

通过强制施加像 $h'(x) = \alpha h(x)$ 这样的物理约束，我们极大地缩小了模型可以表示的可能函数的空间——从所有函数的无限维空间缩小到一个简单的一参数族 $C e^{\alpha x}$ [@problem_id:3130045]。如果我们的物理知识是正确的，这种强偏置会非常有帮助。它引导模型避开拟合数据中的噪声，而趋向于真实的底层信号。这使得 PINN 能够从极其稀疏的数据中学习，并且最重要的是，能够比纯数据驱动的方法更可靠地进行**泛化**和**[外推](@article_id:354951)**。物理学提供了一种全局结构，而一个通用模型需要海量数据才能自行发现这种结构。

### 现实检验：[残差](@article_id:348682)中的魔鬼

为免我们认为这是一种万能药，认识到其中的挑战非常重要。这些混合[损失函数](@article_id:638865)的优化地形可能非常难以导航。当我们研究具有复杂特征的问题时，比如 Burgers 方程中的[激波](@article_id:302844)，一个特别棘手的问题就显现出来了 [@problem_id:2432738]。

Burgers 方程，$u_t + u u_x = \nu u_{xx}$，是一个著名的模型，用于描述[对流](@article_id:302247)和[扩散](@article_id:327616)之间的相互作用，其解可以发展出极其陡峭的梯度，即“[激波](@article_id:302844)”。人们可能会在少量数据点上训练一个 PINN，并发现模型完美地拟合了这些点。数据损失接近于零。解甚至可能看起来很合理。

然而，如果我们仔细地在整个域上绘制出物理[残差](@article_id:348682) $\mathcal{R}(x,t)$，我们可能会发现一个令人不安的景象。虽然在流动的平滑区域[残差](@article_id:348682)很小，但在[激波](@article_id:302844)的位置它可能会变得巨大——它会“爆炸”。模型学会了在容易的地方满足物理定律，但在最关键的区域却未能捕捉到困难的动力学。它找到了一个“作弊”的解。这突出表明，简单地将一个[损失函数](@article_id:638865)应用于问题并不总是足够的。当前大量的研究集中在开发更智能的训练策略上，例如自适应加权损失函数，以迫使模型解决这些具有挑战性的物理特征，并确保物理[残差](@article_id:348682)在任何地方都被最小化，而不仅仅是在方便的地方。

