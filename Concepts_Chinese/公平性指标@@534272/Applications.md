## 应用与跨学科联系

我们已经花了一些时间研究公平性指标的齿轮和杠杆——这些数学定义使我们能够就[算法](@article_id:331821)如何对待不同人群提出精确的问题。但是，一堆齿轮，无论制作多么精美，都不等同于一个正常工作的时钟。真正的魔力、真正的美，在于我们看到这些思想如何被付诸实践，它们如何与现实世界的问题联系起来，以及它们如何呼应人类思想其他领域数百年来一直在提出的深刻问题。这是一段从抽象数学到构建公正社会核心意义的旅程。

在最高层面上，这段旅程关乎于驾驭正义的两个基本方面，这是哲学家们长期以来区分的概念。首先是**[分配正义](@article_id:365133)（distributive justice）**，它关乎利益和负担的最终分配。谁能得到救命的药物？谁能获批贷款？谁来承担新政策的成本？我们讨论过的指标，如[群体均等](@article_id:639589)或[均等化赔率](@article_id:642036)，正是我们量化这一点的尝试。它们是我们衡量结果公平性的标尺。但还有**[程序正义](@article_id:359929)（procedural justice）**，它关乎过程本身的公平性。决策过程是否透明？受影响的人是否有发言权？风险是否得到负责任和可问责的管理？一个结果可能碰巧看起来公平，但如果产生它的过程是不透明、武断或排他的，我们很难称之为正义。一个真实世界的项目，比如在发展中国家部署一种新的合成生物学[结核病诊断](@article_id:348357)技术，必须从这两个方面进行评判。我们不仅要衡量诊断技术是否惠及最贫困的社区（[分配正义](@article_id:365133)），还要看这些社区在项目治理中是否有代表，以及该技术的内在风险是否在透明和监督下得到管理（[程序正义](@article_id:359929)） [@problem_id:2738570]。这种公平结果和公平过程的双重视角，为接下来的一切提供了支架。

### 数字社会：[机器学习中的公平性](@article_id:642174)

公平性指标最直接和最具爆发力的应用是在机器学习领域，这里的[算法](@article_id:331821)现在做出的决策塑造着人们的生活和生计。在这里，我们可以看到这些指标被用于技术流程的每个阶段，从检查原材料到构建最终产品，甚至指导其未来的演进。

#### 审计系统：在机器中寻找幽灵

在我们解决问题之前，必须先发现问题。许多现代机器学习系统非常复杂，其偏见在表面上并不明显。它们隐藏在高维数据错综复杂的模式中。那么，我们如何扮演侦探的角色呢？一种优雅的方法是使用线性代数的工具，在数据自身的结构中寻找偏见的“幽灵”。想象一个用于[信用评分](@article_id:297121)的数据集，它包含每个申请人的数十个特征。我们可以问：这个数据中的主导模式是什么？人们在哪些主要方向上存在差异？**[主成分分析](@article_id:305819)（Principal Component Analysis, PCA）**正是一种旨在回答这个问题的数学技术。它能找到数据中方差最大的轴——即“主成分”。这些轴往往是机器学习模型用来进行预测的依据。现在，如果我们发现其中一个主要的变化轴——比如最重要的那个——与种族或性别等受保护属性[强相关](@article_id:303632)，会怎么样？这将是一个巨大的危险信号。这意味着数据的内在结构使得[算法](@article_id:331821)很容易区分不同群体，即使受保护属性本身已被移除。通过衡量这些主成分与敏感属性之间的相关性，我们可以进行审计，在房子盖好之前就揭示出地基中隐藏的裂缝 [@problem_id:2442804]。

#### 修复数据：[伪相关](@article_id:305673)的危险

通常，[算法](@article_id:331821)学会偏见并非出于某种恶意，而是因为它是一个非常好的学生，师从一位非常糟糕的老师：我们这个充满偏见的世界，正如数据所反映的那样。一个经典的例子来自用于检测网络上有害言论的模型。这些模型在大量的互联网文本上进行训练。在这些数据中，与少数群体相关的身份术语（例如，“同性恋”、“黑人”、“跨性别者”）经常成为辱骂的*目标*。模型在寻找模式的过程中，可能会学到一个极其简单化且错误的教训：它将身份术语的出现本身与有害性联系起来。结果呢？像“我是一个自豪的同性恋者”这样的评论可能会被标记为有害，而一条真正充满仇恨但避开了特定关键词的评论却能通过检测。这是一个**[伪相关](@article_id:305673)（spurious correlation）**的问题。模型学到了错误的特征。可以做些什么呢？最简单而强大的想法之一是通过**重加权数据（reweighting the data）**来改变[算法](@article_id:331821)的学习方式。在训练过程中，我们可以告诉模型更多地关注那些*不*包含有害内容的少数群体样本。通过在学习目标（[经验风险最小化](@article_id:638176)函数）中增加这些样本的权重，我们迫使模型更加努力地正确处理它们。它不能再依赖于其懒惰的[伪相关](@article_id:305673)；它必须学习构成有害内容的更深层次、更真实的模式 [@problem_id:3121407]。这是一种重新平衡课程的方法，以便给学生一个更真实的世界图景。

#### 修复模型：将公平性作为设计约束

有时我们得到一个数据集但无法更改它。或者，我们可能有一系列现有的模型，每个模型都有其自身的缺陷。在这些情况下，我们可以将公平性直接融入模型设计或决策过程中。一个引人入胜的前沿领域是生成模型，如**[生成对抗网络](@article_id:638564)（Generative Adversarial Networks, GANs）**，它们可以创建惊人逼真的合成图像、文本和其他数据。如果一个GAN在一个有偏见的人脸数据集（比如，很少有女性高管的图像）上训练，它将在其生成的面孔中学习甚至放大这种偏见。为了解决这个问题，我们可以修改训练过程本身。GAN由一个生成器（艺术家）和一个[判别器](@article_id:640574)（艺术评判家）组成。我们可以给判别器一个额外的工作：充当“公平警察”。除了判断生成样本的真实性外，判别器还检查这批样本是否满足公平性标准，如[群体均等](@article_id:639589)。如果不满足，[判别器](@article_id:640574)会向生成器发送一个惩罚信号。这样，生成器不仅被迫学习如何创建逼真的样本，还要学习如何以对不同群体都公平的方式来创建它们 [@problem_id:3124572]。另一种被称为后处理的方法，类似于组建一个智慧委员会。假设我们有几个不同的[预测模型](@article_id:383073)。模型A可能对一个群体非常准确，但对另一个群体则不然。模型B可能有相反的问题。我们不必只选择一个，而是可以将它们结合起来。我们可以将其构建为一个优化问题：找到最佳权重来组合模型的预测，使得最终的“集成”预测尽可能准确，*同时受限于*其预测必须满足我们的公平性指标（例如，[均等化赔率](@article_id:642036)差异必须低于某个阈值）的约束。这将“公平”这个模糊的目标转化为寻找最佳混合模型过程中的一个具体数学约束 [@problem_id:3098297]。

#### 修复未来：公平的学习过程

公平不仅仅是最终模型的静态属性，它更是整个学习过程的动态特征。当我们考虑到数据并非只是被给予的东西，而是我们主动收集的东西时，这一点就变得很清楚了。在**[主动学习](@article_id:318217)（active learning）**中，模型可以请求它认为学习后受益最大的数据点的标签。但这个选择具有公平性影响。模型应该请求它最不确定的点的标签吗？这可能会迅速提高整体准确性，但可能导致它只学习到关于多数群体的信息，而对少数群体了解不足。另一种策略是明确地从模型表现不佳的群体中抽样，以努力提高公平性。这表明我们对公平性的定义可以指导科学探究和数据收集的全过程，从而随时间塑造模型对世界的认知 [@problem_id:3098387]。更进一步，**[元学习](@article_id:642349)（meta-learning）**或“[学会学习](@article_id:642349)”的领域提供了一个更深刻的视角。[元学习](@article_id:642349)的目标不是为某个特定任务训练一个模型，而是产生一个能够[快速适应](@article_id:640102)许多新任务的模型*初始化*。我们可以将此应用于公平性。我们能否[元学习](@article_id:642349)一个模型的起点，这个起点不仅为准确性做好准备，而且*为公平性做好了准备*？目标是找到一个初始化，当面对来自一个新的、未见过的群体的数据时，它仅用少量样本就能适应，成为对该群体既公平又准确的分类器。这是一个强大的愿景：不仅仅是逐一构建公平的模型，而是创建一个已经学会了*如何变得公平*这一通用原则的系统 [@problem_id:3149879]。

### 其他殿堂的回响：跨学科联系

也许这次旅程中最令人智识上满足的部分是，我们意识到在[算法公平性](@article_id:304084)中我们正在努力解决的问题并非新生事物。它们是几十年来甚至几个世纪以来在其他领域被探索的深刻、永恒问题的现代转世。

#### 公平即博弈：[极小化极大原则](@article_id:336386)

考虑在不同社区间分配有限公共资源的问题，比如学校经费或医院床位。这是一个经典的公平性问题。我们可以将其构建为一个双人博弈。您是规划者，目标是尽可能公平地分配资源。您的对手是一个想象中的、吹毛求疵的敌手，其唯一目标是找到受到最不公平对待的那个社区并加以指责。您想要最小化敌手能找到的最大不公平。这是一个**[极小化极大博弈](@article_id:641048)（minimax game）**。[极小化极大定理](@article_id:330581)是博弈论的基石，它告诉我们这类博弈解的性质。在最优的“[鞍点](@article_id:303016)”解上，一件美妙的事情发生了：敌手可能选择的那些群体的结果被均等化了。您的最佳策略是以这样一种方式分配资源，使得所有竞争中的群体的“公平分数”（例如，一种福祉的度量）都相同。您将最坏情况下的结果尽可能地变好。这个强大的思想——即公平的解决方案通常是均等化的解决方案——直接源于[博弈论](@article_id:301173)的冷酷逻辑，并为许多公平性标准提供了深刻的理论依据 [@problem_id:3199129]。

#### 公平即社会选择：投票箱与[算法](@article_id:331821)

当我们审视社会选择理论——对投票进行数学研究的领域时，最终的联系便浮现出来。什么是投票系统？它是一个[算法](@article_id:331821)。其输入是一组个人偏好（选票），其输出是一个集体决策（获胜者）。几个世纪以来，政治理论家和经济学家一直在问：是什么让一个投票[算法](@article_id:331821)变得*公平*？他们发展了一套属性语言来描述这一点。例如，**单调性（monotonicity）**是一个公平属性：如果一个候选人赢得选举，当一些选民在选票上将他排得*更高*时，他不应突然变成失败者。另一个是**无关备选方案独立性（Independence of Irrelevant Alternatives, IIA）**：集体对候选人A和B的偏好不应仅仅因为一些选民改变了他们对一个不相关的第三候选人C的看法而翻转。当我们分析像波达计数法这样的经典投票程序时，我们发现它满足其中一些公平属性（如单调性），但有趣的是，它不满足另一些（如IIA）[@problem_id:3226939]。这反映了我们在[算法公平性](@article_id:304084)方面的经验，即我们在不同指标之间发现了权衡——例如，一个模型不能总是同时满足[群体均等](@article_id:639589)和[均等化赔率](@article_id:642036)。Kenneth Arrow 著名的不可能定理表明，没有任何投票[算法](@article_id:331821)（对于三个或更多候选人）能够同时满足一小撮看似显而易见的公平标准。这是一个深刻而令人谦卑的认识。我们在设计公平[算法](@article_id:331821)时面临的挑战，不仅仅是我们代码中的技术缺陷。它们是将个体需求汇集成一个集体、公平结果这一本质中，基本且经数学证明的悖论的体现。今天的计算机科学家在定义和实现公平性的斗争中，正在延续一场由启蒙运动的政治哲学家和20世纪的经济学家开启的对话。我们的[算法](@article_id:331821)仅仅是这个古老而高尚的舞台上最新的演员。