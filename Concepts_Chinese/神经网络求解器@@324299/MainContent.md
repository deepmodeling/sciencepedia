## 引言
几个世纪以来，[微分方程](@article_id:327891)一直是科学的语言，描述着从[行星轨道](@article_id:357873)到[化学反应](@article_id:307389)的一切。传统的建模方法是首先从基本原理推导出这些方程，然后进行求解。然而，如果其基本原理未知，或者得出的方程过于复杂，我们该怎么办？一个新[范式](@article_id:329204)正在机器学习和科学计算的[交叉](@article_id:315017)领域兴起：利用[神经网络](@article_id:305336)不仅分析数据，还要发现控制规律本身。这标志着一个深刻的转变：从学习一个拟合数据的函数，转向学习生成数据的动态规则。

本文旨在弥合经典数值方法与这类新型基于[深度学习](@article_id:302462)的求解器之间的知识鸿沟。它超越了将[神经网络](@article_id:305336)视为黑箱[插值器](@article_id:363847)的观点，揭示了它们作为科学发现强大工具的本质。在接下来的章节中，您将对这种革命性的方法有一个全面的了解。首先，我们将深入探讨“原理与机制”，探索像神经普通[微分方程](@article_id:327891)（Neural ODEs）这样的模型如何学习系统的连续时间动态，它们如何克服维度灾难等计算障碍，以及它们面临的实践和理论挑战。之后，在“应用与跨学科联系”部分，我们将遍览一系列真实世界的用例，了解这些求解器如何在生物学中创建[数字孪生](@article_id:323264)，与经典工程方法建立强大的联盟，并推动物理学和化学领域复杂性的前沿。

## 原理与机制

想象一下，您想预测一颗行星的运行轨迹。一种方法是获取它过去一千年位置的庞大表格，然后训练一个机器来找到一条拟合这些数据点的曲线。这是一项值得尊敬的任务——称为插值或回归。如果您想知道行星在过去某个时间的位置，只需在您的曲线上查找对应值即可。许多传统机器学习模型就是这样思考的：它们是专业的函数拟合器，学习从输入（时间）到输出（位置）的直接映射。但还有一种更深刻、更优美的方式来理解行星的运动。如果我们不只是记忆路径，而是学习支配这条路径的*定律*，那会怎样？如果我们能直接从数据中学习牛顿的[万有引力](@article_id:317939)定律 $F = G \frac{m_1 m_2}{r^2}$，那会怎样？

如果我们知道了定律，我们就不再局限于已经走过的路径。我们可以预测行星遥远未来的运动，引入一颗新卫星看看会发生什么，或者模拟整个太阳系。我们将不仅仅捕捉到数据，还捕捉到生成这些数据的*动态*——即变化的规则。这正是[神经网络求解器](@article_id:344162)核心思想的深刻转变。

### 学习游戏规则

传统的神经网络可能会被训练来学习一个函数 $P(t)$，该函数直接根据时间 $t$ 预测系统的状态（例如，蛋白质浓度）。它本质上是您所提供数据点的一个精密[插值器](@article_id:363847)。而**神经普通[微分方程](@article_id:327891)（Neural ODE）**则做了更有趣的事情。它不学习状态随时间变化的函数，而是学习定义状态*变化率*的函数 $f$ [@problem_id:1453788]。该神经网络被用来表示一个[微分方程](@article_id:327891)的右侧：

$$
\frac{d\mathbf{z}}{dt} = f_{\theta}(\mathbf{z}(t), t)
$$

这里，$\mathbf{z}(t)$ 是我们系统的状态（比如行星的位置和速度，或相互作用的化学物质的浓度），而 $f_{\theta}$ 是一个带有参数 $\theta$ 的[神经网络](@article_id:305336)。该网络将*当前*状态 $\mathbf{z}(t)$ 作为输入，并输出[瞬时速度](@article_id:347067)向量 $\frac{d\mathbf{z}}{dt}$，告诉我们状态接下来将如何变化。从本质上讲，神经网络学习了一个潜在的连续时间动态模型——也就是“游戏规则”本身 [@problem_id:1453792]。为了预测未来某个时间点的状态，我们不只是询问网络。我们请求一个标准的数值 ODE 求解器通过从某个初始状态 $\mathbf{z}(0)$ 开始对这些学习到的规则进行时间上的前向积分来“玩这个游戏”。

### 超越教科书：发现复杂的现实

为什么学习规则如此强大？因为我们教科书上写下的简单规则往往就是——过于简单。思考一下狐狸和兔子之间经典的捕食者-被捕食者关系，著名的 [Lotka-Volterra 方程](@article_id:334524)就是对其的建模。例如，这些方程假设狐狸吃兔子的速率与它们种群数量的乘积 $\beta R F$ 成正比。这意味着狐狸的食欲是无限的，而兔子也永远不会更擅长躲藏。

真实世界要混乱得多。当兔子数量充足时，一只狐狸能吃的量是有限的；它的捕食率会*饱和*。当兔子稀少时，少数幸存的兔子是躲在洞穴里的专家，形成了一个“避难所”，使得捕食率急剧下降。我们可以尝试手动构建更复杂的方程来解释这些效应，但可能会忽略其他更微妙的相互作用。

这正是神经普通[微分方程](@article_id:327891)（Neural ODE）大放异彩的地方。我们可以不假设一个固定的数学形式如 $\beta R F$，而是让神经网络直接从真实的种群数据中学习整个相互作用函数。作为一个[通用函数逼近器](@article_id:642029)，该网络足够灵活，可以自行发现并表示这些复杂的非线性效应，如饱和和避难所效应，而无需我们明确指定它们 [@problem_id:1453830]。它学习到一个数据驱动的**[向量场](@article_id:322515)**，捕捉了生态系统真实、混乱的动态，而不仅仅是一个简化的教科书版本。

### 连续时间的自由

这种连续时间的视角也提供了深刻的实践优势，尤其是在处理真实世界数据时。许多流行的时间序列模型，如**[循环神经网络](@article_id:350409)（RNNs）**，是在[离散时间](@article_id:641801)内运行的。它们就像一部有固定帧率的电影，一次处理一步数据：$h_1, h_2, h_3, \dots$。如果您的数据以规则的时间间隔到达，这种方法效果很好。但如果不是呢？比如您是一位监测病人的医生，测量时间分别是上午 8:05、上午 11:23 和下午 4:40？RNN 会要求您笨拙地处理数据，比如假设中间没有发生任何事，或者在规则的网格上插入虚假的测量值。

而神经普通[微分方程](@article_id:327891)（Neural ODE）本质上是一个连续时间模型。由于它已经学习了规则 $\frac{d\mathbf{z}}{dt} = f(\mathbf{z}, t)$，我们可以要求一个 ODE 求解器在我们希望的任意时间区间上对这个规则进行积分——例如，从上午 8:05 到上午 11:23。它能够原生而优雅地处理不规则采样的数据，这对于医学、生物学和天文学等领域的应用来说是一个至关重要的特性 [@problem_id:1453831]。

### 一项超能力：克服[维度灾难](@article_id:304350)

到目前为止，我们的例子只涉及少数几个变量。但是，当我们转向金融、量子力学或[流体动力学](@article_id:319275)中的问题时，情况会怎样呢？在这些领域，系统状态可能由成千上万甚至数百万个变量来描述。在这里，我们遇到了计算科学中的一个巨大难题：**[维度灾难](@article_id:304350)**。

想象一下，试图为一个具有 $d$ 维的系统求解其控制方程——现在是一个[偏微分方程](@article_id:301773)（PDE）。一种经典方法是创建网格。如果在一个维度上需要 100 个点来获得足够的分辨率，那么在二维中就需要 $100^2 = 10,000$ 个点，三维中需要 $100^3 = 1,000,000$ 个点，而在 $d$ 维中则需要 $100^d$ 个点，这是一个荒谬的数字。[计算成本](@article_id:308397)呈指数级爆炸，使得基于网格的方法对于任何超过几个维度的问题都完全无望。

[神经网络求解器](@article_id:344162)，特别是那些用于高维[偏微分方程](@article_id:301773)和[倒向随机微分方程](@article_id:371456)（BSDEs）的求解器，用一个简单而巧妙的想法解决了这个难题：采样。它们不试[图构建](@article_id:339529)一个大得不可能的网格来覆盖整个空间，而是依赖于[蒙特卡洛方法](@article_id:297429)的逻辑。它们在多维空间中模拟了数量可控的随机路径或场景。把它想象成一次政治民意调查：您不需要调查每一个公民（$K^d$）来获得选举结果的准确图像；一个精心挑选的随机样本（$M$）就足够了。[蒙特卡洛估计](@article_id:642278)的误差随着样本数量 $M$ 的增加而以 $M^{-1/2}$ 的速率减小，这个速率完全独立于维度 $d$！虽然复杂度仍然随维度增长（通常是多项式级，而非指数级），但这已经是天壤之别。这种方法，结合深度神经网络已被证明能够有效逼近某些类别的高维函数的能力，使我们能够为成百上千维度的问题找到解决方案，而这些问题在十年前是完全无法触及的 [@problem_id:2969616]。

### 工程师的工具箱：如何实现

这一切听起来很美妙，但我们实际上如何构建和训练这些强大的机器呢？有两个关键思想使其变得实用：一个用于注入先验知识，另一个用于使训练过程在计算上可行。

#### 强制执行已知信息

神经网络功能强大，但我们不应该强迫它们重新发现我们已知的确定事实。如果我们正在模拟一座桥梁的挠度，我们*知道*它的两端是固定在地上的。我们可以通过两种主要方式来整合这种被称为**边界条件**的物理约束。

第一种是**软执行**。我们让网络的输出完全自由，但在[损失函数](@article_id:638865)中增加一个惩罚项。如果网络预测桥梁的末端在移动，[损失函数](@article_id:638865)就会得到一个很大的惩罚，从而推动训练[算法](@article_id:331821)去寻找满足该条件的参数。第二种是**硬执行**。我们改变神经网络自身的架构，使其在*数学上不可能*违反边界条件。例如，我们可以将网络的输出 $u_{\theta}(x)$ 表述为 $u_{\theta}(x) = \text{boundary\_term}(x) + \text{vanishing\_term}(x) \times N(x; \theta)$。`boundary_term` 负责在边界处取正确的值，而 `vanishing_term` 被设计为在边界处为零，从而确保神经网络 $N(x; \theta)$ 可以在内部区域做任何事情而不会干扰已知的边界值。硬执行保证了物理定律得到遵守，而软执行则提供了更大的灵活性，但可能以牺牲准确性和增加训练难度为代价 [@problem_id:2656059]。这种将已知物理定律与数据驱动的[函数逼近](@article_id:301770)器相融合的方式是**[物理信息神经网络](@article_id:305653)（PINNs）**的一大特点。

#### [伴随方法](@article_id:362078)的节省内存魔法

当我们尝试训练神经普通[微分方程](@article_id:327891)（Neural ODE）时，会出现一个主要障碍。训练[神经网络](@article_id:305336)需要反向传播——计算[损失函数](@article_id:638865)相对于网络所有参数的梯度。对于一个神经普通[微分方程](@article_id:327891)，最终状态是 ODE 求解器执行的一长串操作的结果。一种天真的[反向传播](@article_id:302452)方法是展开这整个（可能数百万步的）求解器步骤链，并存储所有中间结果。内存成本将是天文数字，与求解器步数成正比。

解决方案是一项优美的数学技术，称为**[伴随灵敏度方法](@article_id:323556)**。它不存储整个前向路径，而是通过*反向*求解第二个相关的[微分方程](@article_id:327891)——伴随方程——来计算梯度。这种方法巧妙地动态重构所需信息，使其能够计算损失函数相对于网络参数的精确梯度，而内存成本是恒定的，且与求解器所走的步数无关！正是这一项创新使得训练神经普通[微分方程](@article_id:327891)在实践中变得高效可行，即使对于非常长的时间跨度或高精度模拟也是如此 [@problem_id:1453783]。

### 提醒：新方法的艺术与科学

与任何革命性工具一样，[神经网络求解器](@article_id:344162)也有其自身独特的特性和“陷阱”。为了明智地使用它们，我们必须既欣赏其强大之处，也认识到其局限性。

#### 无法自我解释的神谕

一个神经普通[微分方程](@article_id:327891)（Neural ODE）或许能从蛋白质数据中完美地学习到[细胞周期](@article_id:301107)的复杂动态。它变成了一个完美的神谕，能够以惊人的准确性预测细胞的未来。但如果我们问这个神谕*为什么*，它就沉默了。如果我们试图查看训练好的网络内部，并问：“哪个特定的权重对应于蛋白质 A 对蛋白质 B 的抑制作用？”，我们找不到简单的答案。关于那单一生物相互作用的知识并没有局限在单个参数中，而是以一种复杂、非唯一且纠缠的方式，弥散并分布在成千上万个[权重和偏置](@article_id:639384)中。多组不同的参数可能导致完全相同的动态行为。这是一个根本性的权衡：我们以牺牲经典模型简单的一对一可解释性为代价，获得了巨大的表达能力 [@problem_id:1453837]。

#### 光滑表面下的隐藏粗糙度

这里有一个最后且微妙的意外。您训练一个网络来表示[引力场](@article_id:348648)。您绘制其输出，它看起来非常平滑。然后您使用一个自适应 ODE 求解器来模拟一个探测器飞越该场。令您震惊的是，模拟几乎陷入停顿，求解器采用了极小的时间步长。出了什么问题？罪魁祸首是隐藏的粗糙度。虽然网络输出的*值*可能形成一个光滑的[曲面](@article_id:331153)，但其*高阶导数*可能极其“尖锐”甚至不连续。这是网络内部构建块（如流行的 ReLU [激活函数](@article_id:302225)，其[导数](@article_id:318324)是阶跃函数）的产物。自适应求解器使用对这些[高阶导数](@article_id:301325)敏感的公式来估计[局部误差](@article_id:640138)。当求解器遇到这种“隐藏粗糙度”的区域时，其[误差估计](@article_id:302019)会爆炸，并陷入恐慌，急剧减小步长以保持在容差范围内 [@problem_id:1659020]。从某种意义上说，这个光滑的函数是一个美丽的幻象，其背后隐藏着一个锯齿状的基础。

#### 数学家对保证的需求

最后，当我们在传统的[科学计算](@article_id:304417)框架中使用这些网络时——例如，在 Newton 法求解器中使用神经网络来近似雅可比矩阵——我们必须小心。经典的[数值方法](@article_id:300571)带有关于其可靠性和收敛速度的严格[数学证明](@article_id:297612)。然而，这些证明依赖于对所用近似质量的严格假设。一个在数据上训练的[神经网络](@article_id:305336)，无法*先验地*保证其对[雅可比矩阵](@article_id:303923)的近似能够以定理所要求的精确方式“足够好”。没有对网络误差的这种定量界限，所有关于收敛的理论保证都将消失。该方法可能效果非凡，也可能严重发散。这突显了一个激动人心的前沿领域：正在进行的工作旨在将[深度学习](@article_id:302462)的经验性力量与经典[数值分析](@article_id:303075)的严格、可证明的保证联系起来 [@problem_id:2381889]。