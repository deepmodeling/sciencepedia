## 引言
在人工智能与物理科学的融合中，一个深刻的挑战浮现出来：我们如何教导机器不仅通过数据，而且通过支配世界的基本定律来理解世界？标准的机器学习模型功能强大，但往往需要海量数据集，且如同“黑箱”一般运作，忽略了数个世纪以来建立的科学原理。在数据稀疏但物理定律已知的科学和工程等高风险领域，这一差距限制了它们的可靠性。本文介绍了一种变革性的解决方案：[物理损失函数](@article_id:641312)，它是物理信息神经网络（[PINNs](@article_id:305653)）背后的引擎。我们将探讨这种创新方法如何将物理定律直接[嵌入](@article_id:311541)神经网络的训练过程中。在第一章“原理与机制”中，我们将剖析[物理损失函数](@article_id:641312)的构成，揭示它如何约束模型以尊重现实。随后，在“应用与跨学科联系”一章中，我们将见证这一概念如何开启新的前沿，从求解复杂方程到发现隐藏的物理参数，乃至从零开始设计新颖的材料。让我们首先揭示那些让[神经网络](@article_id:305336)能够学习物理语言的基本原理。

## 原理与机制

想象你是一位在犯罪现场的侦探。你有一些零散的线索——这里一个脚印，那里一枚指纹。这是你的**数据**。但你还有更强大的武器：你理解人类行为、逻辑和物理学的法则。你知道人不能穿墙而过，也不能同时身处两地。这些是你的**[第一性原理](@article_id:382249)**。一个好的侦探不仅仅是查看线索，他们会构建一个既与线索相符又与这些基本 법칙相符的叙述。物理信息神经网络（[PINNs](@article_id:305653)）就是我们的科学侦探。它们通过巧妙地将实验数据的稀疏线索与物理定律的普适法则编织在一起，学会讲述一个物理系统的故事。

### 没有法则的学习之徒劳

这听起来可能有些奇怪，但若不做出任何假设，学习从根本上来说是不可能的。在机器学习的世界里，这个思想被形式化为所谓的**[没有免费午餐定理](@article_id:638252)**。它本质上是说，如果你对一个问题不做任何假设，那么任何学习[算法](@article_id:331821)在平均表现上都不会优于随机猜测。如果任何可能的函数都同样可能是“真实”答案，那么看到几个数据点对于你未曾见过的点来说， абсолютно没有告诉你任何信息。要进行预测，你必须拥有所谓的**[归纳偏置](@article_id:297870)**——一种关于你所寻找答案性质的内置假设。

这正是与物理学深刻联系之所在。若非我们拥有的最强大、最经受时间考验的归na偏置，物理定律又是什么呢？[@problem_id:3153391]当我们假设一个系统遵守[能量守恒](@article_id:300957)定律时，我们正在做一个巨大的假设，这个假设极大地缩小了可能行为的空间。我们在说，在系统*可能*做的所有疯狂事情中，它只会做那些保持其总能量恒定的事情。这个假设并非盲目猜测；它是在数百年实验与理论的熔炉中锻造出的原理。一个PINN采纳了这个思想并加以发扬。它使用一个系统的控制方程——即其[偏微分方程](@article_id:301773)（PDEs）——作为其主要的[归纳偏置](@article_id:297870)。网络不仅仅被要求拟合数据，它被约束到只考虑那些“说物理语言”的解。

### PINN大脑的解剖：损失函数

那么，我们如何教一个[神经网络](@article_id:305336)尊重物理定律呢？我们不能仅仅对它说教。相反，我们设计一个非常特殊的“记分卡”，称为**损失函数**。我们将[神经网络](@article_id:305336)称为函数$\mathcal{N}(\mathbf{x}; \theta)$，它依赖于输入$\mathbf{x}$和可训练参数$\theta$。网络对系统的行为做出猜测，然后我们使用[损失函数](@article_id:638865)对这个猜测进行评分。训练的目标是调整参数$\theta$以获得尽可能低的分数。

一个PINN的[损失函数](@article_id:638865)不是一个单一的数字，而是由几个项精心调配的混合物，每一项代表一个不同的目标。让我们看看这些成分：

#### 数据损失：现实的锚点

首先，我们有**数据损失**，$\mathcal{L}_{data}$。这是传统机器学习中最熟悉的组成部分。它衡量网络预测与我们拥有的实际实验测量值之间的不匹配程度。对于一组数据点$(x_i, u_i)$，它可能看起来像这样：

$$
\mathcal{L}_{data} = \frac{1}{N} \sum_{i=1}^{N} (\mathcal{N}(x_i; \theta) - u_i)^2
$$

这一项如同一个锚，将我们的模型 tethering 到现实中。想象一个PDE有无限多个可能的解族。数据损失使我们能够挑选出通过我们特定测量点的那*一个*解。它扮演的角色与传统求解PDE时边界和初始条件的作用相同，即为一个普遍定律提供具体的上下文[@problem_id:2126334]。即使是几个稀疏的数据点也可以作为强大的约束，引导PINN在无数可能性中找到正确的解。

#### 物理损失：法则之声

奇迹在这里发生。**物理损失**，$\mathcal L_{physics}$，惩罚网络违反控制性物理定律的行为。我们通过计算方程的**[残差](@article_id:348682)**来做到这一点。假设我们的控制方程是$\mathcal{F}(u) = 0$形式的PDE。[残差](@article_id:348682)$r(x; \theta)$，就是将网络的近似解$\mathcal{N}$代入方程后得到的结果：

$$
r(x; \theta) = \mathcal{F}(\mathcal{N}(x; \theta))
$$

如果网络是一个完美的解，那么[残差](@article_id:348682)在任何地方都将为零。由于它不完美（至少一开始不是），[残差](@article_id:348682)将是非零的。物理损失则是遍布整个定义域的大量“配置点”上[残差](@article_id:348682)的均方值：

$$
\mathcal{L}_{physics} = \frac{1}{N_{coll}} \sum_{i=1}^{N_{coll}} r(x_i; \theta)^2
$$

这一项迫使网络的输出在任何地方都符合物理定律的结构，而不仅仅是在数据点上。这就是PINN以一种物理上合理的方式在稀疏测量值之间进行“插值”的方式。这种方法的美妙之处在于其通用性。

- 对于由$\ddot{x}(t) + p_1 \dot{x}(t) + p_2 x(t) = 0$控制的[简谐振子](@article_id:306186)，[残差](@article_id:348682)就是方程的左侧，其中代入了网络的输出$\hat{x}(t)$及其[导数](@article_id:318324)[@problem_id:1595359]。

- 对于一个复杂的[连续介质力学](@article_id:315536)问题，[残差](@article_id:348682)可能是[平衡方程](@article_id:351296)$\nabla \cdot \boldsymbol{\sigma} + \boldsymbol{b} = 0$，其中应力$\boldsymbol{\sigma}$是网络输出及其空间[导数](@article_id:318324)的函数[@problem_id:2668921]。

- 这个框架非常灵活，甚至可以处理**积分-[微分方程](@article_id:327891)**，其中定律同时涉及[导数](@article_id:318324)和积分。对于像$\frac{du}{dx} + \alpha u = \beta \int_a^b K(x,y)u(y)dy + S(x)$这样的方程，[残差](@article_id:348682)计算只需包含积分项的数值近似。核心思想保持不变：写下方程，代入网络，并惩罚剩余的[残差](@article_id:348682)[@problem_id:2126357]。

当然，我们还会为**边界和[初始条件](@article_id:313275)**（$\mathcal{L}_{BC}$）添加损失项。这些只是应用在我们问题定义域的空间和时间边界上的特定约束，确保我们的解被正确定位。

总[损失函数](@article_id:638865)是这些分量的加权和：

$$
\mathcal{L}(\theta) = \mathcal{L}_{data} + \lambda_{phys} \mathcal{L}_{physics} + \lambda_{BC} \mathcal{L}_{BC} + \dots
$$

### 训练之舞：摇摆着走向解决方案

一旦我们有了损失函数，我们可以将其想象成一个巨大的、高维的“[能量景观](@article_id:308140)”，训练过程就开始了。我们想要找到这个景观中的最低点，这对应于最佳的网络参数集$\theta$。实现这一目标的[算法](@article_id:331821)是**[梯度下降](@article_id:306363)**。它就像一个徒步者试图在浓雾中到达山谷底部；他们感受脚下的坡度（梯度），并朝着最陡峭的下坡方向迈出一步。

在现代[深度学习](@article_id:302462)中，我们通常使用**[随机梯度下降](@article_id:299582)（SGD）**。我们不是一次性计算所有数据和所有配置点的损失（这将是计算量巨大的），而是在每一步使用一个小的、随机的“小批量”点。这使得我们计算出的梯度是嘈杂和近似的。但在这里，一个来自[统计力](@article_id:373880)学的精彩类比出现了[@problem_id:2008407]。这种噪声不仅仅是一个缺陷；它是一个特性！

来自[噪声梯度](@article_id:352921)的随机[抖动](@article_id:326537)就像一个**有效温度**。在物理系统中，温度对应于随机的热运动。这种运动允许粒子从小的凹坑和洼地（局部能量最小值）中“[抖动](@article_id:326537)”出来，探索景观以找到一个更深得多的山谷（全局最小值）。同样，SGD中的噪声为训练过程提供了一个有效热能$k_B T_{\text{eff}}$，这有助于它逃离[损失景观](@article_id:639867)中的不良局部最小值，并找到一个更好、更符合物理的解。这个有效温度与训练参数有关：更大的[学习率](@article_id:300654)或更小的[批量大小](@article_id:353338)会增加“热量”，从而允许更剧烈的探索。

### 平衡的精妙艺术

构建损失函数只是战斗的一半。数据、物理、边界条件这些组成部分常常相互冲突，它们的数值尺度甚至物理单位可能大相径庭。简单地将它们相加是灾难的根源。权重因子$\lambda$的选择是一门微妙的艺术。

- **物理 vs. 正则化**：在标准机器学习中，我们经常在损失中添加一个“[权重衰减](@article_id:640230)”项，该项惩罚大的网络权重以防止过拟合并鼓励“更简单”的模型。但在PINN中，物理本身就是最终的[正则化](@article_id:300216)器！如果我们给[权重衰减](@article_id:640230)项过大的权重，我们可能会迫使模型变得“简单”，而代价是违反我们知道是真实的物理定律。这是一个关键的权衡：我们必须相信物理是判断一个“好”解是什么样子的主要指南[@problem_id:2656054]。

- **物理 vs. 数据**：当数据稀疏且嘈杂时，我们必须小心，不要给数据损失项太大的权重。如果我们这样做，网络会扭曲自己以完美拟合嘈雜的数据点，这种现象称为**过拟合**。这将导致一个在其他任何地方都物理上荒谬的解。物理损失必须足够强大，以充当“脚手架”，确保解在稀疏数据锚点之间以尊重底层物理结构的方式进行插值[@problem_id:2668921]。

- **[多物理场](@article_id:343859)平衡**：在耦合的多物理场问题中，如[热弹性](@article_id:318851)问题，当我们同时求解温度和位移时，挑战变得更大[@problem_id:2668953]。力学[残差](@article_id:348682)的单位可能是能量，而热[残差](@article_id:348682)的单位是功率。将它们相加在物理上是无意义的！第一个不可或缺的步骤是**[无量纲化](@article_id:338572)**整个问题，这是物理和工程学中一个经典而强大的技术。通过用特征量缩放所有变量，我们使[损失函数](@article_id:638865)中的所有项都变为无量纲。即便如此，它们的量级也可能不同。最 principled 的解决方案涉及自适应加权方案，这些方案在训练期间自[动平衡](@article_id:342750)不同的损失项，就像一位复杂的指挥家，确保管弦乐队的每个部分都能被听到。

### 从求解到发现

到目前为止，我们已经使用PINNs来*求解*我们知道所有参数的方程。但如果我们不知道呢？如果我们有一些来自[阻尼振荡](@article_id:323145)器的数据，但我们不知道阻尼和刚度系数的确切值呢？在这里，[PINNs](@article_id:305653)揭示了它们的终极技巧：它们可以执行**系统辨識**。

我们可以将未知的物理参数，如阻尼系数$p_1$和刚度$p_2$，视为可训练变量，就像网络自身的[权重和偏置](@article_id:639384)一样[@problem_id:1595359]。然后我们计算损失函数相对于网络参数$\theta$的梯度，同时也计算相对于$p_1$和$p_2$的梯度。[梯度下降](@article_id:306363)[算法](@article_id:331821)随后同时执行两个任务：它调整网络以找到解的最佳近似，*并且*它调整物理参数以找到使控制方程与观测数据最匹配的值。侦探不仅在重建犯罪现场；它还在推断罪犯的作案手法。这将PINN从一个复杂的求解器转变为一个强大的科学发现工具，能够直接从数据中推断出系统的隐藏定律和属性。

