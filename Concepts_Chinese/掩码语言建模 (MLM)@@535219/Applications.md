## 应用与跨学科联系

我们花了一些时间来理解[掩码语言建模](@article_id:641899)（MLM）的机制——这个在巨大规模上进行的、异常简单的“填空”游戏。但是，一个伟大科学原理的真正美妙之处，不仅在于其机制的巧妙，更在于其影响的广度。事实证明，“填空”游戏不仅仅是一个游戏。它是一把钥匙，能解开几乎任何有规则、有上下文、有模式的系统的“语法”。

我们即将看到，语言并不仅限于我们口头和书面的词语。有一种用DNA序列写成的生物学语言，一种用计算机代码写成的逻辑语言，一种用古代文献残片写成的历史语言，甚至还有一种隐藏在电子表格行中的数据语言。通过教机器掌握这个简单的游戏，我们无意中给了它一块通用的罗塞塔石碑。现在，让我们踏上一段旅程，探索这个思想出人意料而又美妙的应用，看看[掩码语言建模](@article_id:641899)不仅在重塑我们的技术，也在重塑我们对世界本身的理解。

### 对人类语言更深层次的驾驭

从 MLM 的本土领域——[自然语言处理](@article_id:333975)——开始是再自然不过的了。在这里，它既能向前看又能向后看的能力——从完整的上下文中理解一个词——使其对语言的驾驭比其前辈们要深刻和细致得多。

这种深刻理解所带来的最惊人的结果之一，是无需经过专门训练就能执行任务的能力。想象一下，你希望一个模型来分类一篇电影评论的情感。传统的方法是收集数千篇标注为“正面”或“负面”的评论，并专门针对这个任务来训练模型。而 MLM 的方法要优雅得多。我们可以简单地给模型一个提示，比如：“这部电影太棒了。它很 `[MASK]`。” 模型经过训练，会用最合理的词来填补空白，可能会预测出“棒极了”、“优秀”或“精彩”等词。如果我们给它的是：“这部电影是一场灾难。它很 `[MASK]`。” 它可能会预测“糟糕”、“差劲”或“烂”。

通过简单地检查模型预测的是哪类词，我们就能推断出原始句子的情感。这就是“零样本”分类的精髓。我们没有重新训练模型；我们只是在与它对话，并利用它天生的语言理解能力来解决我们的问题。然而，这种方法揭示了一个有趣的微妙之处：模型的决定可能对我们用来代表标签的词——我们的“言语化标签”（verbalizers）——非常敏感。如果模型预测了“好”，这篇评论算是“正面”的吗？还是必须预测“很棒”才算？这种敏感性为我们提供了一个观察模型内部语义空间的窗口，向我们展示了它是如何对不同概念进行分组和关联的 [@problem_id:3102497]。

这种对语言的掌握并不局限于单一语种。通过将 MLM 与其他目标相结合，我们可以在不同语言之间搭建桥梁。考虑一个模型，它被给予了大量的英语和法语文本，以及成对的互为翻译的句子。我们可以设计一个联合训练过程。MLM 的目标迫使模型分别学习每种语言内部的语法和词汇。同时，一个*[对比学习](@article_id:639980)*目标教会模型认识到句子“The cat is on the mat”和“Le chat est sur le tapis”应该有非常相似的表示，而“The dog barks”和“Le chat est sur le tapis”则不应该。模型不仅学习了两种语言，还学习了它们之间的*对应关系*，有效地成为了一个在语言边界之上共享概念理解的通用翻译器 [@problem_id:3164805]。

### 破译科学的语言

当我们意识到 MLM 的“填空”游戏可以应用于根本不是由人类书写的文本时，它的真正力量才显现出来。科学充满了其自身的符号语言，而 MLM 正在证明自己是破译这些语言的卓越工具。

#### 生命的语法：[基因组学](@article_id:298572)与[蛋白质工程](@article_id:310544)

也许最深奥的文本莫过于基因组，这本用 A、C、G、T 四个[核苷酸](@article_id:339332)字母写成的生命之书。人类基因组是一部长达三十多亿个字符的文本。通过在大量未标记的 DNA 上训练一个 MLM，模型学会了生物学的“语法”。它学会了识别重复出现的基序，即控制基因如何表达的 DNA 的“词语”和“短语”。它学会了[染色体](@article_id:340234)上相距遥远部分之间的统计关系，这些[长程依赖](@article_id:361092)关系构成了基因组的“句法”。

这种[预训练](@article_id:638349)知识非常强大。假设我们想找到*[启动子](@article_id:316909)*——即作为基因“开启”开关的短序列。获得一个大型的、经实验验证的[启动子](@article_id:316909)数据集既缓慢又昂贵。但有了一个[预训练](@article_id:638349)的“DNA-BERT”，我们就拥有一个已经了解 DNA 总体情况的模型。我们只需要少量标记样本来为我们的特定任务微调这个模型。[预训练](@article_id:638349)提供了一套丰富的特征，这是一个巨大的领先优势，极大地减少了所需的数据量，并提高了最终模型的准确性。这就是[迁移学习](@article_id:357432)在生物学中的力量 [@problem_id:2429075]。

从编码它们的 DNA，我们可以转向蛋白质本身——这些执行细胞工作的分子机器。设计具有特定功能的新蛋白质是医学和工程领域的一项重大挑战。蛋白质是一串氨基酸序列，但其功能由它折叠成的复杂三维形状决定。这种折叠是一个全局过程：序列开头的氨基酸在最终结构中可能紧挨着序列末尾的氨基酸。

在这里，MLM 的双向性显示出其与物理现实的深刻契合。较早的*自回归*模型一次一个氨基酸地、从左到右地构建序列。这就像写一个句子却不能回去修改。很难强制执行一个长程约束，比如要求第 10 个和第 100 个氨基酸形成一个特定的键。模型在为第 10 个[残基](@article_id:348682)做选择时，并不知道第 100 个会是什么。

MLM 以一种迭代的“掩码-重填”方式运作，其行为非常不同。它可以一次性查看*整个*序列，掩盖一个区域，并在其他所有部分的上下文中重新生成它。这种迭代式精化更接近蛋白质折叠的物理过程，即整个链条通过全局协作稳定到一个低能状态。这使得 MLM 成为处理具有复杂、长程结构约束的设计问题的天然更优选择，例如指定一个完整的[残基](@article_id:348682)间接触图 [@problem_id:2767979]。它在每个精化步骤中整合全局指导的能力，是[理性蛋白质设计](@article_id:374358)的一次[范式](@article_id:329204)转变。

#### 符号的逻辑：代码与数学

计算机编程和数学这些结构化、逻辑化的世界也是 MLM 的沃土。一个计算机程序不是符号的随机集合；它受制于严格的语法、作用域规则（变量在哪里可见）以及类型系统（对给定变量哪些操作是有效的）。一个在像整个 GitHub 这样庞大的代码语料库上训练的 MLM，会隐式地学习这些规则。

当被要求预测一个被掩盖的变量名时，模型学到的不仅仅是猜测像 `i` 或 `x` 这样的常见名称。它可以从上下文中推断——比如，该变量正在与一个数字相加——被掩盖的变量必须是数值类型。它可以检查一个候选变量是否在当前作用域内实际声明并可见。通过玩这个简单的游戏，模型逆向工程了编程语言设计的原理 [@problem_id:3147308]。

这甚至可以扩展到抽象的数学语言。MLM 可以被训练来补全数学方程。通过在 `y = sin(x) + x ^ [MASK]` 中掩盖一个运算符或一个数字，模型可以从大量例子中学习到，这个空白很可能是一个数字，比如 `2`。更重要的是，它学习了代数的语法。它学习到像 `sin` 这样的函数通常后面跟着 `(`，像 `+` 这样的[二元运算](@article_id:312685)符两边需要有有效的项，以及一个方程通常应该只有一个 `=` 符号。这表明 MLM 不仅能捕捉到一种语言的统计模式，还能捕捉到其严格的、逻辑上的约束 [@problem_id:3147229]。

### 从[数据科学](@article_id:300658)到数字人文

“填空”[范式](@article_id:329204)是如此通用，以至于它超越了特定领域，为数据分析和历史等领域的旧问题提供了新的视角。

#### MLM 作为终极[数据插补](@article_id:336054)器

考虑一个数据科学中的常见问题：一张有缺失值的数据表。电子表格中的一行——比如说，`[年龄: 青年, 收入: ?, 结果: 购买]`——可以被看作一个短句。缺失值就是一个 `[MASK]` 词元。MLM 可以在数据集的完整行上进行训练，以学习列与列之间的关系。然后，它可以被用来根据一个人的年龄和购买历史，预测最可能的收入值。

但它所做的远不止于此。模型不仅给出一个最佳猜测，还提供了一个覆盖所有可能收入等级的*[概率分布](@article_id:306824)*。从这个分布中，我们可以计算出*熵*，这是衡量[模型不确定性](@article_id:329244)的一个指标。如果模型以 99% 的概率预测“高收入”，那么熵就很低；这是一个自信的预测。如果它以 51% 的概率预测“高收入”，以 49% 的概率预测“低收入”，那么熵就很高；模型在告诉我们它非常不确定。这种量化不确定性的能力是无价的。它不仅告诉科学家模型*认为*什么，还告诉他们*在多大程度上*可以信任这个预测，这一原则无论我们是在插补表格数据还是在文本中识别命名实体都同样适用 [@problem_id:3147317] [@problem_id:3147336]。

#### 重构过去

让我们用一个最具诗意的应用来结束我们的旅程。想象一位历史学家正在检查一块风化的罗马铭文，或是一位语言学家在研究一份被时间损坏的中世纪手稿。文本的大块内容可能已经缺失——这些是岁月流逝造成的天然 `[MASK]` 词元。在这里，MLM 可以充当一个数字合作者。在一个包含同一时期和文化的大量文本库中进行训练后，模型可以建议最合理的词或短语来填补这些空白（*lacunae*）。

当然，模型的建议不是最终定论。真正的力量在于将模型的统计[模式匹配](@article_id:298439)能力与人类专家的深厚知识相结合。历史学家可以指导模型，向其提供关于上下文的特定知识，例如单元语法先验（unigram priors，即在那个时代哪些词是常见的）和转移概率（哪些词倾向于跟随其他词）。其结果是一种强大的协同作用：一个能够提出人类可能未曾想到的重构方案的工具，同时又受到专家直觉的引导。这时的 MLM 不再是一个简单的机器，而是一个增强人类智慧的工具，帮助我们拼凑起我们自己过去的碎片化故事 [@problem_id:3147237]。从 HTML 文档到古代历史，模型学习结构（无论是层次结构还是顺序结构）的能力证明了其令人难以置信的多功能性 [@problem_id:3147243]。

[掩码语言建模](@article_id:641899)的历程是对一个美妙思想的证明。预测缺失部分的简单、自我监督的任务，促成了一种深刻的、上下文相关的理解的出现。我们现在看到，这种理解并不局限于任何一个领域。它是一把通用的钥匙，让我们能用一个单一、优雅的原则来探索自然、逻辑和历史的语法。