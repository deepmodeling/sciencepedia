## 引言
我们如何仅凭几片拼图就重构出一幅完整的画面？这个问题是现代科学与工程的核心，从医学成像到天文观测莫不如此。答案通常在于[稀疏性](@entry_id:136793)原则——即许多复杂信号本质上是简单的，仅由少数基本分量构成。然而，如果我们的构建模块过于相似，就会产生大量的模糊性，这种简单性便会轻易丧失。基于[相干性](@entry_id:268953)的恢复为应对这一挑战提供了一个强大的框架，它在基本信号的结构与我们从有限数据中唯一识别它们的能力之间建立了直接联系。

本文将对这一基本概念进行全面概述。我们将探讨如何量化一组信号的相似性或“[相干性](@entry_id:268953)”，以及该性质如何与稀疏性相结合，为保证[信号重构](@entry_id:261122)奠定基础。以下各节旨在循序渐进地建立这种理解。在**原理与机制**部分，我们将定义[互相关性](@entry_id:188177)，通过一个基础的“不确定性原理”探讨其与稀疏性的关系，并了解它如何保证实用恢复算法的成功。然后，在**应用与跨学科联系**部分，我们将见证这些原理的实际应用，揭示相干性如何指导从[单像素相机](@entry_id:754911)到[微分方程](@entry_id:264184)数值求解器等各种设计，从而展示其作为贯穿科学与计算的统一概念所扮演的角色。

## 原理与机制

要真正领会基于[相干性](@entry_id:268953)的恢复的力量，我们必须踏上一段旅程，但起点不是复杂的方程，而是一个简单而根本的问题：我们如何从有限的观察中明确地理解世界？想象一下，你是一位[音频工程](@entry_id:260890)师，试图从鸡尾酒会的录音中分离出单个声音；或者你是一位医生，试图从复杂的血液样本中识别出几种关键的疾病[生物标志物](@entry_id:263912)。在这两种情况下，挑战都是将一个复杂的[信号分解](@entry_id:145846)为其简单而有意义的分量。这便是[稀疏恢复](@entry_id:199430)的艺术与科学。

### 模糊性的困境

让我们更正式地描述我们的问题。我们有一个由可能的原因或基本信号组成的“字典”，可以表示为一个矩阵的列，称之为 $A$。每一列，或称为**原子**，都是我们世界的一个基[本构建模](@entry_id:183370)块——一个单独的音符、一种特定的视觉模式，或某种蛋白质的特征。我们的观测结果，一个向量 $y$，是这些原子的混合。我们的目标是找到配方，即一个系数向量 $x$，它告诉我们哪些原子被以何种量级使用，使得 $y = Ax$。

如果我们的字典原子都完全不同——比如说，彼此完全不相关（数学上称为**正交**）——这个任务将变得微不足道。我们只需测量观测值 $y$ 与每个原子 $a_i$ 的相似度（通过计算它们的[内积](@entry_id:158127)）即可找到相应的系数 $x_i$。

但如果这些原子并非如此独特呢？如果其中一些非常相似呢？考虑一个字典，其中有两个原子 $a_1$ 和 $a_2$ 是相同的 [@problem_id:2906067]。现在，假设我们的观测值是 $y = a_1 + a_2$。由于 $a_1 = a_2$，我们也可以写成 $y = 2a_1$ 或 $y = 2a_2$。我们甚至可以写成 $y = 3a_1 - a_2$。“真实”的配方迷失在模糊的海洋中。我们无法区分 $a_1$ 和 $a_2$ 的贡献。这个本质问题就是我们所说的**相干性**。

### 量化相似性：[互相关性](@entry_id:188177)

为了取得进展，我们需要一种方法来量化我们字典中的这种“相似性”或“模糊性”。衡量两个向量相似度的一个自然方法是它们[内积](@entry_id:158127)的[绝对值](@entry_id:147688)（或者对于单位长度的向量，是它们之间夹角的余弦值）。我们可以定义一个单一的数字来表征整个字典：**[互相关性](@entry_id:188177)**，记为 $\mu(A)$。它就是我们字典中任意两个*不同*原子之间的最大相似度 [@problem_id:3464843]。

$$
\mu(A) = \max_{i \neq j} \frac{|\langle a_i, a_j \rangle|}{\|a_i\|_2 \|a_j\|_2}
$$

[互相关性](@entry_id:188177) $\mu(A)$ 是一个介于 $0$ 和 $1$ 之间的数字。如果 $\mu(A) = 0$，所有原子都是正交的，不存在模糊性。如果 $\mu(A) = 1$，则至少有两个原子是线性相关的（就像我们那对相同的原子一样），代表了最大的模糊性。对于一个典型的字典，[相干性](@entry_id:268953)会介于两者之间。例如，在一个简单的假设字典中，我们可能会发现 $\mu(A) = \frac{1}{4}$ [@problem_id:3435269]。这个数字为我们提供了一个具体把握字典结构的工具：它告诉我们，集合中的任意两个基本信号的重叠度都不超过 25%。

### [稀疏性](@entry_id:136793)：组织原则

相干性的概念似乎描绘了一幅黯淡的图景。如果我们的字典具有任何非零的[相干性](@entry_id:268953)，我们还能期望找到一个唯一且有意义的解吗？一个优美简单而又强大的思想拯救了这一局面：**稀疏性**原则。在许多现实世界的场景中，我们观察到的复杂信号实际上仅由少数几个基本分量构成。音频信号主要由少数几个声音主导，图像由少数几种纹理构成，疾病由少数几个[生物标志物](@entry_id:263912)指示。这个配方向量 $x$ 是**稀疏的**，意味着它的大部分元素都是零。

[稀疏性](@entry_id:136793)的假设从根本上改变了游戏规则。它让我们能够引用一个深刻的结果，一种针对稀疏信号的“不确定性原理”[@problem_id:3491559]。该原理指出，一个信号在同一个字典中不可能有两种不同的、高度稀疏的表示。更精确地说，如果一个字典 $A$ 的[互相关性](@entry_id:188177)为 $\mu(A)$，那么任何可以通过字典原子组合形成的非零信号 $h$（即 $h=Az$ 对某个 $z$ 成立）其非零系数必须有一个最小数量：

$$
\|z\|_0 \ge 1 + \frac{1}{\mu(A)}
$$

这里，$\|z\|_0$ 是“$\ell_0$范数”，它只是 $z$ 中非零元素的计数。这告诉我们，任何模糊性——即同一观测值 $y$ 的两个不同配方 $x_1$ 和 $x_2$——都必须涉及一个“稠密”（非稀疏）的差分向量 $z = x_1 - x_2$。因此，如果我们寻找的是一个*非常稀疏*的配方，它就保证是唯一的！

### 从理论到实践：如何找到稀疏的真相

知道存在一个唯一的[稀疏解](@entry_id:187463)是一个重大突破。但我们如何找到它呢？尝试所有可能的稀疏原子组合将比宇宙的年龄还要长。我们需要一个高效的算法。

这就是[凸优化](@entry_id:137441)的魔力所在。我们不去尝试解决那个计算上不可能的问题——找到具有最少非零元素的解（最小化$\ell_0$范数），而是解决一个相关且容易得多的问题：找到其系数[绝对值](@entry_id:147688)之和最小的解（最小化**$\ell_1$范数**）。这个方法就是著名的**[基追踪](@entry_id:200728)（BP）**。

这似乎好得令人难以置信，但它确实有效。其有效的原因与[相干性](@entry_id:268953)密切相关。正是保证[稀疏解唯一性](@entry_id:755128)的低相干性，也确保了易于处理的$\ell_1$最小化问题能够找到它。这引出了该领域的核心成果之一：如果信号的稀疏度 $s$ 满足条件

$$s  \frac{1}{2} \left( 1 + \frac{1}{\mu(A)} \right)$$

那么[基追踪](@entry_id:200728)保证能够精确恢复稀疏配方 [@problem_id:3435269, @problem_id:3440270]。让我们回到那个 $\mu(A) = \frac{1}{4}$ 的字典。公式给出 $s  \frac{1}{2}(1+4) = 2.5$。这是一个具体的、可预测的保证：这个字典可以用来完美恢复*任何*由其一或两个原子组合而成的信号。其他贪婪方法，如**[正交匹配追踪](@entry_id:202036)（OMP）**，通过迭代选择与剩余信号最相关的原子，在相同条件下也享有类似的保证 [@problem_id:3464843]。

### 超越最坏情况：更清晰的[相干性](@entry_id:268953)视角

[互相关性](@entry_id:188177)是一个强大的概念，但它也有点悲观。它仅根据字典中表现最差的一对原子来评判整个字典。如果这对原子只是个例，而字典的其余部分表现良好呢？

在这种情况下，由 $\mu(A)$ 提供的保证可能过于保守。例如，可以构建一个字典，其中基于相干性的界限无法保证恢复一个 2-[稀疏信号](@entry_id:755125)，但更详细的、针对特定支撑集的分析表明，恢复实际上是完全可能甚至微不足道的 [@problem_id:3444677]。这是因为通用界限必须对*任何*给定稀疏度的稀疏信号都有效，但对于一个*特定*的[稀疏信号](@entry_id:755125)，其所涉及的原子可能与其余原子分离得很好。

这一观察促使我们寻求更精细的度量。其中一个工具是**累积[相干性](@entry_id:268953)** $\mu_1(s)$，也称为 Babel 函数 [@problem_id:3435272]。它不再关注单个最差的成[对相关](@entry_id:203353)性，而是衡量任意单个原子与*一组* $s$ 个其他原子的最大*总*相关性。这提供了一个更全局的视角。对于那些“高相干性”原子对稀少且分散的字典，这种度量可以提供显著更优的保证。在一个巧妙构建的例子中，标准的[互相关性](@entry_id:188177)可能只保证恢复 1-[稀疏信号](@entry_id:755125)，而累积[相干性](@entry_id:268953)则正确地证明了高达 4-稀疏的信号也可以被完美恢复。这表明，通过更全面地审视字典的结构，我们可以对其能力获得更准确的描绘。

### 宏观视角：语境中的相干性

基于[相干性](@entry_id:268953)的分析，无论其形式如何，都是一个极具价值的工具，因为它直观，而且最重要的是，它是可计算的。你可以拿来你的字典，计算其相干性，然后得到一个具体的性能保证。然而，它并非分析[稀疏恢复](@entry_id:199430)的唯一方法，也并非总是最强的。

一个更强大但更抽象的概念是**受限等距性质（RIP）** [@problem_id:2906043, @problem_id:3474596]。RIP 不再关注单个原子之间的相关性，而是提出了一个更深刻的几何问题：字典矩阵 $A$ 在多大程度上保持了所有稀疏向量的长度？一个具有 RIP 性质的矩阵在作用于稀疏向量时，其行为类似于一个近[正交系统](@entry_id:184795)，这是恢复的理想属性。

基于 RIP 的保证通常比基于[相干性](@entry_id:268953)的保证强得多，特别是对于作为现代[压缩感知](@entry_id:197903)主力的大型[随机矩阵](@entry_id:269622)。对于这些矩阵，RIP 保证了恢复所需的测量次数与环境维度成对数关系，而[相干性](@entry_id:268953)保证则需要要求高得多的多项式级关系。但问题在于，验证一个给定矩阵是否满足 RIP 是计算上的 NP-hard 问题。这就产生了一个美妙的权衡：相干性为你提供一个实用、可计算但有时保守的保证，而 RIP 则提供一个理论上接近最优但在实践中难以验证的保证。

当我们考虑到不可避免地存在噪声的现实[世界时](@entry_id:275204)，这个相互作用的原理体系变得更加丰富。处理[相关噪声](@entry_id:137358)的标准技术是“[预白化](@entry_id:185911)”数据，这在数学上是对系统进行重新缩放，以使噪声均匀化。但这种变换也改变了字典，正如一个优雅的例子所示，白化噪声可能会无意中*增加*有效字典的相干性 [@problem_id:3462352]。在试图简化噪声的同时，我们可能复杂化了信号结构，从而可能削弱了我们的[恢复保证](@entry_id:754159)。这揭示了一个深刻的工程和科学教训：没有免费的午餐，只有权衡。[相干性](@entry_id:268953)为我们提供了理解和驾驭这些基本折衷的语言。

