## 引言
在构建预测模型的过程中，最关键的步骤之一是如实评估其性能。我们需要知道一个模型在未见过的新数据上表现如何，而不仅仅是在训练它所用的数据上。[交叉验证](@entry_id:164650)是完成此任务的标准工具，但这项强大的技术有一个虽细微却很重大的弱点：当处理[不平衡数据集](@entry_id:637844)（即某一类数据远比另一类稀少）时，它可能会失效。纯粹的随机数据划分可能导致测试集完全缺失稀有类别，使得评估无法进行，并导致结果不稳定且不可信。

本文通过深入探讨分层 K 折[交叉验证](@entry_id:164650)来应对这一根本性挑战，这是一种保证评估具有代表性和公平性的优雅解决方案。我们将探讨该方法如何为机器学习中的[科学诚信](@entry_id:200601)提供坚实的基础。第一章“原理与机制”将解析分层背后的统计机制，解释它如何抑制方差，以及为什么它优于朴素的随机划分。随后，“应用与跨学科联系”将展示这一原则如何应用于医学和化学等高风险领域，从一个简单的修正方法演变为构建可信模型的综合理念。

## 原理与机制

想象你是一位正在完善一道革命性新菜品的大厨。要知道你的创作是否真是杰作，你不能只自己品尝——因为你带有偏见。你需要看看别人的反应。于是，你准备了一大批菜，并邀请一群评论家来品尝。一个简单的方法是随机地将食物和评论家分成（比如说）十桌。你为九桌上菜，观察第十桌。然后你换另外九桌上菜，观察另一桌，如此循环，直到每一桌都有机会成为“测试”组。这种在部分数据上迭代训练并在留出数据上测试的过程，正是一种名为**[交叉验证](@entry_id:164650)**的技术的核心。

但如果你的菜品中有一种稀有、奇特的成分，而它恰恰定义了这道菜的特色呢？又如果，纯粹因为运气不好，你的其中一个测试桌得到的菜品中完全没有这种关键成分呢？他们的反馈将毫无用处，完全无法告诉你这种成分的影响。这正是分层法优雅解决的核心挑战。在数据科学和医学的世界里，我们的“稀有成分”往往正是我们最关心的东西：患有罕见病的患者、生产线上的次品，或是特定思维的微弱神经信号。

### 朴素划分的风险

当我们评估一个预测模型时，我们的目标是获得一个关于它在*新的、未见过的数据*上表现如何的真实评估。交叉验证是我们模拟这一过程的工具。我们将数据集划分为 $k$ 个组，或称**折**。然后，我们进行 $k$ 次实验。在每次实验中，我们使用一折作为[留出测试集](@entry_id:172777)，其余 $k-1$ 折作为训练集。我们在训练数据上训练模型，并在测试数据上衡量其性能。通过对所有 $k$ 次实验的性能取平均，我们希望能得到一个关于模型真实能力的稳定且无偏的估计。

但是，朴素的、纯粹随机的划分可能是一个统计雷区，尤其是在处理**[不平衡数据集](@entry_id:637844)**时——即一个类别比另一个类别稀少得多。考虑一个用于诊断罕见癌症的医疗数据集，该癌症只影响 $1\%$ 的人口。我们有 20,000 条患者记录，其中 200 个“阳性”案例（癌症）和 19,800 个“阴性”案例（健康）。如果我们使用标准的 10 折交叉验证，我们会随机打乱所有 20,000 条记录，并将它们分成 10 个各有 2,000 条记录的折。[@problem_id:1912436]

这会有什么问题？和我们大厨品尝会上的问题一样。由于随机性，我们的某个测试折中可能最终*一个*阳性案例都没有。某个特定的阳性案例*不*落入给定折的概率是 $\frac{k-1}{k}$。如果我们总共有 $n_+$ 个阳性案例，并且它们的分配是独立的，那么一个折中*完全没有阳性案例*的概率是 $\left(1 - \frac{1}{k}\right)^{n_+}$。[@problem_id:4389534] 如果 $n_+$ 很小，这个概率会高得吓人。

当一个测试折没有阳性样本时，我们就不可能评估[模型识别](@entry_id:139651)阳性类别的能力。像**灵敏度**（[真阳性率](@entry_id:637442)）或 **F1 分数**这类衡量模型发现稀有案例能力的关建指标，会变得无定义或毫无意义。它们的分母为零。其结果是，该折的评估遭遇灾难性失败。即使一个折中只有少数几个阳性案例，比如一两个，性能评估也会极其不稳定——单次错误分类就可能使测得的灵敏度从 $1.0$ 摆动到 $0.0$。这种从一折到另一折的性能指标的剧烈波动，即高**方差**，使得最终的平均结果不可靠。我们试图衡量我们模型的能力，但我们的测量工具却到处摇摆不定。

### 分层的精妙思想

解决这种混乱的方案既优雅又简单：**分层 K 折交叉验证**。我们不再将所有数据混在一个大锅里一起搅乱，而是分别处理每个类别。想象我们有两副牌，一副小的包含 200 个“阳性”案例，一副大的包含 19,800 个“阴性”案例。为了创建我们的 10 个折，我们首先分发阳性案例的牌，在每个折中放入 20 个。然后，我们分发阴性案例的牌，在每个折中放入 1,980 个。瞧！我们的 10 个折现在每一个都是原始数据集的完美缩影，阳性案例的比例恰好为 $1\%$。

这就是分层的精髓。我们划分数据，使得每个折中的类别比例尽可能接近整个数据集中的总体比例。[@problem_id:4174402] [@problem_id:4389534]

当然，数字并不总是能如此整齐地划分。如果我们有 $P=36$ 个阳性案例，并希望创建 $k=5$ 个折，该怎么办？我们不能在每个折中放入 $7.2$ 个案例。处理这个问题的算法非常合乎逻辑。[@problem_id:4152130] 首先，我们计算每个折的基础案例数，即 $\lfloor P/k \rfloor = \lfloor 36/5 \rfloor = 7$。我们有 $36 - (7 \times 5) = 1$ 个剩余案例。所以，我们只需将这一个额外的案例添加到一个折中。结果就是一个折有 8 个阳性案例，四个折有 7 个。任何一个折中最少的阳性案例数是 7。

这个简单的过程给了我们一个强大的设计原则。如果我们想*保证*每个验证折至少有 $m$ 个阳性样本，我们必须选择一个折数 $K$，使得阳性总数 $P$ 满足条件 $P \ge mK$。[@problem_id:3804487] 这确保了即使是“最不幸运”的、分到最少阳性案例数的折（$\lfloor P/K \rfloor$），也仍然能满足我们的要求。

### 驯服统计猛兽：偏差与方差

为什么从根本的统计学角度来看，分层要好得多？让我们谈谈良好测量的两个敌人：**偏差**和**方差**。偏差是系统性误差，就像一个总是读数高出 1 公斤的秤。方差是[随机误差](@entry_id:144890)，就像一个读数不可预测地[抖动](@entry_id:262829)的秤。

一个令人惊讶的事实是，分层实际上并不会改变我们交叉验证评估的偏差。[@problem_id:5187348] 在标准的随机划分中，每个折的阳性案例的*期望*或*平均*数量已经是正确的了：阳性类别为 $N_1/k$，阴性类别为 $N_0/k$。[@problem_id:4535090] 问题不在于平均值，而在于围绕它的大幅波动。分层不会改变这个长期平均值；它只是抑制了可变性。

分层的真正胜利在于它能显著降低**方差**。分类器在给定折中的准确率取决于其类别条件准确率（$a_+$ 对阳性， $a_-$ 对阴性）以及该折中阳性案例的比例 $q$。该折的准确率是一个简单的线性函数：$A(q) = a_+ q + a_- (1-q)$。[@problem_id:4152136] 在标准 K 折交叉验证中，$q$ 是一个在不同折之间跳跃的随机变量，导致 $A(q)$ 也随之跳跃。这就是高方差的来源。分层的作用是通过将 $q$ 固定为在每个折中都与总体流行率 $p$ 几乎相同。通过将随机变量 $q$ 变成一个常数 $p$，我们完全消除了这个主要的方差来源。我们摇摆不定的测量工具变得坚如磐石。

这里还有一个更深层次的影响。许多分类器会隐式或显式地使用训练数据的类别比例来确定其决策阈值。[@problem_id:3134712] 当模型在一个非分层的折上训练时，它可能会看到一个极不具代表性的类别比例。它会尽职地从这个扭曲的视角学习，从而采纳一个次优的决策规则。当我们接着测试这个被误导的模型时，它的错误率会比它从一个具代表性的训练集学习时更高。因为训练集的类别比例是随机波动的，所以产生的误差也会波动。根据一个称为[詹森不等式](@entry_id:144269)的数学性质，这些被夸大的误差的*平均值*保证会高于一个在正确比例上持续训练的模型的误差。分层通过确保每个训练集都是真实世界的高保真反映来防止这种“误差膨胀”，从而得到一个更准确、更可靠的模型真实性能评估。

### 黄金法则：对未来保密

交叉验证是模拟未来的一种尝试。每一步中的测试折代表了模型从未见过的未来数据。为了得到一个诚实的评估，我们必须遵守一个神圣的原则，一条“黄金法则”：**测试数据绝不能以任何方式影响训练过程。** [@problem_id:4174402]

这看起来显而易见，但这条规则却出奇地容易被违反。想象你有一套构建模型的流水线操作。一个常见的首要步骤是**预处理**数据，例如，通过缩放每个特征使其均值为零、标准差为一（这个过程称为 z-score 标准化）。这需要计算数据的均值和标准差。一个诱人的错误是在开始[交叉验证](@entry_id:164650)*之前*，在*整个数据集*上计算这些统计数据。

这是一种“信息泄露”。通过使用所有数据来计算缩放参数，你已经让来自测试集的信息（它的均值和标准差）泄露到了训练过程中。你的模型通过窥探测试数据的属性来“作弊”。同样的规则适用于任何数据驱动的步骤：估计类别先验概率，在[线性判别分析](@entry_id:178689)中计算协方差矩阵，或者选择使用哪些特征。所有这些步骤都是“训练”的一部分，并且必须在[交叉验证](@entry_id:164650)循环*内部*执行，只使用该特定折的训练数据。然后，学习到的参数（如缩放因子或协方差矩阵）被应用于留出的测试折。这种纪律确保我们的性能评估不是幻觉，而是[模型泛化](@entry_id:174365)能力的真实反映。[@problem_id:4174402]

### 超越基础：优化评估

分层原则是可靠模型评估的基石，它连接着一个更广泛的验证技术家族。

单次运行分层 10 折[交叉验证](@entry_id:164650)可能仍然会因为数据的划分方式而受到一些随机运气的影响。为了得到更稳定的评估，我们可以执行**重复分层 K 折[交叉验证](@entry_id:164650)**。我们只需将整个分层 K 折过程运行（比如说）10 次，每次都用不同的初始随机打乱，然后对所有 $10 \times 10 = 100$ 个测试折的结果进行平均。这种重复不会降低评估的偏差，但它能平滑由特定划分引起的方差，给我们一个更精确的最终数值。[@problem_id:4802804] [@problem_id:5187348]

这自然引出了选择何种 $k$ 值的问题。这涉及一个经典的**[偏差-方差权衡](@entry_id:138822)**。[@problem_id:5187348] 一个大的 $k$（比如 $k=n$，称为**[留一法交叉验证](@entry_id:637718)**）意味着我们的训练集非常大，所以我们训练出的模型与在全部数据上训练的模型非常相似。这导致了一个低偏差的评估。然而，由于训练集之间非常相似，它们的性能评估高度相关，这会导致最终平均值的方差很高。相反，一个小的 $k$（例如，$k=3$）产生的评估相关性较低（方差较低），但代价是偏差较高，因为模型是在显著较小的数据集上训练的。因此，$k=5$ 或 $k=10$ 作为一种常见的、有经验支持的折中点而出现，它平衡了这些相互竞争的力量，为我们模型的真实能力提供了一个可信且稳定的评估。

