## 引言
机器学习正迅速从一个专门的计算机科学分支，演变为科学探究的基础工具，如同一种新型透镜，能让我们察觉到人眼无法看到的模式。然而，在自然世界复杂混乱的现实与[算法](@article_id:331821)严谨的数字语言之间架起桥梁，是一项重大挑战。简单地将原始数据喂给模型是远远不够的，这可能导致[伪相关](@article_id:305673)和物理上不可能的预测。真正的艺术在于深思熟虑地准备数据，并将科学知识[嵌入学习](@article_id:641946)过程。本文旨在探讨如何掌握这种转换。在接下来的章节中，我们将首先剖析[科学机器学习](@article_id:305979)的核心原理和机制——从将生物数据转化为[特征向量](@article_id:312227)，到理解模型的内在局限性。随后，我们将巡览激动人心的应用和跨学科联系，见证这些方法如何在解读生命之书，并利用智能的[物理信息](@article_id:312969)系统构建未来。

## 原理与机制

我们有了这个绝妙的想法：教机器去思考、去预测、去发现。但我们该如何开始呢？你不能只是给电脑看一张花的图片，然后说：“学习这个。”世界，以其全部壮丽而混乱的复杂性，必须首先被翻译成计算机唯一能真正理解的语言：冰冷而严谨的数字逻辑。这种翻译不仅仅是一项技术性的杂务，它是一种艺术形式，也是真正思考的起点。

### 翻译自然语言

在解决一个问题之前，你必须首先知道如何提出问题。在机器学习中，这通常归结为一个简单的选择：你问的是“哪一个？”的问题，还是“多少？”的问题？如果你想教机器区分猫和狗，你就在**分类** (classification) 的领域。答案会落入一个清晰、离散的类别。但如果你想预测一种酶的效率——它的[转换数](@article_id:373865) $k_{cat}$ 呢？这不是一个简单的“是”或“否”。答案是一个数字，一个尺度上的连续值。这是一个“多少？”的问题，它的名字是**回归** (regression) [@problem_id:1426760]。你向机器提供酶及其底物的特征——它们的分子量、化学性质——然后要求它学习一个函数，将这些特征映射到一个单一的连续输出：$k_{cat}$ 值。

一旦我们确定了问题框架，我们面临下一个挑战：如何将输入本身——比如分子、基因，甚至是实验用的细胞系——转换成机器可以处理的一串数字，即**[特征向量](@article_id:312227)** (feature vector)？

让我们从简单的开始。想象一下，你正在处理不同的癌细胞系，比如‘HeLa’、‘MCF7’和‘A549’ [@problem_id:1426091]。一种天真的方法可能是给它们分配数字：HeLa=1, MCF7=2, A549=3。但这是一个严重的错误！这样做，你无意中告诉了你的[算法](@article_id:331821)，`MCF7`在某种程度上“大于”`HeLa`，并且`A549`和`HeLa`之间的“距离”是`MCF7`和`HeLa`之间距离的两倍。这些是毫无意义的人为关系，会迷惑你的模型。

优雅的解决方案是一种叫做**[独热编码](@article_id:349211)** (one-hot encoding) 的方法。它非常简单。如果你有三个类别，你为每个类别创建一个三维向量。对于‘A549’，你可能使用 `(1, 0, 0)`。对于‘HeLa’，使用 `(0, 1, 0)`。对于‘MCF7’，使用 `(0, 0, 1)`。现在，每个类别都由其自己专属的维度来表示。它们彼此之间的差异是均等的，正如它们本应如此。没有虚假的排序，也没有人为的距离。

当处理更复杂的数据，比如DNA序列时，这个想法变得更加强大。基因是一串字母：A、C、G、T。这些不是数字，同样，$A \lt C \lt G \lt T$ 这种说法也毫无意义。为了预测诸如[CRISPR基因编辑](@article_id:309223)工具的[脱靶效应](@article_id:382292)之类的事情，模型不仅需要知道存在哪些[核苷酸](@article_id:339332)，还需要知道它们的确切位置 [@problem_id:2060864]。[独热编码](@article_id:349211)完美地处理了这一点。DNA序列中的每个位置都拥有自己的一组四个维度。第一个位置的‘A’变成 `[1,0,0,0]`，第二个位置的‘T’变成 `[0,0,0,1]`，以此类推。然后你将这些向量连接起来。一个5个[核苷酸](@article_id:339332)的序列就变成了一个20维的向量，清晰而明确地表示了碱基的分类性质及其关键位置，而没有捏造虚假的数学关系。我们成功地将生命语言翻译成了线性代数的语言。

### 创造公平的竞争环境

现在我们的数据已经是数值形式了。我们可能有一个用于发现新材料的数据集，其中一个特征是熔点，范围从300到4000开尔文，而另一个特征是[电负性](@article_id:308047)，其变化范围仅在0.7到4.0之间 [@problem_id:1312260]。如果我们把这些数字直接输入某些[算法](@article_id:331821)，就会遇到麻烦。

许多[算法](@article_id:331821)，特别是那些依赖于测量数据点之间“距离”的[算法](@article_id:331821)（如[k-近邻算法](@article_id:641047)），就像一个只能用一种单位测量长度的人。如果你告诉他们一个维度是3000（开尔文），另一个是2（泡林单位），第一个数字是如此巨大，以至于第二个数字的任何变化都变得完全看不见。熔点特征将完全主导计算，而[电负性](@article_id:308047)中微妙但至关重要的信息将淹没在噪音中。

为了解决这个问题，我们必须对**特征进行缩放** (scale our features)。可以把它想象成把你所有的变量放在一个平等的基础上，确保每一个都有发言权。两种常见的方法是**标准缩放** (standard scaling)，它将每个特征转换为均值为0、[标准差](@article_id:314030)为1；以及**最小-最大缩放** (min-max scaling)，它将每个特征压缩或拉伸以恰好适应像0到1这样的范围。这不仅仅是一个微小的调整；它可以从根本上改变你数据的“形状”。一项关于这些缩放方法如何影响基因表达谱之间距离的定量分析表明，缩放器的选择可以将两个样本之间的感知距离改变超过两倍 [@problem_id:1425849]。通过缩放，我们确保问题空间的几何形状反映的是我们关心的生物学或化学，而不是我们为测量选择的任意单位。

### 声音太多的诅咒

在现代生物学中，我们正被数据淹没。为仅仅100名患者测量20,000个基因的表达水平已非罕见之事 [@problem_id:1440789]。这给我们带来了一个奇特而微妙的问题，即**[维度灾难](@article_id:304350)** (curse of dimensionality)。

想象一下，你试图在一个一维世界——一条长长的走廊里找到一个朋友。很容易。现在想象在一个二维的足球场上找他们。有点难。在一座三维的摩天大楼里呢？更难了。随着维度的增加，空间的体积会爆炸式增长。在一个20,000维的空间中，每个数据点都与其他任何数据点相距甚远。这个空间大部分是空的。

危险何在？有如此多的特征——如此多可能的解释——模型变得极其容易找到一个“模式”来完美解释它所见过的这100名患者。它可以抓住**[伪相关](@article_id:305673)** (spurious correlations)，即数据中与实际疾病无关的随机波动。模型变成了记忆大师，而不是学习者。它在模拟测试（训练数据）中表现优异，但在任何新患者身上都会惨败，因为它学到的是噪音，而不是信号。这被称为**过拟合** (overfitting)，是机器学习的原罪之一。

解决方案是让一些声音安静下来。**降维** (dimensionality reduction) 技术旨在找到那些真正重要的、少数几个宽泛的变化模式，并丢弃成千上万维度的噪音。这就像面对20,000件乐器奏出的嘈杂声，然后意识到它们其实都在演奏仅仅几个基本旋律的变奏。通过专注于那些核心旋律，我们让模型有机会学到一些真实且可泛化的东西。

### 从失败中学习

好了，我们有了干净、行为良好的数据。模型实际上是如何学会画一条线，来区分比如说“功能性”遗传电路和“非功能性”遗传电路呢？在这里，我们偶然发现了一个出乎意料的深刻哲学观点：要理解某物*是*什么，你必须也理解它*不是*什么。

想象一下，你正在训练一个模型来设计功能性遗传电路，而你只给它看完美工作的电路的例子 [@problem_id:2018104]。模型会学到什么？它会学到*一切*都是功能性遗传电路！当面对一个新设计时，它的预测将是无可救药的乐观，因为它从未被给予过理由去作他想。它没有失败的概念。

要学习，模型必须同时看到成功和失败。**负样本** (negative examples)——那些被正确构建但未能工作的电路——与正样本同样重要。它们提供了对比，即硬币的另一面。它们是让模型能够定义一个**决策边界** (decision boundary) 的东西，这条线（或在多维空间中的复杂[曲面](@article_id:331153)）将设计空间中的“功能性”区域与“非功能性”区域分开。学习不是记忆一长串好东西，而是理解定义它们的边界。

### 知道你所不知道的

一个训练好的机器学习模型感觉就像一个现代神谕。但它的智慧是有限的，而成为一名优秀的科学家意味着要理解这些局限。模型只知道它被展示过的东西。

考虑一个被训练来预测新药效力的模型 [@problem_id:2423881]。如果它只在具有特定化学结构（一个“骨架”）的分子家族上进行训练，它就会成为该家族的专家。如果你随后要求它预测一个具有完全不同骨架的分子的活性，你就是在要求它远远超出其经验进行**外推** (extrapolate)。模型内部的规则，那些将化学特征与活性联系起来的规则，是为第一个家族的结合机制学习的。新分子可能以完全不同的方式与靶蛋白结合，使得旧规则变得无关紧要。一个模型的预测可以被信任的化学空间区域被称为其**适用域** (applicability domain)。走出这个领域无异于自找灾难。

这种对训练数据的依赖导致了一个更为隐蔽的问题。如果我们最初的数据有缺陷怎么办？想象一个过程：一位专家首先标记一些细胞图像，然后一个模型基于这些数据进行训练。接着，那个初级模型被用来标记一个更大的数据集，这个数据集又被用来训练一个更强大的第二代模型，如此循环往复 [@problem_id:1422055]。如果最初的人类专家有一个微小但系统性的偏差——也许他们总是低估细胞的大小——这个迭代过程就会变成一个危险的回音室。第二个模型学习了第一个模型的偏差。第三个模型以更大的信心学习了这个偏差。最初微不足道的错误不仅会传播，它还可能被**放大** (amplified)，每一代模型都对自己所处的有缺陷的现实越来越确信。如果这样一个系统中的[放大因子](@article_id:304744) $\alpha$ 大于1，偏差 $\beta_N$ 可能会指数级增长，$\beta_N \approx \alpha^N \beta_0$，导致系统强大而自信地走[向错](@article_id:321627)误。这就是“垃圾进，垃圾出”原则的指数级升级版。

这把我们带到了该领域最深刻的问题之一：模型本身的性质是什么？在一个极端，我们有**[黑箱模型](@article_id:641571)** (black-box models)，比如许多[深度神经网络](@article_id:640465)。它们是极其强大、灵活的学习者，可以在海量数据集中找到模式。但它们的推理过程往往是不透明的。在另一个极端，我们有**机理模型** (mechanistic models)，它们建立在科学定律之上 [@problem_id:2727915]。一个用于[CRISPR](@article_id:304245)效率的机理模型不仅仅是从序列数据中学习相关性；它会尝试模拟过程的实际生物物理学，其中包含直接来自[热力学](@article_id:359663)的结合能（$\Delta G$）和温度（$T$）等项。

[黑箱模型](@article_id:641571)在与[训练集](@article_id:640691)非常相似的数据上可能更准确——它是[插值](@article_id:339740)的大师。但机理模型，因为它对潜在的因果关系有初步的“理解”，当条件改变时，更有可能正确地泛化。如果你在一个温度下训练模型，然后在另一个温度下部署它，[黑箱模型](@article_id:641571)会迷失方向，而机理模型则确切地知道温度如何影响[反应速率](@article_id:303093)。这种内置的科学知识是一种**[归纳偏置](@article_id:297870)** (inductive bias)。

未来，或许不在于二选一，而在于将它们融合。我们可以利用机器学习的力量，但用我们已知为真的物理定律来约束它 [@problem_id:2727915]。我们可以构建不仅是强大的[模式匹配](@article_id:298439)器，而且还尊重宇宙基本法则的模型。这样做，我们不仅在构建更好的工具，我们还在教我们的创造物以一种更像科学家的方式看待世界——既关注经验模式，也关注支配它们的、优美而统一的法则。