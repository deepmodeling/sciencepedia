## 应用与跨学科联系

我们已经探究了算法——这些逻辑的典范——如何继承甚至放大人类偏见的原理。现在，让我们离开理论的洁净室，进入一个混乱、充满活力且常常出人意料的世界，在那里，这些算法被付诸实践。因为正是在这里，在应用中，偏见的抽象概念才具有了真正的分量和后果。我们将看到，这并非一个局限于计算机科学的利基问题；它是一个庞大的、跨学科的挑战，触及医学、法律、伦理以及我们作为一个社会做出决策的根本方式。就像物理学家在苹果的下落和行星的轨道中看到相同的运动定律一样，我们将在截然不同的领域发现同样的基本偏见模式。

### 诊断的凝视：当AI无法看清时

或许，[算法公平性](@entry_id:143652)的风险在医学领域无处其高。我们正开始将一项神圣的任务托付给AI：帮助我们看清人体内部，检测疾病，并评估痛苦。但当AI的凝视被其所受教育中的偏见所蒙蔽时，会发生什么？

想象一下皮肤科医生的办公室。一个AI被训练来从临床照片中识别像二期梅毒这样的疾病的细微皮疹。它在肤色较浅的患者身上表现出色。但对于肤色较深的患者，其准确率急剧下降。它会漏诊，导致治疗延迟。为什么？问题不在于代码，而在于AI的“生活经验”——它的训练数据。它被喂食的图像绝大多数是肤色较浅的患者。此外，它看到的少数肤色较深的图像通常是在较差的光线下拍摄的，使得皮疹的特征性红色，即红斑，不那么明显。这是一个典型的**测量偏见**案例：不仅在数据中*谁*存在系统性差异，而且在于不同群体的数据*如何被采集和标记*。算法并非恶意；它只是学到了错误的教训。它可能部分学会了将疾病不仅与皮疹本身联系起来，还与皮疹在良好光线下在浅色皮肤上出现的特定方式联系起来[@problem_id:4440162]。

让我们从皮肤深入到身体内部。考虑一个为将[计算机断层扫描](@entry_id:747638)（CT）中的肿瘤分类为良性或恶性而构建的AI。一家医院使用来自两家扫描仪供应商（供应商A和供应商B）的数据来训练一个模型。由于该医院拥有更多来自供应商A的机器，训练数据是不平衡的——比如说，900张来自供应商A的扫描，只有100张来自供应商B。这是一种经典的**数据偏见**形式。一个标准的算法，在其不懈追求最小化平均错误的过程中，可能会学到一个聪明而危险的捷径。它可能在来自供应商A的扫描上达到近乎完美的表现，而在来自供应商B的扫描上几乎完全失败。它为什么会这样做？因为牺牲来自供应商B的小群体以换取在来自供应商A的大群体上的微小增益，仍然能带来更低的*总体*错误率。算法找到了在其期末考试中获得好成绩的“阻力最小的路径”，但当部署到一家使用供应商B扫描仪的新医院时，这个“解决方案”是灾难性的。这揭示了一个关键的区别：数据的不平衡是数据偏见，但学习算法利用这种不平衡的倾向是一种**[算法偏见](@entry_id:637996)**[@problem_id:4530626]。

这一挑战甚至延伸到我们的基因本身。多基因风险评分（PRS）是一种革命性的工具，通过观察成百上千个遗传变异来估算一个人对冠状动脉疾病或乳腺癌等疾病的遗传易感性。然而，绝大多数用于开发这些评分的遗传学研究都是在欧洲血统的人群中进行的。当这些评分应用于非洲或亚洲血统的个体时，其预测能力往往会急剧下降。原因微妙而美丽，从生物学意义上讲。评分中赋予每个遗传变异的“权重”取决于其与疾病的[统计关联](@entry_id:172897)，而这种关联又与局部遗传变异模式（即[连锁不平衡](@entry_id:146203)）纠缠在一起。这些模式在不同祖源的人群中有所不同。在一个群体中是某种疾病的良好路标的变异，在另一个群体中可能是一个糟糕的路标。将一个在某个群体上训练的PRS应用于另一个群体，就像用巴黎的地图在东京导航一样。底层的现实是不同的，这个工具，无论多么复杂，都变得不可靠。这种可移植性的丧失是确保基因组医学的成果能够被公平分享的一项深远挑战[@problem_-id:5139455]。

### 分院帽：分配资源与机会的算法

除了诊断，算法越来越多地被用作守门人，对人们进行分类，以决定谁能获得稀缺的资源、机会或援助之手。它们正在成为我们社会新的分院帽。

一个里程碑式的真实案例，也在我们的问题集中有所呼应，涉及一个被美国医院用来识别哪些患者最能从“高风险护理管理”项目中受益的算法。该算法的目标是预测哪些患者未来将有最高的医疗保健需求。但你如何衡量“健康需求”？设计者选择了一个看似聪明且客观的代理指标：未来的医疗保健成本。逻辑很简单：病情更重的人花费更多。该算法奏效了，但它系统性地对黑人患者存在偏见。在实际病情相同的情况下，算法分配给黑人患者的风险评分低于白人患者。原因何在？由于复杂的社会因素网络，在任何给定的疾病水平下，黑人患者在历史上产生的医疗保健成本都较低。通过使用成本作为需求的代理指标，该算法无意中学会了复制并放大这些历史上的不平等，实际上是拒绝了它本应帮助的人群所需要的护理[@problem_id:4491370]。

这种情况不仅是技术上的失败，也是法律上的失败。根据美国民权法，一种“表面中立”（即不明确提及种族）但对受保护群体产生不合理的、不成比例的负面影响的做法，可以根据**差异性影响**理论构成非法歧视。存在一种歧视性较小的替代方案——例如，一个直接预测疾病而非成本的算法——成为一个关键的证据。这在计算机科学和法律之间建立了强有力的联系，要求[算法设计](@entry_id:634229)者不仅要考虑预测准确性，还要考虑民权。类似的原则也适用于《遗传信息非歧视法案》（GINA）等法律，其中使用从遗传数据中得出的“风险评分”仍被视为使用“遗传信息”，即使雇主的算法从未看到原始基因。法律，就像一个优秀的程序员，关心的是功能依赖性，而不仅仅是变量的名称[@problem_id:4486109]。

这个主题——算法的结构本身可以与偏见相互作用——即使在计算机科学最抽象的角落也引起了共鸣。考虑著名的[稳定婚姻问题](@entry_id:276830)，该问题旨在根据两组人（比如，住院医师和医院）的排名偏好来匹配他们。[Gale-Shapley算法](@entry_id:635226)提供了一个“稳定”的优美解决方案——没有住院医师和医院会同时宁愿抛弃他们分配的伙伴而与对方配对。该算法甚至是“提议方最优”的，意味着提出提议的一方（例如，住院医师）在任何稳定安排中都能获得他们所能期望的最佳结果。但如果一个AI生成了偏好列表，并且它有偏见地使一组住院医师对所有医院都显得更具吸[引力](@entry_id:189550)呢？[Gale-Shapley算法](@entry_id:635226)的提议方最优特性将*放大*这种偏见，确保受偏爱的群体获得绝对最好的伙伴。然而，如果偏见使得一组*医院*更具吸[引力](@entry_id:189550)，那么对住院医师的提议方最优性则*减轻*了偏见对医院的影响，因为接受方得到的是他们最差的稳定伙伴。算法本身没有偏见，但其固有的不对称性像一个透镜，要么聚焦，要么扩散其输入中存在的偏见[@problem_id:3273968]。

### 看不见的偏见架构

有时，偏见的来源既不在于数据点本身，也不在于算法的逻辑，而在于收集和传输数据的系统管道本身。想象一个医疗系统从两家医院汇总数据，一家拥有现代、高质量的数据系统，另一家拥有老旧、笨重的数据系统。这些不同系统之间“交谈”的能力被称为**[互操作性](@entry_id:750761)**。在互操作性差的医院里，关键的实验室测试可能经常无法被记录在电子健康记录中。如果我们随后在汇总的数据上训练一个风险模型，我们将是在一个有偏见的样本上进行训练——这个样本过度代表了来自现代医院的患者，而低估了来自老旧医院的患者。如果两家医院的患者群体不同，模型将对世界产生一个歪曲的看法。良好的互操作性，通过确保为每个人提供一致的数据捕获，可以成为减轻偏见的强大力量。它甚至可以帮助将一个棘手的统计问题从“[非随机缺失](@entry_id:163489)”（数据因我们看不到的原因而缺失）转变为“[随机缺失](@entry_id:168632)”（我们可以看到，因此可以在统计上纠正其缺失的原因）[@problem_id:4859983]。

### 回音室与反馈循环

最后，人类与算法之间的互动并非单向的。算法从我们的行为中学习，而我们的行为又受到算法输出的影响。这可以产生强大的反馈循环。考虑一个患有妄想症的人——例如，钟情妄想，认为某个公众人物暗恋自己。这个人的确认偏见导致他们不成比例地点击和参与他们认为是其信念证据的在线内容。一个旨在最大化参与度的个性化推荐算法观察到了这种行为。它不理解内容；它只知道某种“类型”的帖子能从该用户那里获得更多点击。它开始向用户展示更多这类内容，创建一个个性化的“证据”推送，似乎在验证用户的妄想。算法和认知偏见进入了一个反馈循环，相互加强，可能在客观中立的信息面前强化一种有害的信念[@problem_id:4706254]。

### 前进之路：呼吁跨学科对话

那么，该怎么办呢？应用本身指明了前进的方向。那个低估了有认知障碍的老年人痛苦的疼痛评分AI问题告诉我们，像**重新校准**这样的技术修复至关重要[@problem_id:4423659]。但它也提出了一个伦理问题：一个错误的代价是什么？在姑息治疗中，低估疼痛的危害可以说远大于过度治疗疼痛的危害。这种非对称成本必须被编程到系统的决策中，这是一项不仅需要工程师，还需要伦理学家和临床医生的任务。

此外，当我们试图“修复”偏见时，我们立即会遇到另一个问题：“公平”意味着什么？想象一下审计一个临床试验招募算法。一个患者权益组织可能将公平定义为所有人口群体享有平等的机会（人口统计均等）。医院可能将其定义为确保没有符合条件的患者被错过（[机会均等](@entry_id:637428)）。试验的赞助商可能将其定义为确保被算法标记的人确实符合条件（预测均等）。这些都是有效的，但在数学上是不同的，并且往往是相互排斥的目标。没有一个单一的“公平”按钮可以按下。决定做出哪些权衡不是一个纯粹的技术问题；它是利益相关者之间的谈判，是一场关于价值观的对话[@problem_id:5068073]。

这是我们对[算法偏见](@entry_id:637996)应用之旅的终极教训。问题并非始于算法，也无法仅由算法专家解决。它是一面镜子，反映了我们数据、机构、语言甚至我们自己思想中隐藏的偏见。要构建不仅强大而且公正的算法，我们需要一种新的对话——一种计算机科学家、医生、律师、伦理学家以及受这些系统影响的社区共同参与的对话，不仅决定我们的算法*能*做什么，还要决定它们*应该*做什么。