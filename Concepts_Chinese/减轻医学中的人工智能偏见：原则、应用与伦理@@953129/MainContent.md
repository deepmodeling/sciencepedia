## 引言
人工智能为医学变革带来了前所未有的希望，它能以超乎人类的准确性诊断疾病和预测结果。然而，这项强大的技术并非天生客观。当使用反映历史不平等的数据进行训练，或采用优先考虑平均性能的算法进行设计时，AI系统会继承、放大并固化有害的偏见。这带来了一个严峻的挑战：我们如何在利用医疗AI优势的同时，确保其公平、公正和值得信赖？本文将直面这一问题，全面概述医疗保健领域中的AI偏见。第一章**“原则与机制”**将剖析偏见的基本来源，探讨有缺陷的数据和过于简化的算法如何造成扭曲，以及这些错误如何通过临床系统层层传递，造成深远的伦理伤害。在此基础上，第二章**“应用与跨学科联系”**将审视这些原则在实践中的应用，展示临床实践中的真实困境，以及构建真正负责任和公平的AI系统所需的复杂法律、伦理和组织框架。

## 原则与机制

想象一下，你正通过一个强大的新镜头观察世界。这个镜头承诺能揭示隐藏的模式，看到肉眼无法察觉的事物。这正是人工智能在医学领域的希望。但如果镜头本身就有缺陷呢？或者，如果它所观察的世界早已被一个扭曲的滤镜所过滤呢？你看到的图像将不是现实，而是一个歪曲的版本。要理解如何负责任地构建和使用这些AI镜头，我们需要深入探究它们的原则和机制，理解其扭曲的根源，并学会如何纠正它们。这不仅仅是一个技术挑战，更是一个深刻的伦理问题，触及关怀、信任和正义的本质。

### 两大原罪：有偏见的数据与有偏见的算法

在AI偏见的核心，我们发现了两个根本性的错误来源。可以把它们看作是机器学习的原罪。第一个是AI学习所依赖的世界存在问题；第二个是它的学习方式存在问题。

第一宗罪是**数据偏见**。AI模型的优劣取决于其所使用的数据。如果数据是对现实的歪曲或不完整的表述，模型将继承并常常放大这种歪曲。一个典型的例子是**标签偏见**[@problem_id:4421580]。我们可能想训练一个AI来检测像败血症这样的疾病，于是我们给它输入成千上万份患者记录，并将最终的诊断代码作为“真实标签”。但那个代码真的是真相吗？实际上，记录下来的标签，我们称之为$Y$，并非真实的临床状态$Z$。它是一个复杂的人类过程的产物，不仅取决于患者的病情，还取决于医生的记录习惯、计费和报销的压力，甚至医生对患者性别或语言能力的无意识偏见。AI天真地将这个由人类产生的、充满混乱的标签$Y$奉为圭臬，勤奋地学习着那些最初扭曲了标签的社会和经济模式。

另一种数据偏见是**代表性偏见**。想象一个AI被训练来通过[CT扫描](@entry_id:747639)对肿瘤进行分类。在我们的假设训练数据集中，900张扫描来自A厂商的扫描仪，而只有100张来自B厂商[@problem_id:4530626]。这个AI将成为A厂商图像的专家，却是B厂商图像的新手。当它被部署到一家主要使用B厂商设备的医院时，其性能可能会急剧下降。它训练时所处的世界并非其应用的真实世界。这在医学中是一个常见问题，历史数据往往过度代表多数群体，而对[边缘化](@entry_id:264637)群体的代表不足，导致模型对最需要它们的人群效果最差。

第二宗罪是**[算法偏见](@entry_id:637996)**。这不是数据中的缺陷，而是镜头本身——即算法的学习过程存在缺陷。大多数标准的AI模型都是通过一个称为**[经验风险最小化](@entry_id:633880)（ERM）**的过程进行训练的。通俗地说，这意味着算法被编程设定了一个单一的目标：最小化整个训练数据集上的平均错误。现在，再次考虑我们那份不平衡的CT扫描数据：900张来自群体$S=0$，100张来自群体$S=1$。为了获得最佳的总体分数，算法可以采取一种懒惰但有效的策略：在900张的多数群体上获得满分，而在100张的少数群体上完全失败。它的平均错误率仍然会很低。在一个严峻但可能的情境中，一个算法可能会学到一种策略，在多数群体上实现零错误，但在少数群体上却有高达$20\%$的错误率，而其总体训练错误率与一个在两个群体上都有少量错误率的更公平模型相同[@problem_id:4530626]。算法由于其设计，学会了牺牲少数来迎合平均。这不是恶意；这只是数学，是我们给它设定的简单化目标的直接后果。

### 偏见的级联：从代码到临床

这些基础性偏见并不会整齐地被限制在计算机内。它们会级联式地进入诊所复杂的人类环境，与我们自身的认知偏见相互作用，创造出一个可能放大不平等的社会技术系统。

最阴险的影响之一是**客观性错觉**的破灭。一位医生可能会使用一个AI工具来评估血栓的风险评分。他们不知道的是，这个工具系统性地低估了某个特定人口群体的风险，比如平均低估了$\delta = 0.10$ [@problem_id:4421662]。医生对所有人采用了一个“公平”的统一治疗阈值：如果分数高于$0.30$就进行治疗。他们相信自己是公正的。然而，对于一个来自弱势群体、真实风险为$0.35$的患者，带偏见的AI可能会报告一个$0.25$的分数，从而使他们无法获得拯救生命的治疗。将一个公平的规则应用于一个不公平的输入，会产生一个不公平的结果。这是一个至关重要的教训：在使用有偏见的工具时，“不知道”患者的群体身份并非公平，而是疏忽。

当人类和算法的偏见方向相同时，情况变得更加危险。考虑一个败血症预测工具，由于历史数据较为稀疏，它为来自社会弱势群体$G_2$的患者生成了较低的风险评分。现在，假设医院里的临床医生已经怀有无意识的偏见，导致他们需要更多证据才会对$G_2$群体的患者进行治疗。他们可能会本能地对这个群体使用更高的决策阈值（$t_2 = 0.45$），而对其他群体则使用较低的阈值（$t_1 = 0.30$）[@problem_id:4849720]。算法的偏见降低了分数，而临床医生的偏见提高了门槛。这两种效应复合在一起，造成了巨大的护理差距。问题不仅仅在于有缺陷的AI或有缺陷的人，而在于一个偏见相互级联和放大的有缺陷的系统。

这把我们引向一个关键的人为因素：**自动化偏见**。这是我们有据可查的倾向，即过度信任和过度依赖自动化系统，尤其是在我们忙碌、疲惫或不确定的时候[@problem_id:4420886]。我们看到屏幕上的一个数字，我们自己的判断就退居二线了。这种认知上的顺从可能导致一种深刻的伤害，称为**认知不公**。这不仅仅是诊断错误的问题；它关乎从根本上不尊重一个人作为其自身经历的知情者的能力。

想象一位来自边缘化社区的患者在急诊室里，试图解释自己的症状。他们的叙述复杂而微妙。AI，一个专有的黑箱，处理了他们的数据并输出一个低风险评分。临床医生，顺从于“客观的”机器，驳回了患者的陈述[@problem_id:4850139]。这是**证言不公**：患者的证词被不公正地给予了过低的信誉度。算法的不透明性使情况变得更糟；因为临床医生无法看到AI的推理过程，他们无法将其与患者的故事进行权衡。这变成了一个人与一个神谕之间的争论。

更深层次的是**诠释不公**。当一个人的经历无法被理解时，就会发生这种情况，因为系统——医院、医学语言、AI——缺乏理解这些经历所需的概念[@problem_id:4436694]。如果一个用于慢性疼痛的AI训练数据主要反映了某个人口群体的经历，那么它内部关于疼痛的“本体论”可能完全无法识别疼痛在其他人身上表现或描述的不同方式。当一个患者的故事不符合模型狭隘的类别时，他们就变得无法被理解。系统不仅仅是不相信他们；它甚至听不懂他们。

### 恶性循环：当偏见自我滋养

也许所有机制中最危险的是**表演性反馈循环**，即AI的预测会主动改变世界，从而强化其自身的偏见。这造成了一个恶性的、自我实现的循环。

考虑一个部署在医院的败血症警报系统[@problem_id:4850167]。AI有一个轻微的初始偏见，导致它对第1组患者的警报频率高于第2组。当临床医生看到警报时，他们自然更有可能去寻找败血症的迹象，并将其记录在患者的病历中。当需要重新训练AI模型时，医院就使用这些病历编码的数据作为“真实标签”。

让我们来看一个假设但现实的场景中的数字。败血症的真实患病率在两个群体中是相同的，$p=0.10$（即$10\%$）。但由于AI对第1组的警报更多，它触发了更多的记录。一个季度后，病历数据中*观察到的*败血症患病率在第1组变为$32.7\%$，而在第2组仅为$29.5\%$。AI现在基于这份新的、被污染的数据进行重新训练。它从数据中学到，败血症在第1组中确实更常见。因此，它会调整其内部逻辑，在下一个季度变得更倾向于为第1-组触发警报。最初的微小偏见被系统自身的运作所证实和放大。模型正在从自身扭曲的回声中学习，每一次循环都离真实情况越来越远。

### 缓解之路：有原则的AI的原则

要应对这种偏见的级联，需要的不仅仅是一个简单的技术修复。它要求一种植根于医学伦理基本原则的整体方法：正义、不伤害、自主和行善[@problem_id:4887177]。

**正义**要求利益和负担的公平分配。这意味着我们必须超越“通过无知实现公平”，并积极审计我们的AI系统是否存在偏见。我们必须比较它们在所有相关的人口和社会群体中的错误率、校准情况以及现实世界的影响。这也意味着重新设计算法本身。与其仅仅最小化平均错误，我们可以构建旨在最小化处境最差群体的错误的模型，这一概念被称为**群体[稳健优化](@entry_id:163807)**[@problem_-id:4530626]。

**不伤害**，即“不造成伤害”的义务，要求我们预见并减轻可预见的风险。这始于**透明度**。一家部署病理学AI的医院实验室必须披露，如果该工具对某一亚群患者的敏感度显著较低——比如$0.78$对$0.94$ [@problem_id:4366370]。这种透明度不仅仅是为了作秀；它是一种至关重要的安全机制，使临床医生能够采取保护性行动，例如对高风险组的所有患者下令进行二次人工审核。为了管理自动化偏见，我们还必须遵守严格的**人因工程**，在AI的局限性和不确定性上提供清晰的标签，并进行现实的验证研究，测试当AI出错时临床医生的行为[@problem_id:4420886]。

**自主**要求尊重临床医生和患者双方的能动性和知情决策。对临床医生而言，这意味着AI不能是一个不透明的黑箱。它必须提供**案例级解释**，让临床医生能够理解并在必要时质疑其推理[@problem_id:4850139]。对患者而言，自主意味着他们的声音不能被压制。我们可以设计正式保护患者叙述的系统。例如，我们可以规定，在任何决策中，给予患者自身陈述的权重$w_P$永远不能为零，而必须大于某个最小阈值$\tau$ [@problem_id:4436694]。我们甚至可以建立程序，在AI的结论与患者的证词强烈不符时，触发强制性的人工审查。

最后，**行善**，即为患者福祉而行动的义务，提醒我们最终的目标。一个AI工具的成功不能用单一的准确性指标来评判。必须通过仔细的评估来证明它能提供净临床效益。并且，它需要一个学习的承诺。当一个系统遇到它无法理解的患者体验时，它不应该置之不理。相反，它应该触发一个流程来扩展自身的诠释资源，更新其内部的世界地图$\Phi$，以便下一次它能听得更清楚[@problem_id:4436694]。

构建公平有效的医疗AI是我们这个时代伟大的科学和伦理征程之一。它迫使我们直面根植于我们制度和我们自身的偏见。通过理解这些系统可能失败的原则和机制，我们可以开始着手设计它们以使其成功——不仅仅是作为预测的工具，更是作为正义和治愈的工具。

