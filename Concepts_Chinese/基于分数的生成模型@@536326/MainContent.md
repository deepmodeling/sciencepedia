## 引言
在飞速发展的[生成式人工智能](@article_id:336039)领域，一类尤为优雅且强大的模型脱颖而出：[基于分数的生成模型](@article_id:638375)。这些模型为创造提供了一个有原则的框架，它并非通过记忆数据，而是通过学习结构如何从混沌中涌现的基本过程。它们解决了生成式建模的核心挑战：机器如何能从纯粹、无结构的随机性出发，创造出连贯、新颖且逼真的数据片段，无论它是一幅图像、一个蛋白质序列，还是一个物理场？本文将深入探讨这项引人入胜的技术。第一章“原理与机制”将揭开核心理论的神秘面纱，解释[扩散](@article_id:327616)的数学原理及其如何通过学习到的[分数函数](@article_id:323040)进行逆转。随后的“应用与跨学科联系”一章将展示这些基础思想如何彻底改变科学发现，从工程化新分子到求解支配我们宇宙的方程。

## 原理与机制

想象一下，你正在观看一部电影，一滴墨水滴入一杯清水中。墨水扩散开来，扭曲成复杂而美丽的卷须状，最终扩散成均匀的淡灰色。这是一个熵增的过程，一个从有序到无序的过程。它简单而自然。现在，想象一下将这部电影倒放。墨水颗粒奇迹般地从均匀的灰色中聚集起来，追溯它们错综复杂的舞蹈轨迹，直到重新汇聚成一滴完美的墨滴。这看起来像是魔法，违背了自然规律。[基于分数的生成模型](@article_id:638375)正是一种计算魔法，它教会计算机完成这一壮举：从纯粹、无结构的噪声（相当于那杯灰色的水）开始，精心地逆转扩散过程，从而创造出像图像、蛋白质或晶体一样连贯、复杂的结构 [@problem_id:77062]。

本章将引导您了解使这种“解扰”成为可能的原理。我们将从零开始构建整个概念，跟随数据从有结构到噪声的旅程，然后，至关重要的是，踏上返回的数学之旅。

### 正向过程：一场刻意走向混沌的沉降

第一步是用数学方式定义这个“加扰”过程。我们需要一种方法，能够获取一份数据——比如说一张猫的图片，它只是一个高维的像素值向量 $\mathbf{x}_0$——并可控地破坏其包含的信息，直到只剩下随机噪声。这通过一个**正向扩散过程**来完成，通常由一个随机微分方程（SDE）描述。

在问题 [@problem_id:2444369] 中探讨过，一个简单而常见的SDE选择是：

$$
d\mathbf{x}_t = \sqrt{\beta(t)} d\mathbf{w}_t
$$

我们不必被这个符号吓到。该方程只是说，在一个微小的时间步长 $dt$ 内，我们的数据向量的变化量 $d\mathbf{x}_t$ 是一小部分随机噪声。在这里，$d\mathbf{w}_t$ 代表维纳过程的一个步骤，这是对粒子随机、[抖动](@article_id:326537)运动的正式名称——可以把它想象成一个完全不可预测的轻推。函数 $\beta(t)$ 是我们选择的“噪声时间表”。它控制着在每个时间点 $t$ 我们添加多少噪声。通常，我们从少量噪声开始，然后逐渐增加。如果我们将此过程应用于从 $t=0$ 到最终时间 $t=T$ 的时间区间，任何初始图像 $\mathbf{x}_0$ 都将被转换为一个样本 $\mathbf{x}_T$，它与纯高斯噪声（就像老式电视机上的雪花）无法区分。我们已经成功地、在数学上“把鸡蛋搅乱了”。

在任意时间 $t$，我们的数据向量的概率密度（记为 $p(\mathbf{x}, t)$）会根据一个相应的[偏微分方程](@article_id:301773)演化，该方程被称为 **Fokker-Planck 方程** [@problem_id:77062] [@problem_id:66016]。这个方程就像一个[概率守恒](@article_id:310055)定律，描述了可能的数据点云如何随时间[扩散](@article_id:327616)和扁平化。

### 秘密地图：[分数函数](@article_id:323040)

现在是见证奇迹的时刻：倒放电影。要逆转这个过程，我们需要一个向导。在任何时间点 $t$ 和对于任何噪声向量 $\mathbf{x}_t$，我们需要知道应该朝哪个方向推动它，才能让它变得稍微*不那么*嘈杂，并且稍微*更像*原始数据。我们需要一张地图，引导我们走出噪声的荒野，回到结构化数据的土地。

这张地图被称为**[分数函数](@article_id:323040)**，或简称**分数**。在数学上，它被定义为[概率密度](@article_id:304297)对数的梯度：

$$
\mathbf{s}(\mathbf{x}, t) = \nabla_{\mathbf{x}} \log p(\mathbf{x}, t)
$$

这个公式虽然简洁，却蕴含着丰富的直觉。项 $p(\mathbf{x}, t)$ 是我们在时间 $t$ 的噪声数据的[概率密度](@article_id:304297)。在该值高的地方，我们处于一个“合理”的噪声图像区域，这些图像可能源自真实数据。对数 $\log p(\mathbf{x}, t)$ 是一种数学上的便利，使处理变得更容易。关键部分是[梯度算子](@article_id:339615) $\nabla_{\mathbf{x}}$。简单来说，梯度是一个指向函数最陡峭上升方向的向量。

因此，分数 $\mathbf{s}(\mathbf{x}, t)$ 是一个向量，对于任何噪声图像 $\mathbf{x}_t$，它都指向能最快增加其[概率密度](@article_id:304297)的方向。它是一个路标，上面写着：“通往更合理数据的方向！”

### 回归之旅：反向SDE

有了我们的地图——[分数函数](@article_id:323040)，我们现在可以写下回归之旅的指令了。正如正向过程由一个SDE描述一样，[反向过程](@article_id:378287)也是如此。根据问题 [@problem_id:2444369] 和 [@problem_id:3279903] 的推导，从噪声生成数据的反向时间SDE可以写成：对于我们引入的简单正向过程，其反向方程非常优雅 [@problem_id:2444369]：

$$
d\mathbf{x}_s = \beta(T-s) \mathbf{s}(\mathbf{x}_s, T-s) ds + \sqrt{\beta(T-s)} d\mathbf{w}_s
$$

在这里，$s$ 是一个从 $0$ 运行到 $T$ 的反向时间变量，对应于原始时间 $t$ 从 $T$ 降到 $0$。仔细观察第一项，即提供方向的“漂移”项。它是我们的[分数函数](@article_id:323040) $\mathbf{s}$，由噪声时间表 $\beta$ 进行缩放。这个方程精确地告诉我们如何生成一幅图像：从一个[随机噪声](@article_id:382845)向量 $\mathbf{x}_T$ 开始。在每个微小的时间步长 $ds$ 内，计算你当前位置的分数向量，它会告诉你朝向“更像数据”的结构的方向。将你的向量朝那个方向轻推，并加入一点新的随机性（$d\mathbf{w}_s$ 项）以保持探索。通过从时间 $T$ 逐步回溯到 $0$，你将描绘出一条从纯噪声到全新、有结构且逼真的数据样本的路径。

一个常见的误解是，这个[反向过程](@article_id:378287)只是简单地使用反向的噪声来追溯某条特定的正向路径。事实并非如此。反向SDE是从所有可能导致最终噪声状态的路径的*系综*中生成一个*新*样本，这是在问题 [@problem_id:3279903] 中强调的一个关键点。这并非重放一部电影，而是创造一部遵循相同物理定律的新电影。

有趣的是，还存在一个相应的确定性过程，即一个称为**[概率流](@article_id:311366)[常微分方程](@article_id:307440)（ODE）**的常微分方程，它能沿着平滑的[轨迹生成](@article_id:354305)样本，这些样本与SDE具有完全相同的[边际概率](@article_id:324192)密度 [@problem_id:66016]。这揭示了随机世界与确定性世界之间深刻而美妙的联系，为生成过程提供了一条没有SDE随机[抖动](@article_id:326537)的“高速公路”。

### 学习地图：[分数匹配](@article_id:639936)与能量

这里有一个主要障碍：我们实际上并不知道所有中间噪声状态的真实[概率密度](@article_id:304297) $p(\mathbf{x}, t)$，因此无法计算真实的分数 $\mathbf{s}(\mathbf{x}, t)$。这就是[深度学习](@article_id:302462)登场的时刻。我们训练一个强大的[神经网络](@article_id:305336)，我们称之为 $\mathbf{s}_{\theta}(\mathbf{x}, t)$，来近似真实的分数。

但是，你如何训练一个网络去匹配一个你并不知道的函数呢？这项技术被称为**[分数匹配](@article_id:639936)**。虽然其形式化的目标函数可能看起来很复杂 [@problem_id:90201]，但最流行和最直观的方法是**[去噪分数匹配](@article_id:642175)** [@problem_id:2749047]。其过程非常简单：
1.  从你的数据集中取一个干净的数据样本 $\mathbf{x}_0$。
2.  随机选择一个时间 $t$，并加入相应量的已知随机噪声 $\boldsymbol{\epsilon}$，得到一个噪声样本 $\mathbf{x}_t$。
3.  将这个噪声样本 $\mathbf{x}_t$ 和时间 $t$ 输入到你的神经网络中。
4.  训练网络来预测你在步骤2中添加的噪声向量 $\boldsymbol{\epsilon}$。

事实证明，训练一个网络来预测所添加的噪声，在数学上等同于训练它学习[分数函数](@article_id:323040)！这个网络变成了一个通用的“[去噪](@article_id:344957)器”，对于任何程度的损坏，它都知道如何从噪声中提取信号。

正如问题 [@problem_id:3122236] 中所探讨的，这个框架揭示了与物理学更深层次的联系。如果我们对分数网络强制施加某种结构会怎样？任何[梯度场](@article_id:327850)的一个关键性质是它是**保守的**（其旋度为零）。真实分数作为 $\log p(\mathbf{x}, t)$ 的梯度，根据定义就是一个保守[矢量场](@article_id:322515)。我们可以通过将分数网络参数化为一个[标量势](@article_id:339870)（我们称之为**能量函数** $E_{\theta}(\mathbf{x}, t)$）的梯度，将这个物理先验构建到我们的模型中。即：

$$
\mathbf{s}_{\theta}(\mathbf{x}, t) = -\nabla_{\mathbf{x}} E_{\theta}(\mathbf{x}, t)
$$

这是一个意义深远的步骤。它在基于分数的模型和**[基于能量的模型](@article_id:640714)（EBMs）**之间建立了一座正式的桥梁，其中高概率状态对应于低能量状态。生成过程现在可以被看作是在一个时变[能量景观](@article_id:308140)上的下降过程。这种约束不是一种限制，而是对问题真实内在结构的反映，是一个物理原理如何指导强大机器学习系统设计的美妙实例 [@problem_id:3122236]。

### 引导创造：[条件生成](@article_id:641980)

到目前为止，我们的模型可以从数据分布中生成随机样本——例如，如果在猫的数据集上训练，就可以生成随机的猫的图像。但是，如果我们想控制输出呢？如果我们想生成一个*特定*类别的图像，比如一只虎斑猫，或者一个具有*特定*功能的蛋白质呢？这通过**引导**来实现。

#### 分类器引导

一种直接引导生成的方法是使用一个独立的、[预训练](@article_id:638349)的分类器网络。假设我们有一个分类器 $p_{\phi}(y | \mathbf{x}_t)$，即使对于一个噪声图像 $\mathbf{x}_t$，它也能预测其属于某个类别 $y$（例如，“虎斑猫”）的概率。根据问题 [@problem_id:3115994] 的推导，我们可以使用该分类器对数概率的梯度 $\nabla_{\mathbf{x}_t} \log p_{\phi}(y | \mathbf{x}_t)$ 来引导这个过程。这个[梯度向量](@article_id:301622)指向使噪声图像 $\mathbf{x}_t$ 看起来*更像*类别 $y$ 的方向。

我们可以简单地将这个引导向量加到我们原始的[分数函数](@article_id:323040)上：

$$
\mathbf{s}_{\text{guided}}(\mathbf{x}_t, y) = \mathbf{s}_{\theta}(\mathbf{x}_t, t) + g \cdot \nabla_{\mathbf{x}_t} \log p_{\phi}(y | \mathbf{x}_t)
$$

这里，$g$ 是一个引导强度参数，控制我们引导的力度。然后将这个修改后的分数代入反向SDE，生成过程将偏向于产生所需类别的输出。但需要注意的是：过多的引导可能并非好事。正如在问题 [@problem_id:3115994] 中所展示的，将 $g$ 设置得太高可能导致“过冲”伪影——即不自然、夸张的特征——因为模型过于努力地去满足分类器。

#### 无分类器引导

一种更现代、更优雅的技术，称为**无分类器引导（Classifier-Free Guidance, CFG）**，无需独立的分类器即可达到同样的目标 [@problem_id:3116005]。诀窍在于训练过程。我们训练一个单一的分数模型，但在训练期间，我们有时向其提供条件信息 $\mathbf{y}$（例如，文本描述），有时则隐藏它（使用一个特殊的空标记）。

这使得单个模型能够同时学习到条件分数 $\mathbf{s}_c(\mathbf{x}_t, t, \mathbf{y})$ 和无条件分数 $\mathbf{s}_u(\mathbf{x}_t, t)$。这两个向量之差 $\mathbf{s}_c - \mathbf{s}_u$ 代表了纯粹的“引导方向”——它正是将你从一个通用对象引向一个符合描述 $\mathbf{y}$ 的对象的方向。

在生成时，我们可以通过从无条件分数开始，并在这个引导方向上移动一定距离来创建一个引导分数：

$$
\mathbf{s}_{\text{guided}}(\mathbf{x}_t, \mathbf{y}) = \mathbf{s}_u(\mathbf{x}_t, t) + w (\mathbf{s}_c(\mathbf{x}_t, t, \mathbf{y}) - \mathbf{s}_u(\mathbf{x}_t, t))
$$

在这里，引导权重 $w > 1$ 意味着我们在进行[外推](@article_id:354951)，推动生成结果与条件 $\mathbf{y}$ 的对齐程度*甚至超过*训练中所见的水平。这个简单而强大的思想是许多最先进的文生图模型背后的引擎。其有效性依赖于“无条件”分数对条件信息真正“无知”，这是一个被称为**条件泄露**的问题，在实践中必须小心管理 [@problem_id:3116005]。

从[扩散](@article_id:327616)的简单物理学，到随机微积分的数学，再到[深度神经网络](@article_id:640465)的工程学，基于分数的模型代表了思想的辉煌融合。它们提供了一个有原则、强大且出奇直观的框架，用于教导机器完成终极的创造行为：从混沌的核心中召唤出秩序。

