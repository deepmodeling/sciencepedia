## 引言
在[并发编程](@entry_id:637538)的世界里，确保共享资源被安全访问是一项至关重要的挑战。简单的操作在被多个线程同时执行时可能会灾难性地失败，从而导致竞争条件。为解决此问题，硬件提供了原始的、不可分割的操作。`Test-And-Set` 指令便是这些原子构建块中最基本的一种，它提供了一种看似直接的方法来创建锁并强制实现互斥。

然而，`Test-And-Set` 的表面简单性具有欺骗性。如果仅仅依赖这个原语而没有理解它与底层硬件和[操作系统](@entry_id:752937)的深层交互，可能会导致系统不仅速度缓慢，而且在根本上是不正确或不安全的。本文旨在弥合[原子指令](@entry_id:746562)的简单理论与构建健壮并发系统的复杂现实之间的鸿沟。

我们将首先剖析 `Test-And-Set` 的核心原理，展示其工作方式以及如何用它来构建一个基本的[自旋锁](@entry_id:755228)。在“原理与机制”部分，我们将揭示这种简单锁背后隐藏的风险，从多核系统上的灾难性性能问题到微妙的内存可见性错误。然后，在“应用与跨学科联系”部分，我们将拓宽视野，探讨这些底层挑战如何在[操作系统](@entry_id:752937)、[设备驱动程序](@entry_id:748349)和大规模应用程序中表现为[死锁](@entry_id:748237)和[优先级反转](@entry_id:753748)等关键问题。通过这一旅程，您将了解到 `Test-And-Set` 不仅仅是一条指令，更是一个洞察现代系统设计中相互关联挑战的窗口。

## 原理与机制

### 对不可分割性的追求

想象一下，你和一位朋友正在用一块共享的黑板计票。你们都看到了当前的计票数，比如“7”，然后在脑海中各自加上自己的一票得到“8”，接着擦掉“7”写上“8”。如果你们俩在完全相同的时间这么做，会发生什么？你们都读到了“7”，都计算出“8”，然后都写上了“8”。投了两票，但计数只增加了一。这就是经典的**竞争条件**（race condition），[并发编程](@entry_id:637538)中的一个根本性风险。

在计算机世界里，像 `count = count + 1` 这样的操作并非一步完成。它至少包含三个步骤：从内存中 `load`（加载）、一个 `add`（加法）操作，以及一个 `store`（存储）回内存。如果两个处理器核心试图同时执行此操作，它们的操作可能会交错进行，导致与黑板计票类似的错误。我们需要一种方法使这个操作序列变得**原子化**（atomic）——即表现得像一个单一、不可分割的瞬时步骤。

硬件设计者为此提供了一个强大而原始的工具：**测试并设置（Test-And-Set）** 指令。可以把它想象成一种对内存进行的特殊“设置”操作。你告诉它：“将这个内存位置设为1（我们称之为‘已锁定’），但首先，告诉我你改变它*之前*的值是多少。”其神奇之处在于，硬件保证整个序列——读取旧值和写入新值——作为一个不可分割的单元发生。没有其他核心可以在“测试”和“设置”之间插入操作。

有了这个，我们就可以构建我们的第一个简单锁，即**[自旋锁](@entry_id:755228)**（spinlock）：

获取锁：
```
while (test_and_set(lock_variable) == 1) {
    // The lock was already 1 (locked), so spin and try again.
}
// If we exit the loop, it means test_and_set returned 0 (unlocked).
// We have successfully acquired the lock!
```

释放锁：
```
lock_variable = 0; // Set it back to unlocked.
```
这看起来异常简单。一个想要进入[临界区](@entry_id:172793)（critical section）——我们比喻中的黑板——的线程会反复尝试“赢得”这个锁。如果它发现锁是`0`，就将其设置为`1`并进入。如果发现锁已经是`1`，它就在这场竞争中失败，并在`while`循环中“自旋”，一次又一次地尝试。这保证了**互斥**（mutual exclusion）：在任何时候，只有一个线程能够“赢得”竞争并持有锁。问题解决了吗？

并非如此。当我们层层深入时，会发现这幅简单的图景是危险且不完整的。

### 简单的幻象：仅有原子性是不够的

我们的[自旋锁](@entry_id:755228)成功地阻止了两个线程同时*处于*临界区之内。但是，它能确保一个线程完成的工作能被下一个线程正确地看到吗？让我们考虑一个场景：线程$T_1$获取锁，写入`x = 1`，然后释放锁。紧接着，线程$T_2$获取同一个锁并读取`x`的值。$T_2$是否保证能看到`x = 1`？

直觉上的答案是“当然！”但在现代处理器上，答案却惊人地是“不一定”。处理器为了运行得更快会使用各种技巧，比如将写操作缓冲在本地的“存储缓冲区”（store buffer）中，或者对指令进行重排序。这可能导致$T_1$在主内存中更新锁变量的操作发生在其对`x`的写入对其他核心可见*之前*。于是，$T_2$可能获取锁，读取`x`，却看到旧值`0`。我们的锁提供了互斥，却没有提供数据**可见性**（visibility）。[@problem_id:3656524]

这揭示了一个更深层次的原理：仅仅对锁变量进行[原子操作](@entry_id:746564)是不够的。我们需要在锁操作与其保护的数据之间强制建立一种*排序*。为解决此问题，硬件提供了[内存排序](@entry_id:751873)语义。其中最重要的两种是：

-   **获取语义（Acquire Semantics）**：一个具有获取语义的操作是一个屏障。在其后的代码中，任何内存读或写操作都不能被重排序到它*之前*发生。当你获取一个锁时，你希望确保能看到前一个所有者所做的所有更改。

-   **释放语义（Release Semantics）**：一个具有释放语义的操作也是一个屏障。在其前的代码中，任何内存读或写操作都不能被重排序到它*之后*发生。当你释放一个锁时，你希望确保你的所有更改在允许其他线程进入前都已变得可见。

一个正确的锁实现必须使用带有**获取（acquire）**语义的 `test-and-set` 进行加锁，并使用带有**释放（release）**语义的简单存储操作进行解锁。这种配对创建了一种“同步于”（synchronizes-with）关系。$T_1$在其释放操作*之前*所做的一切，都保证对$T_2$在其获取操作*之后*可见。原子性与[内存模型](@entry_id:751871)之间这种微妙但至关重要的联系，是正确[并发编程](@entry_id:637538)的真正基石。

### 性能灾难：一场消息风暴

现在我们有了一个*正确*的锁，但它是一个*好*锁吗？当许多不同核心上的许[多线程](@entry_id:752340)试图同时获取我们的 `test-and-set` [自旋锁](@entry_id:755228)时，会发生什么？答案是一场性能灾难。

要理解原因，我们需要一个关于多核心如何[共享内存](@entry_id:754738)的简单模型。每个核心都有自己的小型、高速内存，称为**缓存**（cache）。当一个核心需要写入某个内存位置时，[缓存一致性协议](@entry_id:747051)（如常见的**MESI**协议）要求该核心拥有该数据的*独占*副本。如果其他核心拥有副本，它们必须被置为无效。这是通过在共享的系统总[线或](@entry_id:170208)互连上传输消息来完成的。

我们那个朴素的 `test-and-set` [自旋锁](@entry_id:755228)让每个等待的线程都在一个紧凑的循环中执行 `test_and_set(lock_variable)`。记住，`test-and-set` 总是执行一次*写*操作。这意味着每个自旋线程的每一次尝试都构成一次写操作。每个自旋的核心都在大喊：“我需要对锁变量的独占访问权来写入它！”这会在总线上触发一个**为获得所有权的读取（Read-For-Ownership, RFO）**请求。[@problem_id:3678516]

想象一下，当一个核心持有锁时，有15个核心正在自旋等待。核心A发出一个RFO请求，获取了缓存行，并执行了失败的 `test-and-set`。紧接着，核心B发出一个RFO请求，这会使核心A的副本失效，并将缓存行带到核心B。然后核心C也做同样的事情，依此类推。包含我们锁变量的单个缓存行在核心之间疯狂地传递，这种现象被称为**缓存行乒乓**（cache-line ping-ponging）。[共享总线](@entry_id:177993)完全被这些一致性消息所饱和。CPU都处于100%的使用率，但它们只是在互相“喊叫”，没有取得任何有用的进展。

这些浪费的一致性消息的总数与竞争处理器的数量$P$成线性关系。对于单次锁的获取，缓存未命中的次数可能在$2(P-1)$的量级。[@problem_id:3636425] 这是一个可扩展性的噩梦；你增加的核心越多，性能就越差。

一个常见的急救措施是**测试并测试并设置（test-and-test-and-set, TTAS）**锁。在这种方式下，线程首先在一个简单的读操作上自旋，读取锁变量。由于读取共享值不需要独占所有权，许[多线程](@entry_id:752340)可以在它们本地的缓存副本上自旋，而不会产生总线流量。只有当一个线程看到锁的值变为`0`时，它才会尝试执行昂贵的 `test-and-set`。这是一个巨大的改进，但它并没有完全解决问题。当锁被释放时，所有等待的线程几乎同时看到它变为空闲，导致“惊群效应”（thundering herd），它们蜂拥而上执行 `test-and-set`，从而引发一阵争用风暴。

### 调度器的暴政与真实世界

到目前为止，我们只考虑了硬件。当我们把[操作系统](@entry_id:752937)（OS）的因素加进来时，情况会变得更糟。现代[操作系统](@entry_id:752937)是抢占式的；它们可以在任何时候中断一个线程，让另一个线程运行。如果[操作系统](@entry_id:752937)决定在一个线程*持有我们的[自旋锁](@entry_id:755228)期间*抢占它，会发生什么？

这就是**锁护航**（lock convoy）的成因。[@problem_id:3686902] 被抢占的线程$T_H$进入休眠状态，但仍然持有锁。所有其他等待该锁的线程现在都在一个*不可能被释放*的锁上自旋，直到[操作系统](@entry_id:752937)最终决定再次调度$T_H$。在一个有很[多线程](@entry_id:752340)的系统上，这可能是在几毫秒之后——对于CPU来说是永恒的时间。所有运行自旋线程的核心都被完全浪费了，它们燃烧着[电力](@entry_id:262356)却一事无成。这种病态情况可能使整个系统陷入瘫痪。

如果持有锁的线程做了某些导致其长时间阻塞的事情，比如引发一个**页错误**（page fault），情况会更具灾难性。[@problem_id:3686954] 如果临界区代码触及了不在物理内存中的数据，[操作系统](@entry_id:752937)将阻塞该线程并从磁盘启动一个缓慢的I/O操作。现在，这个锁可能被持有数百万个CPU周期。在这种情况下，纯粹的[自旋锁](@entry_id:755228)是毁灭性的，因为它将导致所有等待的核心在这整个期间内无用地自旋。

这引出了一条来之不易的智慧：**纯[自旋锁](@entry_id:755228)只应用于保护那些极短且保证不会阻塞的[临界区](@entry_id:172793)。**

### 通往公平与可扩展性之路

我们的简单 `test-and-set` 锁已被证明充满了危险。它不仅效率低下，而且极不公平。在疯狂争抢一个被释放的锁时，无法保证等待时间最长的线程会获胜。完全有可能一对“幸运”的线程在它们之间来回传递锁，而一个“不幸”的线程则永远自旋，永远得不到机会。这被称为**饥饿**（starvation）。[@problem_id:3686904]

幸运的是，我们可以使用简单的原子原语构建更好的锁。

为了解决公平性问题，我们可以设计一种**票据锁**（ticket lock）。其思想如同熟食店的柜台一样优雅：“取个号”。它使用两个计数器：`next_ticket` 和 `serving_now`。要获取锁，线程原子地增加 `next_ticket` 并将结果值作为其个人票号。然后它开始自旋，等待 `serving_now` 等于其票号。当一个线程释放锁时，它只需增加 `serving_now`。这强制执行了严格的先进先出（FIFO）顺序，保证了公平性并消除了饥饿。[@problem_id:3686904]

为了解决缓存行乒乓的[可扩展性](@entry_id:636611)问题，计算机科学家们设计了卓越的 **Mellor-Crummey and Scott (MCS) 锁**。其见解是革命性的：停止在共享位置上自旋。相反，[MCS锁](@entry_id:751807)构建了一个等待线程的链表。当一个线程想要锁时，它将自己添加到列表的末尾。然后，它在*自己列表节点中的一个标志*上自旋。这个标志是该线程本地的，因此自旋产生的总线流量为零。当当前的锁持有者完成时，它只需查看列表中的后继者，并翻转该后继者节点中的标志，从而有效地传递接力棒。每次锁获取的通信成本变为常数$O(1)$，与等待线程的数量无关。它公平、无饥饿，并且具有极佳的[可扩展性](@entry_id:636611)。[@problem_id:3678516]

我们与 `test-and-set` 的这段旅程揭示了计算机科学的一个缩影。我们从一个为解决根本问题而设计的简单、强大的硬件原语开始。然后我们发现了它微妙的缺陷——缺乏[内存排序](@entry_id:751873)保证、在争用下的灾难性性能、与[操作系统](@entry_id:752937)的危险交互。每一个缺陷都迫使我们更深入地探究，并建立起更复杂的理解层次，并最终构建出更精密的软件算法。从一条单一的[原子指令](@entry_id:746562)中，我们看到了硬件架构、[操作系统](@entry_id:752937)以及使并发计算成为可能的美妙算法之间相互交织的本质。

