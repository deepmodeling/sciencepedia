## 引言
在经济学、生物学和医学等多个不同领域，研究人员经常会遇到一种令人困惑的数据状况：大量的零值与有偏的正值结果分布并存。这个“过量零值”问题对传统的统计方法构成了重大挑战，这些方法常常产生无意义的预测，或无法捕捉真实的基础数据生成过程。本文旨在通过全面介绍两部分模型——一种优雅而强大的统计解决方案——来弥补这一差距。我们将首先深入探讨“原理与机制”，解释这些模型如何将一个复杂问题分解为两个更简单、更易于管理的部分。随后，“应用与跨学科联系”一章将展示这个灵活的框架如何为现实世界现象提供更深刻的见解，从医疗保健利用率建模到理解生命本身的逻辑。

## 原理与机制

### 双过程的故事：过量零值的问题

想象你是一位研究自然现象的科学家。你收集数据，并在绘制图表时，发现一个奇怪且反复出现的模式。无论你是分析年度医疗成本的健康经济学家，还是追踪每日黑巧克力消费量的营养流行病学家，抑或是计算肠道中特定微生物丰度的微生物学家，你都会看到同样的情况：一大堆零值。你的研究对象中，有大量的人没有产生任何费用，在某一天没有吃巧克力，或者体内没有那种特定的微生物。其余的数据，即正值，则散布在数轴上，通常形成一个长长的、有偏的尾部。 [@problem_id:4374973] [@problem_id:4615562] [@problem_id:2498632]。

你该怎么办？你的第一反应可能是求助于标准的统计工具。但你很快就会遇到麻烦。例如，一个经典的[线性回归](@entry_id:142318)模型，它根本没有结果不能为负的概念。它可能会轻松地预测某人将产生-100美元的医疗费用，这显然是荒谬的。

“好吧，”你说，“我将使用一个为非负数据设计的模型。”但这些模型也有其自身的问题。像 Gamma 分布或对数正态分布这样的分布是为连续流动的数据设计的。它们的数学基因中没有空间容纳在单一点上的巨大离散尖峰。试图将[连续分布](@entry_id:264735)强加于有一堆零值的数据，就像试图用一条光滑的毯子盖住一张中间竖着旗杆的床。你最终会扭曲毯子，并且在任何地方都得不到好的拟合效果。[@problem_id:4615562]

即使是像泊松分布这样的标准计数模型，虽然它自然包含零，但也有其自身的严格规则。泊松过程规定了其平均值 $\mu$ 和方差 $\sigma^2$ 之间的严格关系：它们必须相等。它还有一个特定的、内置的产生零的概率，即 $\Pr(Y=0) = \exp(-\mu)$。在现实世界的数据中，这些规则常常被惊人地打破。例如，在微生物组数据中，方差可能是均值的五倍（一种称为**过度离散**的现象），而观测到的零值比例可能是75%，而模型预期的仅为45%（一种称为**零膨胀**的现象）。[@problem_id:2498632]。模型根本没有描述我们所看到的现实。

有人可能会建议一个临时技巧，比如对 $\log(Y+1)$ 而不是 $Y$ 建模。但这不是真正的解决方案，它只是一个烟幕弹。在 $Y=0$ 处的一堆零值，只是变成了在 $\log(1)=0$ 处的一堆零值。根本问题——一个部分是离散尖峰，部分是连续涂抹的分布——依然存在。我们没有解决问题，只是给它换了个标签。[@problem_id:4615562]

教训是明确的：我们的数据并非由单一、简单的过程生成。它们在讲述一个由两部分组成的故事。而要理解这个故事，我们需要一个能同时倾听这两个部分的模型。

### [分而治之](@entry_id:139554)：两部分解决方案的精妙之处

科学中最美的想法往往是最简单的。与其寻找一个把所有事情都做得一团糟的复杂模型，不如将问题拆分为两个我们能很好解决的简单问题？这种“[分而治之](@entry_id:139554)”的策略正是**两部分模型**的精髓所在。

这一见解源于一个基本的[概率法则](@entry_id:268260)，即**[全期望定律](@entry_id:265946)**。这个名字听起来很花哨，但它的直觉却非常美妙。它指出，某个量 $Y$ 的[总体平均值](@entry_id:175446)可以这样分解：

$$ E[Y] = \Pr(Y > 0) \cdot E[Y \mid Y > 0] $$

用大白话说就是：某物的平均量等于拥有*任何*该物的概率，乘以*在那些拥有它的人中*的平均量。想一下如何计算一个群体中看医生的平均次数。[@problem_id:4597301]。这个公式告诉我们，它就是去看医生的总人数比例，乘以那些确实去看医生的人的平均就诊次数。

这个方程不仅为我们提供了一种计算平均值的方法，它还为我们提供了一个模型的蓝图。它将我们单一、困难的问题分解为两个不同的、可管理的问题：

1.  **“是否”问题（外延边际）：** 是什么决定了一个人是否具有非零值？这是一个简单的“是/否”问题。一个人是否去看医生，是或否？他们是否产生了任何医疗费用，是或否？对于这个问题，我们可以使用一个[二元选择模型](@entry_id:637424)，比如**[逻辑斯谛回归](@entry_id:136386)**，它非常适合对概率进行建模。

2.  **“多少”问题（内涵边际）：** *在给定*一个人有非零值的情况下，是什么决定了其值的大小？他们有多少次就诊？他们的费用有多高？对于这个问题，我们只看那些具有正值的人的数据。由于这些值通常是有偏的，我们可以使用灵活的模型，如**Gamma 回归**或**对数转换线性模型**，这些模型专为正值、有偏数据设计。[@problem_id:4374973]

这种方法的力量在于其灵活性。一个变量可能影响决策的一个部分，但不影响另一部分，或者可能以不同方式影响两者。考虑一下看医生的自付费用。[@problem_id:4597301]。高额的共付额可能会强烈阻止某人进行第一次就诊（对“是否”问题有很大影响）。但一旦他们病得足够重去看病，后续就诊的次数可能由医生的建议决定，而不是价格（对“多少”问题影响很小或没有影响）。单一模型将难以捕捉这种细微差别，但两部分模型却能优雅地处理它。通过分别对这两个[过程建模](@entry_id:183557)，我们得到了一个更丰富、更真实的对潜在行为的描绘。

### Hurdle 模型与[混合模型](@entry_id:266571)：深入探究“零”

随着我们深入研究，我们发现即使是零值本身也可能有其故事。到目前为止，我们一直将所有零值视为相同：它们代表未能跨越从“零”到“正”的单一障碍。这是 **Hurdle 模型**（或称跨栏模型）的逻辑。这是一个清晰的两阶段过程：首先你决定是否要跨越障碍，如果跨越了，再决定跳多高。

但如果存在两种根本不同类型的零值呢？这就引出了一个稍微更复杂且引人入胜的想法：**[零膨胀模型](@entry_id:756817)**。[@problem_id:2498632] [@problem_id:4964086]。想象一下你正在研究一位垂钓者一年中捕获的鱼的数量。你的数据中的一些零值将来自那些去钓鱼但一无所获的垂钓者。这些是“抽样零”。但另一些零值则来自那些甚至不拥有鱼竿的人。他们根本不属于垂钓人群。这些是“结构性零”。

[零膨胀模型](@entry_id:756817)是一种**[混合模型](@entry_id:266571)**，它明确承认这两种通往零的路径。对于每个人，模型都想象一次抛硬币。

-   以 $1- \pi_i$ 的概率，此人是一个“结构性零”——一个非垂钓者。他们的结果永远是零，没有例外。
-   以 $\pi_i$ 的概率，此人是一个垂钓者，他们的结果从一个标准的计数分布（如泊松分布或[负二项分布](@entry_id:262151)）中抽样，该分布本身也可能产生零（对于不幸的垂钓者来说是“抽样零”）。

因此，观测到零的总概率是这两种可能性的总和：

$$ \Pr(Y_i=0) = \underbrace{(1-\pi_i)}_{\text{Structural Zero}} + \underbrace{\pi_i \cdot \Pr(\text{Count}=0)}_{\text{Sampling Zero}} $$

这个框架常用于零膨胀泊松（ZIP）或零膨胀负二项（ZINB）模型中，其功能极其强大。它允许我们分别提出问题：哪些因素决定某人是否属于“风险”人群（$\pi_i$ 的逻辑斯谛部分），以及对于那些属于风险人群的人，哪些因素影响事件发生的频率（计数部分）。[@problem_id:4964086]

### 统计学家如侦探：挑战与解决方案

构建这些复杂的模型就像当一名侦探；它有其自身的一系列挑战，需要巧妙的工具来解决。
一个微妙的问题是**可识别性**。当你在一个[零膨胀模型](@entry_id:756817)的两个部分都包含相同的解释变量——比如病人的年龄——会发生什么？[@problem_id:4993545]。模型可能会感到困惑。如果老年人有更多的零计数，是因为他们更有可能是“结构性零”（在逻辑斯谛部分），还是因为他们处于“风险”组但事件发生率较低（在计数部分）？数据可能没有足够的信息来清晰地分离这两种效应，导致模型各部分之间出现“拉锯战”，[参数估计](@entry_id:139349)也不稳定。统计学家已经开发出诊断方法来检测这种情况，比如通过分析[似然函数](@entry_id:141927)来查看不同效应组合是否产生几乎相同的结果，或者在[贝叶斯分析](@entry_id:271788)中检查[参数估计](@entry_id:139349)值之间的相关性。[@problem_id:4993545]。这是最高水平的统计侦探工作。

另一个挑战是确定我们对结果的确定性有多大。对于这些模型，计算[标准误](@entry_id:635378)的数学可能变得复杂。在这里，**bootstrap**（[自助法](@entry_id:139281)）提供了一个优雅而强大的解决方案。[@problem_id:4948637]。这个被称为**非参数成对[自助法](@entry_id:139281)**的想法，简单得美妙。把数据集中的每个主题——他们的协变量和他们的结果——想象成一个单一、不可分割的数据“乐高”积木。为了理解结果中的不确定性，你通过有放回地随机抽取 $n$ 个这样的积木，创建数千个新的“自助”数据集。一些原始主题会被多次选中，另一些则完全不会被选中。然后，你在每个新数据集上重新拟合整个两部分模型，并收集结果。你在这数千次拟合中看到的变异，为你提供了一个直接、稳健的原始估计不确定性的度量。这是一项计算上的杰作，使我们能够在不迷失于极其复杂的公式的情况下，做出可靠的推断。

### 从两部分到统一整体：宏大视角

两部分模型的基本原则——识别并分别建模不同但相互关联的过程——是现代统计学中最富有成果的思想之一。它远远超出了简单的零值情况。
考虑这样一个挑战：随时间追踪患者的生物标志物（如肿瘤标志物），同时还想知道该生物标志物的水平如何影响他们发生临床事件（如疾病进展）的风险。[@problem_id:5025558]。一个幼稚的两阶段方法——首先对生物标志物的轨迹进行建模，然后将这些预测值代入生存模型——是充满危险的。它会因测量误差（预测并非完美）和高风险轨迹患者更可能发生事件并“退出”研究（从而使数据产生偏差，即**信息性删失**）而产生偏误。

解决方案是两部分思想的推广：**联合模型**。它构建了一个单一、统一的似然函数，同时描述生物标志物随时间变化的路径和事件发生的风险。这两个过程通过共享的[潜变量](@entry_id:143771)（随机效应）联系在一起，很像 NCI 营养模型的两个部分。[@problem_id:4615530]。通过将纵向过程和生存过程一同建模，该模型正确地解释了测量误差，并利用关于事件发生时间（或是否发生）的信息来更准确地描绘整个生物标志物的轨迹。这是一个绝佳的例子，说明了承认不同数据生成过程之间的相互关联性，如何导向对世界更深刻、更准确的理解。从一堆简单的零值到生命与死亡的复杂动态，“[分而治之](@entry_id:139554)”的原则，即通过对部分建模来理解整体的原则，揭示了统计推理的内在统一性与美感。

